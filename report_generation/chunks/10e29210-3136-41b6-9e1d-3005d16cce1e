{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/751-US20210102527(Pending) re (Done on website already).pdf"}, "page_content": "[ 0094 ] FIG . 6 shows a general framework of an example\n\nsaid layer which may be initialized as the number of the\n\nmodel training process 600. In source domain learning , Dst\n\ngreatest eigenvalues that can make up 80 % of the sum of all\n\n514 and Ds , 516 according to FIG . 5 are used to train a base\n\neigenvalues obtained by PCA .\n\nDNN 606. The DNN structure may be self - organizing\n\n[ 0104 ] 3 \u2014 Determining the dimension of { We , Wa } and\n\nwithout a prior specification , in which details will be elabo\n\n{ B , BC ) .\n\nrated below with FIGS . 7 to 9 .\n\n[ 0105 ] FIG . 9 shows the method of determining the num\n\n[ 0095 ] Hyper - parameters of the base DNN 606 , { L , N ,\n\nW { 0 } , B { C } } 608 , may be automatically determined by two\n\nber of hidden nodes , denoted as N2 + 1 , after inserting a\n\ncompact hidden layer 1 + 1 referring to the step 2 706 in FIG .\n\npre - specified thresholds , e , and 0 , 612. Then , in target\n\n{ i } , B { } i ,\n\n7. The expansion of the inserted hidden layer contains three\n\nK , can\n\nbe finally obtained by optimizing the network parameters\n\nsteps :\n\nusing backpropagation ( BP ) method based on { W { 0 } , B { 0 } }\n\n[ 0106 ]\n\nStep 1. Initialize 802 : add one or some hidden\n\nnodes on the inserted hidden layer . Denote the weights\n\nas the initial values .\n\nconnecting the hidden layer 1 and these new hidden nodes as\n\n[ 0096 ] FIG . 7 shows an example process of the source\n\nWeu , and the weights connecting the new hidden nodes and\n\ndomain training 700. In this example , the DNN structure in\n\nsource domain learning is determined by self - expanding an\n\nthe output layer as W\n\nand the biases of the new hidden\n\ndu '\n\ninitial network through continuously stacking hidden layers\n\nnodes as Beu . At each update , Weu Wdu and Beu will be\n\nrandomly initialized and aggregated to the We , W. and Be ,\n\nand units until a suitable size is achieved . The hyperparam\n\nrespectively .\n\neter 0 , and 62 612 referred to in FIG . 6 may assist in deciding", "type": "Document"}}