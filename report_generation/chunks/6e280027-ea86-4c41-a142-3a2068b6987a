{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/751-US20210102527(Pending) re (Done on website already).pdf"}, "page_content": "[0098] Afterwards, the reconstruction error (RE) on vali- dation set,\n\n1 \u958b Ry = Fp Dale RIP\n\n[0086] Once the data grouping is performed, the model training process is ready to begin. This step is to train each of the learning networks such that they can model each individual WT. Various types of machine learning networks can be used but preferably, a deep neural network (DNN) is trained for each WT with both the source domain 602 and the target domain data 604. In this example embodiment as shown in FIGS. 5 and 6, the DNNs 606 may consist of an input layer, one or more hidden layers and an output layer with the purpose of reconstructing the input data itself.\n\n[0099] where x, denotes the j-th reconstructed data point of D,,, and M,, means the data size of D,,, is computed at step 3 to evaluate model performance 708. The network expan- sion procedure from step 1 through 3 may then be repeated, until R, continuously increases for gl times 710. Finally, the network causing the minimal R,, in search history will be selected as the base DNN 712.\n\n[0100] The manipulation of the DNN is represented by the diagrams of FIGS. 8 and 9, which shows a visual represen- tation of the method of inserting a hidden layer, i.e. the step 1 702 in FIG. 7. Consider a DNN containing hidden layers 802, whose weights and biases can be represented as Wi and B, while Wy, , and Bg, \u00bb denote the i-th weight and bias matrices in W, and B,, i=1, 2,... , 1+1. Anew hidden layer 804 may be inserted between the hidden layer 1 802 and output layer 806. In such operation, all elements of W, and B, except Wu \u00bb41) and By, 241) are firstly inherited to W,,, and B,,1- Meanwhile, weights, {W,, W,} and biases, {B,, B,}. produced due to the insertion of a new hidden layer are also included into W,,, and B;,,, where e stands for the encoder and d stands for the decoder.\n\n[0087] Solving such an example of a modelling problem equivalent to minimizing a reconstruction loss (RL),\n\ngta?", "type": "Document"}}