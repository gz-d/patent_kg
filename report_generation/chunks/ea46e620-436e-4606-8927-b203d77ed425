{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/751-US20210102527(Pending) re (Done on website already).pdf"}, "page_content": "and units until a suitable size is achieved . The hyperparam\n\nrespectively .\n\neter 0 , and 62 612 referred to in FIG . 6 may assist in deciding\n\nwhen to stop adding new hidden layers or hidden nodes .\n\n[ 0107 ]\n\nStep 2. Update 804 : train the weights and biases of\n\nthe augmented partial network noted in FIG . 9. We , W. Be\n\n[ 0097 ] At the beginning of source domain learning , a\n\nneural network may start with no hidden layers inside 702 .\n\nand B , will be trained using the gradient descent method\n\nUS 2021/0102527 Al\n\nApr. 8, 2021\n\nHowever as the learning process begins, a hidden layer with several hidden nodes is inserted to the network 704, refer- ring to the step 1. The number of hidden nodes on the inserted hidden layer is next determined 706 at the step 2. The step 1 704 and step 2 706 will be explained in detail later with reference to FIGS. 8 and 9.\n\n[0082] To enable the two-phase training scheme, source domain 602 and target domain 604 can be con- structed as shown in FIGS. 5 and 6, where DB is divided into two parts:\n\n[0084] the other for its own target domain 512.\n\n[0085] The source domain 510 may contain the parts of data from all WTs is further divided to source domain training set D,, 514 and source domain validation set D,,, 516. The target domain of the i-th WT 512 is split into fine-tuning set Di 518 and test set D,,\u201c 520.\n\n[0098] Afterwards, the reconstruction error (RE) on vali- dation set,\n\n1 \u958b Ry = Fp Dale RIP", "type": "Document"}}