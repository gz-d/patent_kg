{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/777-WO2021087651A1(Filed) not sealed.pdf"}, "page_content": "transition probabilities.\n\nEXAMPLE 8\u2014Training individual SHMMs\n\nAn Expectation-Maximization (EM) algorithm was performed to estimate the SHMM parameters. In the Expectation step (E-step), the responsibilities were calculated using the standard forward-backward algorithm with the block transition matrix, initial state vector, and emission densities. In the Maximization step (M-step), the prior and pairwise responsibilities were summed over the high-level and the low-level states, respectively, to yield the parameter updates for both the high-level states and the low-level states.\n\nFor example, the prior responsibilities were summed over the low-level hidden states for each of the high-level state to yield the parameter updates for the low-level state sequence, and then they were summed over the high-level states to yield the parameter updates for the high-level state sequence. Similarly, the pairwise responsibilities were summed over the low- level hidden states for each high-level state to yield the updates for each low-level transition,\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n27\n\nthen were summed over the high-level hidden states to yield the updates for the switching (transition) matrix of the high-level state sequence.\n\nand\n\nFor each participant, two SHMMs were trained;\n\none using the data from the left-", "type": "Document"}}