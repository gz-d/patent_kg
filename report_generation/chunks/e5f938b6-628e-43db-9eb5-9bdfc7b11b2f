{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/751-US20210102527(Pending) re (Done on website already).pdf"}, "page_content": "[0096] FIG. 7 shows an example process of the source domain training 700. In this example, the DNN structure in source domain learning is determined by self-expanding an initial network through continuously stacking hidden layers and units until a suitable size is achieved. The hyperparam- eter 01 and 0, 612 referred to in FIG. 6 may assist in deciding when to stop adding new hidden layers or hidden nodes.\n\n[0107] Step 2. Update 804: train the weights and biases of the augmented partial network noted in FIG. 9. WwW\u3002 Wz, B\u3002 and B, will be trained using the gradient descent method\n\n[0097] At the beginning of source domain learning, a neural network may start with no hidden layers inside 702.\n\nthe\n\n[0083]\n\none for constructing the source domain 510, and\n\nis\n\nithayyytih_\n\n[0089] eterized\n\nparam-\n\nwhere L refers\n\nthe total number of hidden\n\nB\n\n[0093] Such parameterization is motivated by an assump- tion that the homogeneous patterns and heterogeneous char- acteristics among K WTs can be modelled by a compro- mised {L, N} and differentiated {W\u2122, BO}.\n\n[0095] Hyper-parameters of the base DNN 606, {L, N, Ww}, BYP) 608, may be automatically determined by two pre-specified thresholds, 0, and 0, 612. Then, in target domain learning 604, the {w\", Bia}, i=l, 2,...,K, can be finally obtained by optimizing the network parameters using backpropagation (BP) method based on {W?}, BO as the initial values.\n\nby\n\n[0105] FIG. 9 shows the method of determining the num- ber of hidden nodes, denoted as N,,;, after inserting a compact hidden layer 1+1 referring to the step 2 706 in FIG. 7. The expansion of the inserted hidden layer contains three steps:\n\nApr. 8 , 2021\n\nUS 2021/0102527 A1\n\n6\n\nroutines , programs , objects , components and data files\n\nwhile fixing the parameters from input layer to the hidden\n\nassisting in the performance of particular functions , the\n\nlayer 1 , i.e. { W1 ,\n\nW ; } and { B1 , .. B , } . Then , RE on\n\n1 ...", "type": "Document"}}