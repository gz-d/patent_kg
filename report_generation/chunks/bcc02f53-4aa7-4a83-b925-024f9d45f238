{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/723-US20210044791A1(Pending) re (Done on website already).pdf"}, "page_content": "coefficients . P ( Yuva Yu , di ) and A ( Yuva Yu , vdi ) are both\n\n0,2\n\n0,1\n\n?\n\ndictionaries for sparse representation , \" IEEE Trans . Signal\n\namong the range [ 0 , 1 ] . The combination of Equations ( 8 )\n\nand ( 9 ) can be used to measure the integral similarity\n\nProcess . , vol . 54 , no . 11 , pp . 4311-4322 , November 2006 .\n\nDuring learning , the sparse coefficients are solved by OMP\n\nbetween patch y . , and yu , dui . In fact , P ( y2.0 , y.d. ) and\n\nYu , vd.1 ) represent the non - flicker features and they\n\nalgorithm presented in Y. C. Pati , R. Rezaiifar and P. S.\n\n0,1\n\nA ( Yu , 2\n\nKrishnaprasad , \" Orthogonal matching pursuit : recursive\n\nmay be large in representing flicker distortions .\n\nfunction approximation with applications to wavelet decom\n\n[ 0111 ]\n\nSince human eyes tend to perceive the flicker\n\nposition , \" Proc . Conf . Rec . 27th Asilomar Conf . Signals ,\n\ndistortion in the form of regions instead of lines , the flicker\n\ndistortion can be computed over multiple temporal layers\n\nSyst . Comput . , vol . 1 , 1993 , pp . 40-44 . The learned temporal\n\ndictionary is 64x256 .\n\ninstead of a single layer . For simplicity , the sparse coeffi\n\nYu . , d \u00bb ) and A ( Yu , Yu.vd. ) of a group of\n\ncients P ( Yu , 0,1\n\n[ 0109 ] To demonstrate the difference between the learned\n\n0,1\n\ntemporal layers , i.e. , Ux = { L ; lh , ( k - 1 ) +1 < i < hk } ,\n\ntemporal dictionary with the normal spatial dictionary ,\n\nFIGS . 7A and 7B illustrate the two types of dictionaries for\n\ncomparison . As shown in FIGS . 7A and 7B , both diction\n\nke [ 1 . hs\n\naries were learned with the same training sequences and\n\nH\n\nlearning parameters . The difference is that the spatial dic\n\ntionary was learned from the conventional image patches\n\nwhile the temporal dictionary was learned from the patches\n\nare averagely merged , and the integral similarity for patches\n\nextracted from temporal layers . It is observed from FIGS .\n\nlocating ( u , v ) at", "type": "Document"}}