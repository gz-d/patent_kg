{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/751-US20210102527(Pending) re (Done on website already).pdf"}, "page_content": "[0087] Solving such an example of a modelling problem equivalent to minimizing a reconstruction loss (RL),\n\ngta?\n\n[0088] where \u6587 \u54c1 denotes the reconstructed feature space by the DNN model.\n\nThe DNN for the i-th WT may therefore be by 608:\n\n{LN, WAY Bray\n\n[0090] to layers, [0091] N={N,,N,,...,N,,,} means the number of nodes of all layers with\n\n[0092] N,=N;,,=1 corresponding to the input layer and output layer. Weetw, 2, wef, Wy} and \u4eba ={Bi 4, BL, 2... B,,, } present weights and bias distributed across connections among all layers of the DNN models 606.\n\n[0101] The initial number of hidden nodes on this layer may be determined based on Principle Component Analysis (PCA) technique, which is a classical dimension reduction technique. A preferred embodiment of PCA in this invention is as follows:\n\n[0102] 1 \u4e00 compute the output of the hidden layer 1 feeding D,, into the original neural network using feedfor- ward algorithm.\n\n[0094] FIG. 6 shows a general framework of an example model training process 600. In source domain learning, D,, \u00a714 and D,,, 516 according to FIG. 5 are used to train a base DNN 606. The DNN structure may be self-organizing without a prior specification, in which details will be elabo- rated below with FIGS. 7 to 9.\n\n[0103] 2\u2014Set the initial number of hidden nodes of the said layer which may be initialized as the number of the greatest eigenvalues that can make up 80% of the sum of all eigenvalues obtained by PCA.\n\n[0104] 3\u2014Determining the dimension of {W,, W,} and {B,, By}.\n\n[0106] Step 1. Initialize 802: add one or some hidden nodes on the inserted hidden layer. Denote the weights connecting the hidden layer | and these new hidden nodes as W,,,, and the weights connecting the new hidden nodes and the output layer as Wu and the biases of the new hidden nodes as B,,,. At each update, W,,,, Wz, and B,,, will be randomly initialized and aggregated to the W,, Wu and Be respectively.", "type": "Document"}}