{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/751-US20210102527(Pending) re (Done on website already).pdf"}, "page_content": "data from all WTs is further divided to source domain\n\ndation set ,\n\ntraining set Dst 514 and source domain validation set Ds\n\n516. The target domain of the i - th WT 512 is split into\n\nfine - tuning set D , { i } 518 and test set Deli } 520 .\n\nR = . | X ; \u2013 8 ; 17\n\n[ 0086 ] Once the data grouping is performed , the model\n\ntraining process is ready to begin . This step is to train each\n\nof the learning networks such that they can model each\n\n[ 0099 ] where \u00d1 , denotes the j - th reconstructed data point\n\nindividual WT . Various types of machine learning networks\n\nof Ds , and M , means the data size of Dev , is computed at step\n\ncan be used but preferably , a deep neural network ( DNN ) is\n\n3 to evaluate model performance 708. The network expan\n\ntrained for each WT with both the source domain 602 and\n\nsion procedure from step 1 through 3 may then be repeated ,\n\nthe target domain data 604. In this example embodiment as\n\nuntil R , continuously increases for 0 times 710. Finally , the\n\nshown in FIGS . 5 and 6 , the DNNs 606 may consist of an\n\nnetwork causing the minimal R , in search history will be\n\ninput layer , one or more hidden layers and an output layer\n\nselected as the base DNN 712 .\n\nwith the purpose of reconstructing the input data itself .\n\n[ 0100 ] The manipulation of the DNN is represented by the\n\n[ 008 ]\n\nSolving such an example of a modelling problem is\n\ndiagrams of FIGS . 8 and 9 , which shows a visual represen\n\nequivalent to minimizing a reconstruction loss ( RL ) ,\n\ntation of the method of inserting a hidden layer , i.e. the step\n\n1 702 in FIG . 7. Consider a DNN containing hidden layers\n\nj { i } = [ { Y { i } - $ { i } | 2\n\n802 , whose weights and biases can be represented as W , and\n\n[ 0088 ] where \u00ca { i } denotes the reconstructed feature space\n\ndenote the i - th weight and bias\n\nBi , while W\n\nand B ( 1,1 )\n\nby the DNN model .\n\n( 1 , 1 )\n\nmatrices in W , and B? , i = 1 , 2 , ... , 1 + 1 . A new hidden layer", "type": "Document"}}