{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/1023 outsourced (Done on website).pdf"}, "page_content": "15\n\nreport. The sample after three filters is called processed textual data 405.\n\n20\n\n25\n\n[0047]\n\nFIG. 5A is a schematic block diagram illustrating one embodiment of a data\n\nextracting module. The DICOM from element 501 is the input of the first filter 502, which excludes the slides with specific meta information from DICOM, including non- brain exam body parts, RGB planes, missing Image Position or Image Orientation information, diagnosis and patient ID. The following image normalization 503 is used to normalize the image size. All images are standardized as squares, such as 512 pixels * 512 pixels pictures. The input of the second filter are the processed structured data 307 from the structured data pre-processing process 300, and the output from images normalization 503, which can screen out the samples that only have one of structured data and image data. The images labelling 505 is designed to select and provide labels to the image that represents a confirmed stroke or the first effective imaging examination after a stroke is selected, because of the multi-level (patient level, accession level, and episode level) of images. The output of the images labelling 505 is the\n\nprocessed image data 506.\n\n30\n\n[0048]\n\nFIG. 6 illustrates the machine learning and deep learning models\u2019 training,\n\nvalidating and testing pipeline for structured data. This element is a component of the\n\nDocket No. UM1104HK00 13\n\nHK 30057394 A\n\n5\n\nelement 114 in FIG. 1. After receiving data from model 307, data are randomly divided", "type": "Document"}}