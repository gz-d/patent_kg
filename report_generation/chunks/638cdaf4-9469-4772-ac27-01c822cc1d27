{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/723-US20210044791A1(Pending) re (Done on website already).pdf"}, "page_content": "A. Temporal Layer Conversion\n\n[0096] A video can generally be considered as 3D volu- metric data V={V(x,y,t)|] SxSW, 1Sy<H, 1<t<T} where H, W, T represent video height (Y), width (X), and frame length (T), respectively. By dividing the 3D-XYT video into multiple 2D-XT layers, the video could be redefined as V={L,) iH}, where L={V(xy.Hl1 <x <W, y=i, 1<t<T} is the i-th temporal layer, and the eight H is also the total amount of temporal layers, i.e., 2D-XT planes. For VQA of synthesized video, there are three main advantages provided by segmenting a video into temporal layers: (1) visuality: it helps visualize the temporal features and can provide an explicit and intuitional cues; (2) capability: the temporal layer picture, a surface formed by space lines varying with time, can be used to present the long-term temporal features of the video; (2) simplicity: it avoids employing motion estimation method to match the atch between t-i-th frame and t-th frame to capture the motion features.\n\nsin 8? \u2014 WP aI + allel, + 200K?) 43D\n\nwhere X*?=[x1, x2,...,X;,...,X,] denotes the 3D training patches set from video data. x,ER* is a 3D volumetric data, consisting of horizontal x, vertical y, and temporal t dimen- sions, which is denoted as 3D-XYT.", "type": "Document"}}