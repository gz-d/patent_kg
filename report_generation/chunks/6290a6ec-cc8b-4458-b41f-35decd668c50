{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/751-US20210102527(Pending) re (Done on website already).pdf"}, "page_content": "denote the i - th weight and bias\n\nBi , while W\n\nand B ( 1,1 )\n\nby the DNN model .\n\n( 1 , 1 )\n\nmatrices in W , and B? , i = 1 , 2 , ... , 1 + 1 . A new hidden layer\n\n[ 0089 ] The DNN for the i - th WT may therefore be param\n\n804 may be inserted between the hidden layer 1 802 and\n\neterized by 608 :\n\noutput layer 806. In such operation , all elements of W , and\n\n{ L , N , wit , Bit )\n\nB , except W ( 1 , 2 + 1 )\n\nare firstly inherited to W and\n\nand B ( 1,2 + 1 )\n\n1 + 1\n\nB2-1 . Meanwhile , weights , { We , Wa } and biases , { Be , Ba } ,\n\n[ 0090 ] where L refers to the total number of hidden layers ,\n\nproduced due to the insertion of a new hidden layer are also\n\n10091 ]\n\nN = { No , N1 , ... , NZ + 1 } means the number of nodes\n\nincluded into W + 1 and B2 + 1 , where e stands for the encoder\n\nof all layers with\n\nand d stands for the decoder .\n\n[ 0092 ] No = N2 + 1 Fr corresponding to the input layer and\n\noutput layer . W { i } = { W , { i } , w , { i } ,\n\nW1 + 1 { i } } and\n\n[ 0101 ] The initial number of hidden nodes on this layer\n\nmay be determined based on Principle Component Analysis\n\n2\n\nB { i } = { B , { i } , B , { i }\n\nBL + 1 { } } present weights and bias\n\n( PCA ) technique , which is a classical dimension reduction\n\ndistributed across connections among all layers of the DNN\n\ntechnique . A preferred embodiment of PCA in this invention\n\nmodels 606 .\n\nis as follows :\n\n[ 0093 ] Such parameterization is motivated by an assump\n\n[ 0102 ]\n\n1 - compute the output of the hidden layer 1 by\n\ntion that the homogeneous patterns and heterogeneous char\n\nfeeding Dst into the original neural network using feedfor\n\nacteristics among K WTs can be modelled by a compro\n\nward algorithm .\n\nmised ( L , N } and differentiated { W { } ,\n\n} ,\n\n[ 0103 ]\n\n2 \u2014 Set the initial number of hidden nodes of the\n\n[ 0094 ] FIG . 6 shows a general framework of an example\n\nsaid layer which may be initialized as the number of the\n\nmodel training process 600. In source domain learning , Dst", "type": "Document"}}