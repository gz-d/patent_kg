{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/777-WO2021087651A1(Filed) not sealed.pdf"}, "page_content": "(12) INTERNATIONAL APPLICATION PUBLISHED UNDER THE PATENT COOPERATION TREATY (PCT)\n\n19) World Intellectual P rt (19) Worke intetecuna) Property Organization\n\nInternational Bureau\n\n(43) International Publication Date 14 May 2021 (14.05.2021)\n\nWIiPOI|PCT\n\n(10) International Publication Number\n\nWO 2021/087651 Al\n\n(51)\n\nInternational Patent Classification: A61B 3/113 (2006.01) A61B 5/00 (2006.01) GO6N 20/00 (2019.01)\n\n(21) International Application Number:\n\nPCTICN2019/115288\n\n(22) International Filing Date:\n\n04 November 2019 (04.11.2019)\n\nAO, AT, AU, AZ, BA, BB, BG, BH, BN, BR, BW, BY, BZ, CA, CH, CL, CN, CO, CR, CU, CZ, DE, DJ, DK, DM, DO, DZ, EC, EE, EG, ES, FI, GB, GD, GE, GH, GM, GT, HN, HR, HU, ID, IL, IN, IR, IN, JO, JP, KE, KG, KH, KN, KP, KR, KW, KZ, LA, LC, LK, LR, LS, LU, LY, MA, MD, ME, MG, MK, MN, MW, MX, MY, MZ, NA, NG, NI, NO, NZ, OM, PA, PE, PG, PH, PL, PT, QA, RO, RS, RU, RW, SA, SC, SD, SE, SG, SK, SL, SM, ST, SV, SY, TH, TJ, TM, TN, TR, TT, TZ, UA Z ZA, ZM, ZW. TL, 1Z, VA, UG, Us, UZ, VE, VN, 2A, aw\n\n(25) Filing Language:\n\nEnglish\n\nor (26) Publication Language:\n\nEnglish\n\n(71) Applicants: THE UNIVERSITY OF HONG KONG [CN/CN]; Pokfulam Road, Hong Kong (CN). CITY UNIVERSITY OF HONG KONG [CN/CN], 83 TAT Chee Avenue, Kowloon, Hong Kong (CN).\n\n(72) Inventors:\n\nHui\n\nFlat\n\nTower\n\nHSIAO, Wen; D, 22/F, 10, land Harbourview, 11 Hoi Fai Rd., Kowloon, Hong Kong (CN). CHAN, Antoni Bert; Department Of Computer Science, City University Of, Hong Kong, Tat Chee Avenue, Kowloon Tong, Hong Kong (CN).\n\nIs-\n\n(84) Designated States (unless otherwise indicated, for every kind of regional protection available): ARIPO (BW, GH, GM, KE, LR, LS, MW, MZ, NA, RW, SD, SL, ST, SZ, TZ, UG, ZM, ZW), Eurasian (AM, AZ, BY, KG, KZ, RU, TY, TM), European (AL, AT, BE, BG, CH, CY, CZ, DE, DK, EE, ES, FI, FR, GB, GR, HR, HU, IE, IS, IT, LT, LU, LV, MC, MK, MT, NL, NO, PL, PT, RO, RS, SE, SI, SK, SM, TR), OAPI (BF. BJ, CF, CG, CL, CM, GA, GN, GO. GW KM. ML MR NE SN TD T Gy\n\nPublished:\n\n\u2014 with international search report (Art. 21(3))\n\n1\n\nAd\n\n(74) Agent: CHINA PATENT AGENT (H.K.) LID. 22/F., Great Eagle Center, 23 Harbour Road, Wanchai, Hong Kong (CN).\n\n(81)\n\nDesignated States (unless otherwise indicated, for every kind of national protection available): AE, AG, AL, AM,\n\n(54)\n\nTitle: EYE MOVEMENT ANALYSIS WITH CO-CLUSTERING OF HIDDEN MARKOV MODELS (EMHMM WITH CLUSTERING) AND WITH SWITCHING HIDDEN MARKOV MODELS (EMSHMM)\n\nFIGURE 2\n\nSR ewe \\ SS a we Re . . ree - or Mt os state ate see eas Lee ya * we Os [ESS SR\n\n2021/087651 (57) Abstract: Provided are an Eye Movement analysis with Hidden Markov Model (EMHMM) with co-clustering, an Eye Movement analysis with Switching Hidden Markov Models (EMSHMM) to analyze eye movement data in cognitive tasks involving stimuli with different feature layouts and cognitive state changes, a switching hidden Markov model (SHMM) to capture a participant's cognitive State transitions during the task and an EMSHMM to assess preference decision-making tasks with two or more cognitive states. The EMSHMM provides quantitative measures of individual differences in cognitive behavior/style, making a significant impact on the use of eye tracking to study cognitive behavior across disciplines.\n\nwo\n\nCO-\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n]\n\nDESCRIPTION\n\nEYE MOVEMENT ANALYSIS WITH CO-CLUSTERING OF HIDDEN MARKOV MODELS (EMHMM WITH CO-CLUSTERING) AND WITH SWITCHING HIDDEN MARKOV MODELS (EMSHMM)\n\nBACKGROUND OF THE INVENTION\n\na\n\nRecent research has shown that\n\nhave\n\npeople idiosyncratic eye movement patterns visual tasks that are consistent across different stimuli and tasks (e.g., Andrews & Coppola, 1999; Castelhano & Henderson, 2008; Poynter, Barber, Inman, & Wiggins, 2013; Kanan, Bseiso, Ray, Hsiao, & Cottrell, 2015). These idiosyncratic eye movement patterns may reflect individual differences in cognitive style or abilities. For example, it was found that participants who demonstrated higher levels of curiosity made significantly more fixations in scene viewing task than those who demonstrated lower levels of curiosity. It was further found that when viewing human faces, those who scored higher on extraversion and agreeableness personality traits looked at the eyes significantly more often than those who scored lower (Wu et al., 2014).\n\nIn recent years, there have been attempts of using machine-learning methods to infer\n\ncharacteristics of the observer from eye movement data (e.g., Kanan et al., 2015). These studies typically use classifiers to discover eye movement features important for distinguishing two or more observers, and do not tell us about overall eye movement patterns associated with a particular observer. To better understand the association between eye movement patterns in visual tasks and individual differences in cognitive style or abilities, the inventors have recently developed a state-of-the-art eye movement data analysis method, Eye Movement analysis with Hidden Markov Model (EMHMM), which takes individual differences in temporal and spatial dimensions of eye movements into account (Chuk, Chan, & Hsiao, 2014. EMHMM Matlab toolbox 1s available at\n\nnitp //visal cs cttye edu bk/research/embmny).\n\nThe EMHMM method is based on the assumption that during a visual task, the currently fixated region of interest (ROI) depends on the previously fixated ROI. Thus, eye movements in a visual task may be considered a Markovian stochastic process, which can be better understood using a hidden Markov model (HMM), a type of time-series statistical model in machine learning. More specifically, in this method, HMMs were used to directly\n\nin\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n2\n\nmodel\n\nand hidden\n\nof the\n\neye movement data, states HMMs corresponded to ROIs eye movements. The transition probabilities between hidden states (ROIs) represented the temporal pattern of eye movements between ROIs. To account for individual differences, one HMM was used to model one person\u2019s eye movement pattern in a visual task in terms of both person-specific ROIs and transitions among the ROIs. An individual\u2019s HMM was estimated from the individual\u2019s eye movement data using a variational Bayesian approach that can automatically determine the optimal number of ROIs. In addition, individual HMMs could be clustered according to their similarities (Coviello, Chan, & Lanckriet, 2014) to reveal common patterns. Differences among models could be quantitatively assessed using likelihood measures, that is, by calculating the log-likelihood of data from one model being generated by another model.\n\nof\n\nThus, the EMHMM method was particularly suitable for examining individual differences in eye movement patterns and their associations with other cognitive measures. Also, since HMM is a probabilistic time-series model, it worked well with a limited amount of data (e.g., 20 trials), which is in contrast to deep learning methods that require large amounts of data to train effectively. Thus, the EMHMM approach was especially suitable for psychological research where data availability 1s limited or data collection 1s time consuming.\n\nThe EMHMM method\n\nface\n\nresearch and\n\nto\n\nwas successfully applied recognition discovered novel findings thus far not revealed by other methods. For example, two common eye movement patterns for face recognition were discovered: a \u201cholistic\u201d pattern in which participants mainly looked at the center of a face, and an \u201canalytic\u201d pattern that involved more frequent eye movements between the two eyes and the face center (see, e.g., FIGURE la). Interestingly, analytic patterns were associated with better recognition performance, and this effect was consistently observed across different culture and age groups (e.g., Chuk, Chan, & Hsiao, 2017; Chuk, Crookes, Hayward, Chan, & Hsiao, 2017; Chan, Chan, Lee, & Hsiao, 2018). In addition, significantly more participants (75%) used same eye movement patterns when viewing own- and other-race faces than different patterns (Chuk, Crookes, et al., 2017). In contrast, only around 60% of participants used the same eye movement patterns between face learning and recognition, and their recognition performance did not differ significantly from those using different patterns. This is in contrast to the scan path theory, which posits that eye movements during learning have to be recapitulated during recognition\n\nfor the recognition to be successful (Chuk et al., 2017).\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\nIt was further found that older adults adopted holistic patterns whereas young adults adopted analytic patterns (see, e.g., FIGURE 1b). This difference was not readily observable from group eye fixation heat maps, demonstrating the power of the EMHMM method (FIGURE 1c; Chan et al., 2018). Among older adults, holistic patterns were associated with lower cognitive status as assessed using Montreal Cognitive Assessment (HK-MoCA; Yeung, Wong, Chan, Leung, & Yung, 2014); particularly in executive function and visual attention abilities (as assessed by Tower of London (TOL) and Trail Making Tests). Interestingly, this association was replicated when models of the discovered common patterns were used to assess new participants\u2019 eye movement patterns when viewing new face images, suggesting the possibility of developing representative models from the population for cognitive impairment screening purposes.\n\nPrevious eye movement data analysis methods, including the use of predefined regions of interest (ROIs) or fixation heat map, did not take temporal information of eye movements into account and the use of predefined ROIs might have involved experimenter biases. Further, in these methods the analysis was typically performed on averaged or group data, and thus did not reflect individual differences in eye movement pattern. In addition, there was no quantitative measure of eye movement pattern similarity that took both spatial (eye fixation location) and temporal information (sequence of fixations) into account.\n\nAlso, the previous EMHMM method was limited to visual tasks where stimuli have the same feature layout (such as face recognition) and did not involve cognitive state changes. Therefore, novel methods are needed for quantitative measurements of eye movement patters that take spatial and temporal information into account and allow a more comprehensive prediction of human behavior based on eye movement patterns.\n\nBRIEF SUMMARY OF THE INVENTION\n\nProvided are methods and systems for the analysis and modeling of cognitive tasks involving cognitive state changes to predict subject behavior and/or cognitive processes. Specifically, provided is an Eye Movement analysis with Hidden Markov Model (EMHMM) with co-clustering and an Eye Movement analysis with Switching Hidden Markov Model (EMSHMM). The models of the invention measure differences in eye movement patterns by summarizing individuals\u2019 eye movements using hidden Markov models, allow discovery of common eye movement patterns, and provide quantitative measures of eye movement pattern similarities that take both temporal and spatial information of eye movements into account.\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n4\n\nAdvantageously, the EMSHMM of the instant invention allows data analysis and modeling for cognitive tasks involving cognitive state changes and provides better predictions and analysis of subject behavior and cognitive processes. Furthermore, the EMHMM with co- clustering allows data analysis in tasks and cognitive behaviors that involve stimuli with different layouts, including, but not limited to, website viewing, information system usage, and visual search.\n\nBRIEF DESCRIPTION OF THE DRAWINGS\n\nFIGURES 1A-C show the detection of two common eye movement patterns for face recognition. FIGURE 1A shows a holistic eye movement pattern on the left and an analytical eye movement pattern on the right. FIGURE 1B shows a correlation of holistic and analytic eye movements with age. FIGURE 1C shows the fixation heat maps of old participants on the left and of young participants on the nght.\n\nFIGURE 2 shows an example of EMHMM with co-clustering of the invention. Circles indicate ROIs.\n\nand\n\nFIGURE 3 shows histograms of symmetric KL (SKL) divergence between Group Group 2 strategies on vehicle or animal images.\n\nFIGURES 4A-C show examples of eye movement strategies. FIGURE 4A shows example stimuli. FIGURE 4B shows eye movement strategies for the 2 groups on the stimuli where the 2 groups use strategies that have a larger difference on the first 2 images and use strategies with a smaller difference on the last 2 images as measured by symmetric KL divergence (SKL). FIGURE 4C shows heat map plots of eye fixations on the first image for the 2 groups and the difference regions in orange for Group 1 and blue for Group 2.\n\nFIGURES 5A-F show correlation analyses. FIGURE 5A shows the correlation between EF scale and average number of fixations. FIGURE 5B shows the correlation between EF scale and average saccade length. FIGURE SC shows the correlation between EF scale and preference rating. FIGURE 5D shows the correlation between EF scale and scene recognition performance in d\u2019. Figure 5E shows the correlation between EF scale and Flanker Congruent Acc. Figure SF shows the correlation between EF scale and TOL planning time before execution.\n\nFIGURE 6 shows an example SHMM model summarizing a participant\u2019s eye movement pattern in the preference decision-making task. The blue arrows indicate the transition probabilities between cognitive states (high-level states); the red arrows indicate the transition probabilities between the ROIs (low-level states).\n\n1\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n5\n\nFIGURE 7 shows the gaze cascade plots during the last 2.5 seconds before response for two participant groups (Group A and B) and all participants (All). The red, blue, and green stars on the top indicate the time points at which the gaze cascade effect occurred; the black stars on the bottom indicate the time points during which the two groups had significant differences ln the proportion of time spent on the chosen item.\n\nFIGURES 8A-B show the differences in transitions between exploration and preference-biased periods between two participant groups. FIGURE 8A shows the average probability of the two groups being in the preference-biased period throughout a trial. FIGURE 8B shows the probability distribution of the number of fixations in the exploration period (top) and the number of fixations in the preference-biased period (bottom).\n\nFIGURES 9A-B show the average inference accuracies of the two participant groups. FIGURE 9A shows the average inference accuracies using partial fixations in a trial, selected as a percentage of each trial\u2019s duration from the beginning. FIGURE 9B shows the average inference accuracies of the two participant groups using different window lengths starting from the beginning of the trial.\n\nFIGURES 10A-B show the average inference accuracies using different fixation patterns. FIGURE 10A shows the average inference accuracies of two groups of participants using fixations in the last 2 seconds before the participants\u2019 response. FIGURE 10B shows the relationship between participants\u2019 eye movement patterns (measured in AB scale) and the corresponding inference accuracy.\n\nwhen\n\nFIGURE 11 shows the average inference accuracy using SHMMs and regular using the last 2 seconds of the trials.\n\nHMMs\n\nDETAILED DISCLOSURE OF THE INVENTION\n\nProvided are methods and systems for the analysis and modeling of cognitive tasks involving cognitive state changes to predict subject behavior and/or cognitive processes.\n\nIn some embodiments, an Eye Movement analysis with Hidden Markov Model (EMHMM,) with co-clustering is provided where EMHMM is combined with the data mining technique co-clustering to detect participant groups with consistent eye movement patterns across stimuli for tasks involving stimuli with different feature layouts. In specific embodiments, this model is applied to scene perception.\n\nIn some embodiments, an Eye Movement analysis with Switching Hidden Markov Model (EMSHMM) is provided to detect participant groups with consistent eye movement\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n6\n\npatters with cognitive state changes. The EMSHMM of the instant invention is particularly useful for tasks involving decision making where participants need to explore different options and decide which option they prefer; a process that involves at least two cognitive states: exploration and decision making.\n\nAlso provided are methods for determining a subject\u2019s perception style and cognitive abilities.\n\nThe EMHMM method of the invention takes individual differences in temporal and spatial dimensions of eye movements into account and uses HMMs to model eye movement data, where the hidden states of the model directly correspond to regions of interest (ROIs) of eye movements.\n\nIn some embodiments, the instant methods summarize each person\u2019s eye movement patters using an HMM in terms of both person-specific ROIs and transitions among the ROIs and clusters the individual HMMs according to their similarities. Differences among models are quantitatively assessed using likelihood measures, which reflect similarities among individual patterns.\n\nIn\n\nsome\n\nembodiments, the EMHMM\n\nof the instant invention is applied to\n\nface\n\nrecognition to assess cognitive status ln executive and visual attention functioning and is used to assess cognitive decline or deficits. For example, using the EMHMM method of the instant invention, a perception style can be assigned to an individual based on the eye movements of the individual measured and evaluated using the EMHMM of the invention. Specific eye movement patterns measured with the EMHMM of the invention are associated with an analytic style of perception while other eye movement patterns are associated with a holistic style of perception. Furthermore, an analytic perception style is associated with improved recognition performance while a holistic perception style is associated with a cognitive decline in executive function and visual attention. Therefore, the methods of the instant invention can be used to detect cognitive performance or the decline thereof in an individual based on the individual\u2019s eye movement patterns. Importantly, the methods of the invention allow such cognitive performance assessment without the need for determining additional physiological parameters. Therefore, the methods of the invention are suitable for applications where detection of eye movement is the only data available such as in driver\n\nassessment and/or eye movement assessment via any computer or electronic device screen.\n\nIn some embodiments, the EMHMM with co-clustering of the instant invention is used to assess individuals that share similar eye movement patterns across stimuli. For\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n7\n\nexample, the EMHMM with co-clustering method of the instant invention comprises the generation of individual HMMs including personalized ROIs and transition probabilities between ROIs; the determination of common patterns through clustering; the quantification of similarities between patterns using data log-likelihoods; and the use of the similarity measures to examine the relationship between eye movement patterns and cognitive measures.\n\nsome embodiments, the EMHMM with co-clustering methods of the invention\n\nIn\n\nassesses individuals\u2019 eye movement patterns across stimuli during scene perception and defines an association of the eye movement patterns with foreground object recognition performance and cognitive abilities. In the EMHMM with co-clustering method of invention the co-clustering formulation ensures that the participant groups are consistent across all stimuli and the quantitative assessment using log-likelihood measures enables determination of the relationships of eye movement patterns with a variety of cognitive measures. Using the methods of the instant invention, types of scene images that induce larger individual differences in eye movement patterns can be determined and object recognition performance and cognitive abilities of individuals be measured based on eye\n\nthe\n\nthe\n\nmovement patterns during scene viewing.\n\nIn some embodiments, EMHMMs of the instant invention use HMMs to model eye movements, where the hidden states of the HMMs are directly associated with regions of interest (ROIs) of eye movements and the temporal dynamics of eye movements are determined in terms of ROIs and transitions among ROIs.\n\nIn\n\nsome\n\nembodiments,\n\nindividual\n\nHMMs\n\nare\n\nclustered\n\nto\n\ndetect representative\n\nstrategies among subjects and co-clustering is used to cluster subjects into groups according whether they use similar eye movement strategies across stimuli. Differences between group strategies are subsequently quantified using symmetric KL (SKL) divergence between group HMMs. Correlations between eye movement patterns and recognition performance and/or cognitive abilities are determined by quantifying the similarities of a subject\u2019s pattern the group strategies using a cluster score where log-likelihoods of a subject\u2019s data under different group strategies are employed. For example, individuals\u2019 eye gazes across images are clustered into two groups using the data mining technique co-clustering resulting two group HMMs for each image. Histograms of SKL divergence between the two general pattern groups are generated, where SKL quantifies the difference between the two group\n\nto\n\nthe\n\nto\n\nthe\n\nin\n\npatterns for each\n\nstimulus. Preferentially, images that induce larger differences between\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\nindividuals are used because larger individual differences are more likely to provide adequate variance among individuals for identifying atypical patterns.\n\nembodiments,\n\npreferred\n\nability\n\ncognitive\n\ntogether\n\nused\n\nIn\n\ntests\n\nare\n\nmovement pattern detection. For example, a Tower of London (TOL) test is used to test for measuring executive function, a Flanker Task test 1s used for measuring visual attention, and a Verbal & Visuospatial Two-Back Task test 1s used for measuring working memory. Using these methods of the invention it has been determined that eye movement pattern in scene perception are particularly suited to measure visual attention and executive function. Therefore, in preferred embodiments, the EMHMM with co-clustering method of the invention is used to measure visual attention and executive function ln individuals, where focused eye movement pattern indicates superior visual attention and executive function compared to an explorative eye movement pattern. In further preferred embodiments, the EMHMM with co-clustering method of the invention is used to measure object recognition performance, where an explorative eye movement patterns indicates superior object\n\nwith\n\nrecognition performance compared to a focused eye movements pattern.\n\nIn some embodiments, hierarchical HMMs are provided with at least two layers: a high-level HMM that acts like a switch that captures the transitions between cognitive states and several low-level HMMs that are used to learn the eye movement pattern of a cognitive state.\n\nAdvantageously, the models of the invention measure differences 1n eye movement patterns by summarizing individuals\u2019 eye movements using hidden Markov models, allow discovery of common eye movement patterns, and provide quantitative measures of eye movement pattern similarities that take both temporal and spatial information of eye movements into account. For example, the EMSHMM of the instant invention allows data analysis and modeling for cognitive tasks involving cognitive state changes and provides better predictions and analysis of subject behavior and cognitive processes. Furthermore, the methods of the invention allow data analysis ln tasks and cognitive behaviors that involve stimuli with different layouts, including, but not limited to, website viewing, information visual and other tasks.\n\nsystem usage,\n\nsearch, driving,\n\ncomplex\n\nIn specific embodiments of the invention, the data may be received from any type of eye tracking mechanism, whether optical, electrical, magnetic, or otherwise that calculate the eye gaze position of a human eye. In preferred embodiments, the eye movements are recorded using an Eyelink 1000 eye tracker, but any eye tracking mechanism may be used ln\n\neye\n\na\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n9\n\nperforming the instant invention without deviating from the spirit or scope of the invention. In preferred embodiments, eye tracking patterns are collected using video camera images.\n\nIn some embodiments, the data received from the eye tracking mechanisms are used in a Hidden Markov model. A Hidden Markov model is a statistical model used primarily to recover a data sequence that is not immediately observable. The model derives probability values for the unobservable data sequence by interpreting other data that depend on that sequence and are immediately observable.\n\nIn some embodiments, the Hidden Markov model of the instant invention represents the visible output, e.g., the raw data received from the eye-tracking mechanism as a randomized function of the invisible internal state, e.g., the cognitive state of a subject.\n\nThe Hidden Markov model can be used to model visual attention or eye movement changes corresponding to transitions in cognitive states during a complicated cognitive task.\n\nIn specific embodiments of the instant invention, HMMs are used directly to model eye movements, and hidden states of the HMMs are directly associated with ROIs of eye movements. This allows the determination of temporal dynamics of eye movements in terms of ROIs and transitions among ROIs specific to an individual.\n\nAdvantageously, the novel methods of the instant invention detect multiple cognitive states that occur during a task and the eye movement pattern associated with each cognitive state; thus allowing a better understanding of individual differences in eye movement patterns in real-life complicated cognitive tasks and predicting cognitive states from eye movement patterns.\n\nIn some embodiments, the instant invention provides switching HMMs (SHMMss) that are hierarchical HMMs with two layers containing a high-level HMM and several low-level HMMs. Each low-level HMM can be used to learn the eye movement pattern of a cognitive state. The high-level HMM acts like a \u2018switch\u2019 that captures the transitions between cognitive states. It does so by learning the transitions between the low-level HMMs.\n\nThe SHMMs of the instant invention are used to model transitions between cognitive states and their associated eye movement patterns in a cognitive task.\n\nIn some embodiments, the high-level states represent the cognitive states, whereas the low-level states correspond to ROIs over the stimul1.\n\nIn some embodiments, eye movements are modeled in a preference decision making task using face preference decision making in which participants are presented with two face images in each trial and asked to judge which face they like more. According to embodiments\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n10\n\nof the invention, at least two different eye movement patterns are observed one involving no fixation preference over either stimulus, which 1s related to exploration and information sampling, whereas the other involving a higher percentage of fixations over the to-be-chosen stimulus, which both reflects and shapes a participant\u2019s preferences.\n\nIn specific embodiments, SHMM 1s used to capture the dynamics in cognitive state and eye movement pattern. In some embodiments, one SHMM per individual is trained to summarize an individual\u2019s eye movement behavior during a task. In some embodiments, two or more SHMMs are trained using data from two different participant selections.\n\nIn preferred embodiments, individual SHMMs according to their similarities are clustered to discover common eye movement patterns in a task. Different eye movement patterns are associated with different decision-making behavior and can be detected and measured with the methods and systems of the instant invention. Thus, the methods of the instant invention can be used to infer, e.g., a decision making behavior from eye movement patterns.\n\nIn some embodiments, the EMHMM with co-clustering is used to discover common patterns in participant groups and analyze tasks or cognitive behavior involving stimuli with different layouts, including, but not limited to, website viewing, information system usage, and visual search. Further tasks or cognitive behaviors that can be analyzed using the methods of the invention are reading, picture viewing, video viewing, scene viewing, driving, navigation, and others.\n\nAdvantageously, the methods of the instant invention enable the assessment of more than one cognitive state in complex tasks and allow the identification of said cognitive states based on the measured eye movement patterns.\n\nIn preferred embodiments, specific eye movement patterns of a specific cognitive state are used to identify in a subject a certain cognitive state.\n\nIn further embodiments, systems are provided to perform the methods of the instant invention. In some embodiments, the systems comprise a camera and a processor configured to detect an eye position within a facial image captured by the camera. In some embodiments, the processor is further configured to perform the steps and/or algorithms of the instant invention. In preferred embodiments, the processor is configured to read and process data from the camera according to a Matlab toolbox to analyze HMMs, EMHMMs, and EMSHMMs.\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\nI]\n\nIn some embodiments, an electronic device is provided that comprises a camera and a processor configured to detect a center position of an eye within a facial image captured via the camera in response to detecting an eye within the facial image; determine an eye gaze position based on the center position and a pupil position; analyse the eye gaze position in consecutively captured facial images; and measure an eye movement patterns based on the eye gaze positions of the consecutively captured facial images.\n\nIn specific embodiments, the processor is further configured to generate an exploration transition matrix and Gaussian emission for each subject. In some embodiments, the processor is further configured to cluster or summarize HMMs into single groups using a Variational Hierarchical Expectation Maximization algorithm, wherein the HMMs are clustered according to their probability distributions. In other embodiment the processor 1s further configured to assimilate external user-entered data to associate certain eye movement patterns with certain external task criteria or cognitive state criteria.\n\nIn further embodiments, the processor is configured to determine the number of fixations of eye movements. Advantageously, from the eye movement pattern and fixation probability distributions, the systems of the instant invention can calculate a probability of an individual being ln a certain cognitive state.\n\nIn some specific embodiments, the system of the invention can be used to infer an individual\u2019s preference for a certain choice based on the eye movement patterns.\n\nIn some embodiments, the system of the invention can be used to determine an individual\u2019s cognitive style based on the eye movement patterns of the individual. In specific embodiments, the processor of the system is configured to calculate a log likelihood of a eye movement pattern and assign a certain cognitive style to the measured eye movement pattern if it matches the eye movement pattern of a representative group of individuals having said cognitive style.\n\nFurther provided are methods for inferring an individual\u2019s preference choice using eye movement patterns. In specific embodiments, the individual\u2019s preference choice is inferred from the individual\u2019s eye movement fixations measured in different time intervals from the beginning of a trial to the making of a preference choice at the end of the trial. In specific embodiments, the individual\u2019s preference choice 1s inferred from the individual\u2019s eye movement fixations measured during the last 25% of the time interval from the beginning of a trial. In preferred embodiments, the individual\u2019s preference choice 1s inferred from the individual\u2019s eye movement fixations measured during the last 15% of the time interval from\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n12\n\nthe beginning of a trial. In more preferred embodiments, the individual\u2019s preference choice is inferred from the individual\u2019s eye movement fixations measured in the last 10% of the time interval from the beginning of a tnal. In most preferred embodiments, the indiviudal\u2019s preference choice is inferred from the individual\u2019s eye movement fixations measured in the last 5% of the time interval from the beginning of a trial.\n\nFurther provided are methods for inferring an individual\u2019s preference choice using eye movement patterns measured at different time intervals before a choice response. In specific embodiments, the individual\u2019s preference choice 1s inferred from the individual\u2019s eye movement fixations measured in the last 10 seconds before the choice response. In preferred embodiments, the individual\u2019s preference choice is inferred from the individual\u2019s eye movement fixations measured in the last 8 seconds before the choice response. In other preferred embodiments, the individual\u2019s preference choice is inferred from the individual\u2019s eye movement fixations measured ln the last 6 seconds before the choice response. In more preferred embodiments, the individual\u2019s preference choice is inferred from the individual\u2019s eye movement fixations measured in the last 4 seconds before the choice response. In most preferred embodiments, the individual\u2019s preference choice is inferred from the individual\u2019s eye movement fixations measured in the last 2 seconds before the choice response.\n\nAdvantageously, the instant methods enable the measurement of individual differences in eye movement patterns and cognitive styles during complex cognitive tasks. Furthermore, the instant methods allow the measurement of an individual\u2019s preferences significantly earlier than previously used methods. Specifically, the instant methods enable a measurement of an individual\u2019s choice preference with only the first 75% of fixations that precede the choice.\n\nAdvantageously, the accuracy in inferring an individual\u2019s preference choices using the methods of the instant invention, particularly using EMSHMM, is significantly higher than that using, e.g., EMHMM.\n\nFurthermore, the EMHMM with co-clustering of the invention can be used to infer an individual\u2019s cognitive style and/or cognitive abilities.\n\nImportantly, the methods of the instant invention enable inferring an individual\u2019s preference choice and cognitive style/abilities from eye gaze information alone using EMSHMM and/or EMHMM with co-clustering without the need for further additional physiological information, making the instant methods and systems especially suited for applications where eye movement measurements are the only data obtained from an\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n13\n\nindividual to determine the individual\u2019s cognitive style, inferring an individual\u2019s preference choice and/or measuring an individual\u2019s cognitive abilities.\n\nAll patents, patent applications, provisional applications, and publications referred to or cited herein are incorporated by reference in their entirety, including all figures and tables, to the extent they are not inconsistent with the explicit teachings of this specification.\n\nFollowing are examples that illustrate procedures for practicing the invention. These examples should not be construed as limiting. All percentages are by weight and all solvent mixture proportions are by volume unless otherwise noted.\n\nMATERIALS AND METHODS\n\nEXAMPLE 1- Use of Eye Movement analysis with Hidden Markov Models (EMHMM) in face recognition\n\nUsing the EMHMM approach, the inventors previously detected holistic and analytic\n\nmovement patterns in face recognition through clustering of eye movement patterns in young and 34 older adults (Chan, Chan, Lee, and Hsiao (2018)). In the representative models, ellipses show ROIs corresponding to 2-D Gaussian emissions (FIGURE 1A). Prior values show the probabilities that a trial starts from the ellipse/ROI. Transition probabilities show probabilities of observing a particular transition to the next ROI from the current ROI. small images on the right show the assignment of actual fixations to the ROIs and corresponding fixation heatmaps. Note that the clustering algorithm (Coviello, Chan, Lanckriet, 2014) summarizes individual models in a cluster into a representative HMM using pre-specified number of ROIs, which may result in overlapping ROIs. Here the number ROIs in the representative models was set to 3, the median number of ROIs in the individual models. FIGURE 1B shows the frequency of young and older adults adopting holistic analytic patterns: significantly more older adults adopted holistic patterns and more young adults adopted analytic patterns. FIGURE 1C shows group fixation heat maps of older\n\neye\n\nthe\n\nThe\n\na\n\nyoung adults.\n\nEXAMPLE 2- Use of EMHMM with different stimuli\n\nBecause the EMHMM method providing quantitative measures of individual differences in eye movement pattern is limited to tasks where stimuli have the same feature\n\n34\n\nthe\n\n&\n\nof\n\nand\n\nand\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n14\n\nlayout (e.g., faces), a novel method was developed that combines EMHMM with the data mining technique co-clustering to detect participant groups with consistent eye movement patterns across stimuli for tasks involving stimuli with different feature layouts. This novel method was applied to scene perception. Using the novel EMHMM with co-clustering method of the invention, explorative (switching between foreground and background information) and focused (mainly on foreground) eye movement strategies were detected. Explorative patterns were associated with better foreground object recognition performance whereas those with focused patterns had better feature integration in the flanker task and more efficient planning in the Tower of London (TOL) task. Advantageously, these novel methods can be used to employ eye tracking as a window into individual differences in cognitive abilities and for screening of cognitive deficits.\n\nScene perception involves complex and dynamic perceptual and cognitive processes\n\nthat can be influenced by both observers\u2019 goals and diverse scene properties at multiple levels. Eye movements during scene perception reflect the complexity of cognitive processes involved, and thus can potentially provide nch information about individual differences in perception styles and cognitive abilities. For example, it was observed that Westerners made more eye fixations on foreground objects whereas Asians looked at the backgrounds more often, and this difference was reflected in their foreground object recognition performance. This phenomenon has been linked to cultural differences in perception styles: Westerners are more likely to attribute the cause of an event to isolated objects (analytic style), whereas Asians are more likely to attribute the cause of an event to the context of the event (holistic style). However, these effects have not been found consistently and some studies found no difference between American and Chinese participants either in foreground object recognition performance or in number of fixations and durations to the foreground and background. Thus, it remains unclear whether eye movement patterns consistently reflect cultural differences in perception styles and individual differences in cognitive abilities.\n\nPrevious studies have attempted to use machine-learning methods to infer characteristics of the observer from eye movement data. These studies typically used classifiers to discover eye movement features important for distinguishing two or more observers. However, the classifiers only look for features important for separating the observers, and do not provide any information about eye movement patterns associated with a particular observer. The EMHMM approach, where HMM is a type of time-series statistical model, has been developed by the inventors. This approach takes individual differences in\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n] >\n\ntemporal and spatial dimensions of eye movements into account.\n\nEMHMM\n\nassumes\n\nthat\n\nduring a visual task, the currently fixated region of interest (ROI) depends on the previously\n\nfixated ROI Thus, eye movements may be considered a Markov stochastic process, which can be better understood using HMMs. Other studies have used HMMs/probabilistic models for modeling eye movement/visual attention and cognitive behavior, where hidden states of the models represented cognitive states. In contrast, EMHMM of the instant invention directly uses HMMs to model eye movement data, and hidden states of the models directly correspond to ROIs of eye movements. To account for individual differences, each person\u2019s eye movement pattern 1s summarized using an HMM in terms of both person-specific ROIs and transitions among the ROIs, estimated from the individual\u2019s data using a variational Bayesian approach that can automatically determine the number of ROIs. Individual HMMs are clustered according to their similarities to reveal common strategies. Differences among models are quantitatively assessed using likelihood measures, which reflect similarities among individual patterns. Thus, this method is particularly suitable for examining individual differences in eye movement patterns and their associations with other cognitive measures. Furthermore, because EMHMM is a Bayesian probabilistic model it works well with limited amounts of data; in contrast to deep learning methods that require large amounts of data to train effectively. The EMHMM of the instant invention has been applied to face recognition research, discovering novel findings thus far not revealed by existing methods. For example, two common eye movement strategies were identified: analytic (more eyes-focused) and holistic nose-focused (FIGURE 1A). Asians and Caucasians did not differ significantly in the frequency of adopting the two strategies, suggesting little modulation from culture (Chuk al., 2017). Analytic patterns were associated with better recognition performance, suggesting that retrieval of diagnostic information (1.e., the eyes) is a better predictor for performance (Chuk, Chan, & Hsiao, 2017). In contrast, older adults adopted holistic patterns compared to\n\net\n\nyoung\n\nadults,\n\nand\n\ntheir\n\nholistic\n\npatterns\n\nwere\n\nassociated\n\nwith\n\nlower cognitive\n\nstatus\n\nparticularly in executive and visual attention functioning: the more holistic (nose-focused) the\n\npattern, the lower the cognitive status (Chan, Chan, Lee, Hsiao, 2018). This correlation was\n\nreplicated with new participants viewing new face images using the group HMMs discovered\n\nfrom the old participants, suggesting the possibility of developing representative HMMs from\n\nthe population for cognitive screening purposes. Similarly, insomniacs\u2019 impaired ability for\n\nfacial\n\nexpression judgments\n\nwas\n\nassociated\n\nwith\n\nusage\n\nof\n\na\n\nless\n\neye-focused pattern,\n\nsuggesting that impaired visual attention control may account for compromised emotional\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n16\n\nface perception in insomniacs (Zhang et al., 2019). Together these results suggest the possibility of using eye tracking as a screening assessment for cognitive decline or deficits.\n\nEMHMM has been limited to tasks involving stimuli with the same feature layout so that discovered ROIs correspond to the same features across stimuli. For tasks where stimuli have different layouts, the inclusion of perceived images as features in the ROI representations has been proposed but, while this method has improved discovery of ROIs, it does not work well when features among stimuli differ significantly, such as in scene perception.\n\n10 EXAMPLE 3- Novel EMHMM with co-clustering\n\nThe instant invention provides a\n\nnew\n\nmethod that models each participant\u2019s eye\n\n] >\n\n20\n\nmovements when viewing a particular stimulus with one HMM, and uses the data mining technique co-clustering (See, e.g., Govaert & Nadif, 2013) to detect subjects sharing similar eye movement patterns across stimuli. The co-clustering formulation ensures that the subject groups are consistent across all stimuli. The result is a grouping of subject and their representative HMMs for each stimulus (FIGURE 2). The similarity between an individual\u2019s eye movement patterns across stimuli and the representative HMMs are quantitatively assessed using log-likelihood measures as in the existing approach. The results are used to examine the relationship between eye movement patterns and other cognitive measures. Using this method, individual differences in eye movement patterns during scene viewing were examined and it was determined (1) what types of scene images (animals in nature vs. vehicles in a city) induce larger individual differences in eye movement patterns, and (2) how eye movement patterns during scene viewing are associated with subsequent foreground\n\nobject recognition performance and cognitive abilities among subjects.\n\n25\n\n30\n\nIn a study, 61 Asian participants (35 females) aged 18-25 (M = 20.77, SD = 1.70) were recruited from the University of Hong Kong. All participants had normal or corrected- to-normal vision. The materials viewed consisted of 150 scene images with animals in a natural environment and 150 scene images with vehicles in an urban environment. Images with different numbers of foreground objects, feature layouts, and locations of foreground objects were used to increase stimulus variability to provide adequate opportunities to elicit individual differences in eye movement pattern.\n\nThe scene perception task consisted of a passive viewing phase and a surprise recognition phase (following Chua et al., 2005). During the passive viewing phase, for each\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n17\n\nscene type (animals vs. vehicles), participants were presented with 60 images one at a time,\n\neach for 5 seconds, and rated from 1 to 5 how much they liked the image. During the surprise recognition phase, for each scene type, participants were presented with 60 images with old foreground objects, with half of them presented in the same old background and the other half in a new background, together with the same number of lure images with new objects in a new background. Participants were presented with the images one at a time, and they judged whether they saw the foreground object during the passive viewing phase. The image stayed on the screen until response. In both phases the animal and vehicle scene images were presented in 2 separate blocks, with the block order counterbalanced across participants. Participants\u2019 eye movements were recorded using an EyeLink 1000 eye tracker, with a chinrest to minimize head movements. Each trial started with a fixation cross at the screen center. The experimenter initiated the image presentation when a stable fixation was observed at the fixation cross. An image was then presented at the screen center, spanning 35\u00b0 x 27\u00b0 of visual angle at a viewing distance of 0.6 m. Before each block, a 9-point calibration procedure was performed. Re-calibration took place whenever drift correction error exceeded\n\n1\u00b0 of visual angle.\n\nIn addition, participants performed 3 cognitive tasks to examine whether their eye movement patterns were associated with cognitive abilities:\n\n(1) Tower of London (TOL) task (see, e.g., Phillips et al., 2001) for testing executive function/planning abilities: In each trial, participants saw 3 beads randomly placed on 3 pegs as the starting position, together with the target position. They were asked to move | bead at a time to reach the target position as quickly as possible with the minimum number of moves and to plan the moves in mind before execution. In total there were 10 trials. The total number of extra moves, number of correct trials, total planning time before executing the first move, and total execution time for the moves were measured.\n\n(2) Flanker task (see, e.g., Ridderinkhof, Band, & Logan, 1999) for testing selective attention: Participants judged the direction of an arrow flanked by 4 other arrows. In the congruent condition, the flanking arrows pointed at the same direction as the target arrow, whereas in the incongruent condition, the flanking arrows pointed at the opposite direction. In the neutral condition, the flankers were non-directional symbols.\n\n(3) Verbal and visuospatial 2-back task for assessing working memory capacity (see, e.g., Lau et al., 2010): In each trial, participants judged whether the presented symbol/symbol\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n18\n\nlocation was the same as the one presented 2 trials back in the verbal/visuospatial task respectively.\n\nParticipants\u2019 eye movements during the passive viewing phase were analyzed using\n\nEMHMM with co-clustering (adapted from htip://visal.cs.cityu.edu hk/research\u2019embnini). Each participant\u2019s eye movements for viewing each stimulus were summarized using HMM. Individual HMMs for viewing each stimulus were clustered to discover representative strategies among the participants. Co-clustering was then used to cluster participants into 2 groups according to whether they used similar eye movement strategies across the stimuli (FIGURE 2). To examine whether natural or man-made images could induce larger individual differences in eye movement pattern, the difference between group strategies for each stimulus were quantified via the symmetric KL (SKL) divergence between the two group HMMs. The SKL is given by (KLj2+KL21)/2, where KL}, 1s the KL divergence between the Group 1 HMM and the Group 2 HMM using Group 1 data (Chuk, Chan, Hsiao, 2014), and vice-versa for KL; (KL 1s not symmetric). SKL measures were compared for natural and man-made images and the characteristics of the images that typically led\n\nlarger SKL were determined.\n\nIt was also examined whether the 2 groups of participants differed in performance in foreground object recognition and the cognitive tasks. To examine the correlations between eye movement pattern and recognition performance/cognitive abilities, the similarity of a participant\u2019s pattern to the group strategies were examined using a cluster score, CS = (|Li|- ILo}) / (Li|+|L2|), where Li and L are the log-likelihoods of a participant\u2019s data being generated under Group | and Group 2 strategy, respectively (Chan et al., 2018). Larger positive values of CS indicate higher similarity to Group 1, and smaller negative values indicate similarity to Group 2.\n\nEXAMPLE 4\u2014Eye Movement Strategies in Scene Perception determined using EMHMM with co-clustering\n\nThe data obtained as described in Example 3 from 61 participants were clustered into 2 groups using EMHMM with co-clustering on the 120 image stimuli. Group 1 contained 37 participants and Group 2 contained 24 participants. The co-clustering model estimates 2 HMMs for each image stimulus, corresponding to the strategies of Groups 1 and 2. For each image, the difference between the strategies was measured in SKL. As shown in FIGURE 3, animal images induced larger differences In eye movement strategies between the 2 groups\n\nan\n\n2\n\n&\n\nto\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n19\n\nthan vehicle images, t(118) =\n\n5.626; p < 0.0001. FIGURE 4A presents 4 example images\n\nand their corresponding HMMs for the 2 groups. FIGURE 4b1 and 4b2 show two examples of eye movement strategies with large SKL difference between the 2 groups. In particular, Group 2 focused more on the foreground object, while Group 1 explored the image, looking at both the foreground object and the background. In addition, while looking at an animal, Group 2 focused more on the eyes, whereas Group 1 looked at the nose (FIGURE 4c). FIGURES 4b3 and 4b4 show examples where the 2 groups had similar eye movement strategies. In general, larger differences in eye movements occurred on images where the foreground object (animal or car) was salient as compared with the background, e.g., an animal among trees, or a car on a road. This indicates that animals are more salient than vehicles, and that animal images generally induce larger differences in eye movement strategies than vehicle images. In contrast, images with cluttered backgrounds and non- interesting foreground objects typically induced similar eye movement strategies. Group 1 and 2 strategies are referred to as the explorative and focused strategy, respectively, and the cluster score between the two strategies as the Explorative-Focused (EF) scale. Indeed, participants using the explorative strategy had a larger average number of fixations, t(59) = 3.793, p<.001, and longer saccade lengths, t(59) = 4.881, p < .001, than those using the focused strategy (Table 1 below), and the more similar their eye movement pattern to the explorative strategy (in EF scale), the larger the average number of fixations, r(60) = .477, p < .001, and saccade length, r(60) = .544, p < 0.001 (FIGURES 5a-e). As EMHMM does not use sequence length information, the difference in average number of fixations emerged the result of the These results consistent with the\n\nnaturally as\n\nclustering.\n\nwere\n\ninterpretation\n\nthat Group | was more explorative than Group 2.\n\n10\n\n15\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n20\n\nTable 1 Group 1 Group 2 Explorative Focused Task Measure mean std mean std \u5df2 t Avg Number of Fixations 14.06 1.659 12.43 1.602 0.000 3.793 Avg Fixation Duration 312.4 43.23 335.9 66.77 0.100 | -1.672 Avg Saccade Length 91.92 | 13.87 | 74.34 | 13.56 | 0,000 | 4.881 Image Preference Rating 2.545 | 0.485 | 2.890 | 0.496 | 0.009 | -2.689 FOR Reaction Time (RT) 1160 303.4 1216 412.4 0.545 | -0.609 FOR d\u2019 Original Background 1.695 | 0.556 | 1.050 | 0.922 | 0.001 3.410 FOR d New Background 0.802 | 0.302 | 0.531 0.406 | 0.004 | 2.986 FOR d Overall 1.203 0.356 0.766 0.638 0.001 3.434 N-back Verbal d\u2019 2.867 0.550 2.640 0.725 0.169 1.391 N-back Verbal RT 7318 169.2 706.2 127.6 0.529 0.633 N-back Spatial d 2.244 0.764 2.201 0.594 0.819 0.230 N-back_Spatial RT 780.2 | 220.3 | 742.9 | 146.2 | 0.468 | 0.730 Flanker Incongruent Acc 89.26 14.79 | 90.21 19.56 | 0.830 | -0.216 Flanker Congruent Acc 98.85 | 2.091 | 99.79 | 0.706 | 0.038 | -2.121 Flanker Neutral Acc 97.97 | 3.272 | 97.81 | 3.240 | 0.852 | 0.188 Flanker Overall Acc 95.36 5.331 95.4 6.677 0.710 | -0.374 Flanker Incongruent RT 414.8 47.03 4153 99.80 | 0.980 | -0.025 Flanker Congruent RT 362.5 35.08 373.1 36.37 0.261 | -1.135 Flanker Neutral RT 366.0 37.88 373.6 36.39 0.440 | -0.778 Flanker Overall RT 381.1 38.23 387.4 48.54 0.579 | -0.558 TOL Total # Moves 24.97 17.00 26.29 18.79 0.779 | -0.282 TOL # Correct Trials 5.194 2.550 4.875 2.983 0.659 0.444 TOL Total Exccutaion Time (s) 177.7_| 69.23 | 181.8 | 77.44 | 0.829 | -0.216 TOL Preplanning Time (s) 137.0 71.33 99.21 36.27 0.020 | 2.395\n\nTable\n\nof\n\nstatistics and\n\ntask\n\nperformance between the\n\n1: Comparison eye movement cognitive explorative and focused strategies (FOR: Foreground Object Recognition).\n\nEXAMPLE 5\u2014Association between Eye Movement Pattern and Recognition Performance/Cognitive Abilities using EMHMM with co-clustering\n\nParticipants using the explorative strategy also had significantly lower image preference ratings, t(59) = -2.689, p = .009; better foreground object recognition in d\u2019, t(59) = 3.434, p = .001; lower accuracy in congruent trials of the flanker task, t(59) = -2.121, p = .038; and longer total planning time before execution in the TOL task, t(58) = 2.395, p = .02 (Table 1; one participant\u2019s TOL result was missing due to technical problem). Note that their advantage in foreground object recognition performance was significantly larger when the foreground object was presented with the original background than with a new background,\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n21\n\nF(1, 59) = 9.126, p = .004, although the advantage was significant in both cases (Table 1). Consistent with these findings, correlation analysis showed that participants\u2019 eye movement pattern similarity to the explorative strategy (EF scale) was correlated negatively with preference ratings, r(60) = .301, p = .018; positively with scene recognition performance in d\u2019, r(60) =.381, p = .002; and marginally positively correlated with TOL planning time before executing moves, r(59) = .223, p = .08 (FIGURES 5a-e). Taken together these findings indicate that the explorative strategy is associated with lower image preference rating, better scene recognition performance, decreased facilitation from consistent surrounding cues in the flanker task, and less efficient planning in the TOL task.\n\nOverall, for images containing animal faces, participants adopting the focused strategy (analytic style) looked more to the eyes of the animal faces, suggesting engagement of local attention. In contrast, those adopting the explorative strategy (holistic style) looked more to the animal face center, suggesting engagement of global processing. These results are generally consistent with Caucasians\u2019 analytic style that is associated with focusing on the foreground object in scene perception and the eyes in face recognition, whereas Asian\u2019s holistic style is associated with looking more often at the background and the face center. However, using EMHMM, the inventors showed that Asians and Caucasians did not differ in the frequency of adopting the eyes-focused or nose-focused strategy, suggesting little modulation from culture on eye movement pattern when individual difference was taken into account (Chuk et al. 2017).\n\nIt was observed that participants adopting the explorative strategy had better foreground object recognition performance than those using the focused strategy, regardless of whether the foreground object appeared in a new or old background; this advantage was larger when the foreground object appeared in the old than a new background. The finding suggests that a more explorative eye movement strategy during scene perception may be beneficial for remembering the foreground object due to more retrieval cues available through exploration. Consistent with this suggestion, it has been shown that associative processing 1s inherent in scene perception suggesting that an explorative strategy may facilitate associative processing and consequently enhance scene memory. This finding is in contrast to the face recognition literature, where the eye-focused strategy, which 1s associated with engagement of local attention and the focused strategy reported here, was reported to lead to better recognition performance due to better retrieval of diagnostic features, the eyes.\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n22\n\nIn contrast, using the methods of the invention, it was shown that those adopting focused strategy performed better in the congruent trials of the flanker task and had shorter preplanning time in the TOL task than those using the explorative strategy. The advantage observed in the congruent but not incongruent or neutral trials in the flanker task suggested that these participants might have better feature integration abilities. The shorter preplanning time in the TOL task without differences In number of moves, correct trials, or execution time suggested that they had more efficient planning abilities. Thus, because participants adopting the focused strategy preferred to look at the eyes of the animals, the focused strategy may be related to a more eyes-focused strategy in face recognition, which was associated with better face recognition performance, visual attention (the trail making task), and executive function (the TOL task; Chan et al., 2018).\n\nthe\n\nUsing the methods of the invention it was also observed that images with a salient foreground object relative to the background tended to induce large individual differences in eye movement patterns, and that animal images induced larger individual differences than vehicle images (FIGURE 3). This phenomenon may be due to humans\u2019 category specific attention to animals that made them more salient than vehicles or other object types, providing better opportunities to induce the difference between the explorative and focused strategies. This finding has important implications for the possibility of using eye tracking to provide screen tools for cognitive disorders because images that induce larger individual differences will be more likely to provide adequate variance among individuals for identifying atypically patterns.\n\nshow that the novel EMHMM with\n\nit\n\nmethod of the\n\nIn\n\nconclusion, was co-clustering invention can effectively summarize and quantitatively assess individual differences in eye movement strategies in tasks involving stimuli with different feature layouts, and in turn lead to new discoveries not yet found by existing methods. By applying the novel method to scene perception, the explorative and focused strategies among Asians were detected. Whereas the explorative strategy was associated with better foreground object recognition performance, the focused strategy was associated with better feature integration and planning abilities. Also, images with a salient foreground object relative to the background induced larger individual differences In eye movement patterns. These results have important clinical and educational implications for the use of eye tracking in cognitive deficit detection and cognitive performance monitoring. Advantageously, the novel EMHMM with co-clustering method of\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n23\n\nthe\n\ninvention can be applied to a large variety of visual tasks and be instrumental in using movement patterns to understand cognitive behavior.\n\neye\n\nEXAMPLE 6- Eye Movement analysis with Switching Hidden Markov Models (EMSHMM) in the detection of different cognitive states using a face preference decision-making task\n\nIn previous probabilistic approaches to\n\nmodel\n\nvisual\n\nattention\n\nin\n\na\n\ncomplicated\n\ncognitive task, the hidden states of the models represented cognitive states, thus capturing temporal dynamics of cognitive state transitions but not the dynamics of eye movements. order to measure multiple cognitive states that occur during a task and the eye movement patterns associated with each cognitive state an EMHMM with hierarchical HMMs 1s used. For example, the hierarchical HMM with two layers contains a high-level HMM and several low-level HMMs. Each low-level HMM acts like a \u201cswitch\u201d that captures the transitions between cognitive states. It does so by learning the transitions between the low-level HMMs. This method of the instant invention is called a Switching HMM (SHMM). The SHMM the invention is used to capture the dynamics in cognitive state and eye movement pattern following an existing EMHMM approach and training one SHMM per participant summarize the participant\u2019s eye movement behavior during a task. The individual SHMMs are then clustered according to their similarities to detect common eye movement patterns task and examine the different movement patterns associated with different decision-\n\nthe\n\nIn\n\nof\n\nto\n\nin\n\nthe\n\neye\n\nmaking behaviors.\n\nApplying the novel EMSHMM of the instant invention, eye movement data were\n\ncollected from a face preference decision-making task and two participant groups were compared. The preference decision-making task was a two-alternative-forced-choice task, which contained two parts. In part 1, participants were presented with 120 (female and male) computer generated, bald faces. They were asked to rate, from 1 to 7, how attractive the faces were. These ratings were used for pairing stimuli for part 2 and the eye movements were not analyzed. After part 1 was finished, the faces of the same gender that received similar ratings were paired to form 60 pairs as the stimuli In part 2. In part 2, in each trial, each pair of faces was shown on the screen with one on the left and one on the nght. Participants were required indicate which face they preferred. There was no time limit. Participants could move their eyes freely to compare the two images. They were told to press a button to indicate which face (left or right) they preferred once they had made their decision. Eye movements were\n\nto\n\nrecorded using an Eyelink 1000 eye tracker. In data acquisition, fixation location information\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n24\n\nwas extracted using Eyelink Data Viewer. Saccade motion threshold was 0.15 degree of visual angle; saccade acceleration threshold was 8000 degree / square second; saccade velocity threshold was 30 degree / second. These are the EyeLink defaults for cognitive research.\n\nEXAMPLE 7\u2014Switching Hidden Markov Model\n\nA standard hidden Markov model (HMM) contains a vector of prior values of the hidden states, a transition matrix of the hidden states, and a Gaussian emission for each hidden state. The prior values indicate the probabilities of the time-series data starting from the corresponding hidden states. The transition matrix indicates the transition probabilities between any two hidden states. The Gaussian emissions indicate the probabilistic associations between the observed time-series data and the hidden states. In the previous EMHMM approach (Chuk et al., 2014), the hidden states corresponded to the regions of interest (ROIs), the emissions were the eye fixation locations, and the emissions in an ROI were represented as a 2-D Gaussian distribution (See FIGURE 1a).\n\nIn contrast to a standard HMM, a switching HMM (SHMM) contains two levels of hidden state sequences; the low-level hidden state sequence models the temporal pattern of the time-series data following a standard HMM, while the high-level hidden state sequences indicate the transitions between the HMM parameters used by the low-level hidden state sequence. Formally, Z,; \u20ac {1,...,K} are the low-level hidden states, and sa \u20ac {1,...,S} the high-level hidden states, and x,; the observations, where 7 indexes the sequences and f indexes time. Both the high-level state sequence and the low-level state sequence are 1*-order Markov chains. The prior probability and transitions of the low-level hidden state depends on the current high-level state,\n\nP(Sn) = P(Sna) M2, P(sn,e| Snt-1) (1)\n\nP(Zn|Sn) = P(Zn1 Sn) Te D(Zn.t| Znt-1v Snt)> (2)\n\nT,, ls the length of the n-th sequence. The high-level state sequence is parametrized prior vector p and transition matrix B,\n\nwhere\n\nthe\n\nby\n\nP(Sn1 \u4e00 j) = Pj P(Snt =J ISnt\u20141 =j)= bj jr.\n\n(3)\n\nGiven that the high-level state is Sa = j, the low-level state sequence is parametrized by prior vector mw) and transition matrix AY )\n\nthe\n\nP(Zn1 =k|sn1 =J) = Tri\uff0c P(Zne =k |Zne-1 =k Sue =f) = ay?, (4)\n\n10\n\n15\n\n20\n\n25\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n25\n\nThe emission densities are Gaussians and depend only on the low-level hidden state (i.e., they are shared among high-level states),\n\nPO ntl2ne =k) = Nn clHe Ax\u2019),\n\nwhere fx, Aj\" are the mean vector and covariance matrix of the Gaussian. It was assumed that the number of low-level states for each high-level state is the same. The joint probability model for the SHMM is\n\np(X, Z, S)\n\nMes [P (S21)? Znalsn1)P%na|Zna) Tite P(Snel Sne-)PZnel2Znt-v Sne)P Ane lZn,e)]- (6)\n\n=\n\nIn practice, the SHMM can be turned into a standard HMM by combining the high-\n\nevel and the low-level hidden state variables into a single hidden state variable, whose values are the Cartesian product of the low- and high-level state values. Here, it was assumed that he low-level states were shared among the high-level states (), and thus the number of low- evel states (K) was the same for each high-level state. Consequently, the equivalent standard HMM had S*K augmented hidden states Z,,. The augmented states took a value pair (7,4), where j indicates the high-level state and & indicates the low-level state. The transition robabilities and the prior values therefore were defined as,\n\n(9\n\n(Zn = Gk) Enea =U, i) = agingte') = by pad, (7)\n\nPGna = OK) = fjD = pep. (8)\n\nThus the relationship between the augmented hidden states and the two separate levels of hidden state sequences were defined as\n\nPn) = PG Sn) = PGnlsn)PCn) = P(Zn1 lSna)P (san) Te, P(Zn,e| Znt- Snt)P(Sne ISne\u20141) = P(Sna Zna) Tite P(Sre> zi Snt-v 2Znt-1) (9)\n\nThe transition matrix and the prior vector had block structures,\n\na= Peo (40) bp AMD, AP ,\n\n(1) = ee (it) p27\n\nIn the implementation, the high-level hidden states represent the cognitive states, whereas the low-level hidden states correspond to ROIs over the stimuli. The high-level state\n\n(5)\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n26\n\nhas its\n\ntransition\n\nwhich\n\nthe transitions between\n\nsequence own matrix, governs cognitive states. The low-level states (ROIs) are shared among the high-level states. Taking the preference decision-making task as an example, it was assumed that participants have two cognitive states: an exploration period that involves information sampling without preference to a specific stimulus, and a preference-biased period where preference is formed and eye movement behavior can be influenced by the preference. A simplified decision process was assumed where a participant started in the exploration period, and that once the participant had sampled enough information in the exploration period, they transitioned to the preterence-biased period and could not transition back to the exploration period. That is, once the preference-biased period was entered it could not be exited until the response decision. To examine a gaze cascade effect, that is a gaze bias towards the later-selected object (see, e.g., Shimojo et al. 2003), it was assumed that the low-level HMM had two ROIs, each corresponded to a stimulus for choice. FIGURE 6 illustrates an example model summarizing a participant\u2019s eye movement pattern. As shown in the figure, the high-level HMM consisted of two cognitive states as its hidden states: exploration and preference-biased periods. The blue arrows indicate the transitions between the two states, and the numbers indicate transition probabilities. Eye movements within each state were modeled with a low-level HMM, whose hidden states represent ROIs of eye movements. The red arrows represent transitions between ROIs. The two cognitive states have the same ROIs but different\n\ntransition probabilities.\n\nEXAMPLE 8\u2014Training individual SHMMs\n\nAn Expectation-Maximization (EM) algorithm was performed to estimate the SHMM parameters. In the Expectation step (E-step), the responsibilities were calculated using the standard forward-backward algorithm with the block transition matrix, initial state vector, and emission densities. In the Maximization step (M-step), the prior and pairwise responsibilities were summed over the high-level and the low-level states, respectively, to yield the parameter updates for both the high-level states and the low-level states.\n\nFor example, the prior responsibilities were summed over the low-level hidden states for each of the high-level state to yield the parameter updates for the low-level state sequence, and then they were summed over the high-level states to yield the parameter updates for the high-level state sequence. Similarly, the pairwise responsibilities were summed over the low- level hidden states for each high-level state to yield the updates for each low-level transition,\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n27\n\nthen were summed over the high-level hidden states to yield the updates for the switching (transition) matrix of the high-level state sequence.\n\nand\n\nFor each participant, two SHMMs were trained;\n\none using the data from the left-\n\nselected trials and one using the data from the right-selected tnals. The two SHMMs were combined into one for each individual as follows. Preliminary analysis indicated that the exploration periods of the left-selected and right-selected trials were similar. In other words, the eye movements during exploration periods were consistent regardless of which side was selected. Thus, the transition matrices for the exploration period of the left-selected and right- selected models were directly averaged together. For the preference-biased period, the two preferred-side parameters and the two non-preferred-side parameters were averaged, essentially normalizing the right-selected trials into left-selected trials. In order to focus the analyses on the transition between the stimuli during preference decision making, only two Gaussian emissions were used per model, one on each side, for the low-level states (FIGURE 6). For SHMM estimation, one Gaussian centered over each stimulus was initialized, with covariance that covered the stimulus. Thus, it could be considered here that the low-level states of the model were pre-specified (and thus not hidden), since it could be determined with good confidence which stimulus was viewed. The advantage of using Gaussian rather than discrete emissions is that it can be easily extended to analyses that explore more ROIs (Le., more hidden lower-level states) on each stimulus. Two high-level hidden states were used to reflect that the participants would transition from the exploration period to the\n\npreference-biased period during a trial.\n\nFor SHMM estimation, the transition matrices of the high-level state were initialized as [0.95, 0.05; 0.0, 1.0] and high-level prior as [1.0, 0.0], which encodes the assumed behavior of starting ln the exploration period and staying there (0.95), and then eventually transitioning to the preference-biased period (0.05) and not back (0.00). During training, this initialization causes the probability of transitioning from the preference-biased to the exploration period to stay at O (since all potential sequences that transition from the preference-biased to the exploration period are given probability zero). The transition matrices of the low-level states were initialized as uniform distributions. After the initialization, the Gaussian ROIs and transition probabilities were updated in the EM algorithm.\n\nEXAMPLE 9\u2014Clustering to discover common\n\npatterns\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n28\n\nTo examine the\n\nall individuals\n\nmovement\n\ngeneral eye pattern adopted by during exploration period, an HMM was created using the exploration transition matrix and Gaussian emissions for each individual, and these HMMs were clustered or summarized into one group using the Variational Hierarchical Expectation Maximization (VHEM) algorithm (see, e.g., Coviello et al., 2014). The VHEM algorithm clusters HMMs into a predefined number of groups according to their probability distributions and characterizes each group using a representative HMM. A similar procedure was performed to obtain the general eye movement for the preference-biased period.\n\nthe\n\npattern\n\ndifferences in decision making behavior, the participants\u2019\n\nTo examine individual\n\nhigh-level transition matrices were clustered into 2 groups using the k-means clustering algorithm (see, e.g., MacQueen, 1967). The aim was to discover differences in the high-level cognitive behavior involving the explorative and preference-biased periods. For each group, representative exploration-period and preference-biased-period HMMs were computed running the VHEM algorithm on the exploration-period and preference-biased-period HMMs of that group respectively\u2019. It was then examined how participants in the two groups differed decision making behavior, including their differences in the gaze cascade plot/effect, transition between the exploration and preference-biased periods, distribution of number fixations in exploration/preference-biased periods, and accuracy of inferring preference\n\nin\n\nby\n\nof\n\nchoices from eye movement data.\n\nEXAMPLE 10\u2014Transition between exploration and preference-biased periods\n\nAfter participants\u2019 high-level transition matrices were clustered into two groups, it\n\nwas further investigated how the participants in these two groups differed in decision-making behavior. More specifically, the probability that the participant was in the preference-biased period was investigated from the beginning to the end of a trial. For each trial, the posterior probabilities of all possible high-level state sequences were computed given the observed eye gaze data. Then a sequence of probabilities of being ln the preference-biased period was computed by computing the expectation over the high-level state sequences, 7.e., by computing the weighted average over all high-level state sequences where the weights are the posterior probabilities. This was performed on all trials in all participants. Since the duration\n\n' Since forced\n\nthe differences in the transitions between the two sides were of interest, all HMMs in this example to use the same set of ROIs that covered each face.\n\nwere\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n29\n\n10\n\ndiffered in each trial, to examine the proportion of time a participant spent in the exploration and preference-biased periods relative to the whole trial, the different durations were normalized across the trials by dividing each trial into the same number of segments (21 in the instant experiments). Then, for each segment in a trial, the probability of time that the participant was in the preference-biased period was calculated. For each participant, the mean probability was calculated across all trials for each segment. Finally, the means across participants in the same group were averaged and the mean probability at each trial segment was plotted for the two participant groups separately. The plot represented the percentage of time at each trial segment the participants were in the preference-biased period. This examination allowed the detection of the difference in temporal dynamics of cognitive state changes ln a trial between groups.\n\n] >\n\nThe number of fixations ln the exploration and preference-biased period between the two groups were also examined. To this end, for each trial and each participant, the posterior probabilities of all high-level state sequences were computed, and then the numbers of fixations in the exploration and preference-biased periods counted. The aggregated probabilities over all trials and all high-level sequences were then used to form a probability distribution of number of fixations in each high-level state for a participant. The participants\u2019 probability distributions were then averaged together in each group.\n\n20 EXAMPLE 11\u2014Inferences of individual preferences\n\n25\n\n30\n\nIt was examined whether SHMMs could be used to infer an individual\u2019s preference\n\nchoice in a trial. For each participant, the trials in the preference decision making task were split into two sets: one for the trials in which the left-side image was chosen to be preferred, and the other for the trials in which the right-side image was chosen. Similar to the preference decision making task, face images used in a trial were matched in gender and attractiveness ratings. Thus, the two sides were expected to be chosen equally often. For each set, all but one trial was used to train a left-selected and a right-selected SHMM and the held-out trial was used for testing. For testing, an aggregated SHMM was created from the two trained SHMMs, which could be used to infer the participant\u2019s choice. In particular, the aggregated SHMM contained 3 high-level states: exploration, left-selected preference-biased, and right- selected preference-biased periods. In the high-level transition matrix of this aggregated SHMM., the transition probability of moving from the exploration period to a preference- biased period was equally divided between the left-selected and right-selected preference-\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n30\n\nbiased periods. Finally, to perform inference of the participant\u2019s choice on the test eye fixation sequence, the posterior probability of high-level state of the last fixation D(S7p|xX1,...,Xr) given the test sequence (%,,...Xx7) was computed, which indicates the probability of being in either the left-selected or right-selected preference-biased period at the end of the trial. The left/right-selected preference-biased period with higher probability was predicted as the choice. This was repeated over all trials for each participant to calculate the inference accuracy.\n\nThe inferences were performed in three ways. First, to examine the percentage of\n\nfixations in a trial required for making above-chance-level inferences, the first 10% of the fixations were used for the inferences and this proportion was increased by 5% each time until all fixations (100%) were used. The inference task was therefore conducted 19 times, and the mean inference accuracy was calculated each time. Second, to examine how quickly a decision could be inferred, inference on increasing duration of fixation sequences was performed from the beginning of the trial (e.g., the fixations In the first 1 second, the fixations the first 2 seconds, etc.). Third, the gaze cascade model suggested that although preferences were shaped during a trial when participants switched their fixations between the two stimuli, the fixations immediately before the end of the trial were usually significantly biased to the preferred stimulus. Thus, these fixations should be better predictors for participants\u2019 preference than the earlier fixations. Accordingly, in a separate examination only the fixations in the last 2 seconds before the decision were used to perform the\n\nin\n\ninferences.\n\nEXAMPLE 12\u2014Categorization of individual SHMMs\n\nOne SHMM was trained for each participant and 1) a standard HMM using the exploration period transition matrix to represent the participant\u2019s eye movement pattern during the exploration period and 2) a standard HMM using the preference-biased period transition matrix to represent the participant\u2019s eye movement pattern during the preference- biased period were created.\n\nTable 2 below shows the average high-level state transition matrix. Participants started in the exploration period, and had a 55% probability to remain in the exploration period. Otherwise there was 45% probability to transition to the preference-biased period, and remained there until the end of the trial.\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n3]\n\nTable 2. High-level state transition matrix of all subjects.\n\n: Exploration : Preference Prior 1.00 3 0.00 Sgioaios ga \u4e00 \u8bb2 \u4e00 \u4e00 \u4e00\n\nNext, the exploration period HMMs of the 24 participants were clustered into one representative HMM using the VHEM algorithm. Tables 3a and 3b below show the transition matrices of the representative exploration period HMM and preference-biased HMM, respectively.\n\n3a. The transition matrix of the representative Exploration HMM obtained clustering the 24 exploration period HMMs into one group.\n\nTable\n\nLeft Right Prior 0.70 0.30 Left \u00a9 | 064 2... 05 036 \u00a9 ) gage \u2018BC\n\nTable\n\n3b. The transition matrix of the representative preference-biased HMM obtained clustering the 24 preference-biased period HMMs into one group\n\nto Chosen to Not-chosen Prior 0.53 0.47 oo oy \u201cfiom Notchosen 033 ssessussnnsnnnnunnsnannnssannnannenaanenaanenaseeene 1, ee\n\nAs indicated in Table 3a above, participants tended to first view the left side with several fixations, and then transition to view the right side for several fixations. After viewing the right side, the participants rarely looked back at the left side (12% probability). This suggested that participants performed a quick scan of the two sides during the exploration period.\n\nby\n\nby\n\n10\n\n] >\n\n20\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n32\n\nSimilarly, Table 3b above shows that participants in the preference-biased period were biased to remain looking at the to-be-chosen side (77%) more often than to the not- chosen side (67%); furthermore, they were more likely to transition from the not-chosen-side to the chosen side (33%), than vice-versa (23%).\n\nTo investigate individual differences in the gaze cascade effect, participants were clustered based on their high-level transition matrices into two groups. It was found that one group (group A) included 11 participants and the other (group B) included 13 participants. Table 4 and Tables 5a and 5b below show the high-level transition matrices, and the transition matrices of the representative exploration and preference-biased period HMMs for the two groups.\n\nTable 4. Transitions matrices of the high-level states of Group A (11 participants) and Group B (13 participants).\n\nGroup A _ Exploration Preference Prior 1.00 : 0.00 \u2018Exploration 068 0.32 \u2018Preference = t\u2014<\u2018i\u2018CS:ts 000 2 84000505;\n\nGroup B Exploration Preference Prior 1.00 : 0.00 \u2018Exploration 045 0.55 2 2 2 \u2018Preference | (000 | 00\n\nTable 5a. Transition matrices of the representative exploration period of Group A (11 participants) and Group B (13 participants).\n\nGroup A Left Right Prior 0.76 0.24 Left 2 067 033 0 ..\u2014 RE\n\n\n\n10\n\n15\n\n20\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n33\n\nGroup B Left Right Prior 0.64 0.36 Left 0.60 \u4e00 \u201c0.40 Right \u4e8e \u4e8e 009 09\n\nTable\n\n5b. Transition matrices of the representative preference-biased period of Group\n\nA (11\n\nparticipants) and Group B (13 participants).\n\nGroup A Chosen Not-chosen Prior 0.50 0.50 se 0 Not-chosen 0.25 0.75\n\nGroup B Chosen Not-chosen Prior 0.54 0.46 Chosen 0.71 0.29 Not-chosen 0.39 0.61\n\nthe exp oration was longer than that o group B. By examining the transition matrix during the exp From the high-level state transition matrix, group A had higher probability to stay in period (68%) than group B (45%). Thus, the exploration period of group A oration period, it was found that group A had a stronger tendency to start exploring from the left side (76%) t an grou than group B (60%). A transition back to the le: ter switching to the right side, group A also had higher probal t side (17% vs 9%). p B (64%) and had a higher probability to stay looking at the left side (67%) ility to\n\nfixation bias to stay looking at the chosen side. More specifically, participants in grou; During the preference-biased period, participants in group A showed an apparent p A had a stronger tendency to keep looking at the to-be-chosen side (83%) than group B (7 switched less often between the two sides than group B. %) and\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n34\n\nEXAMPLE 13\u2014Cascade plot\n\nThe analyses showed that participants were biased to look more at the side that they were about to choose during the preference-biased period. However, the clustering results showed that this difference was more obvious for one group of participants (group A) than the other group (group B). To visualize the difference in the gaze cascade effect between the two participant groups, a gaze cascade plot was generated. The plot showed the probability that participants looked at the image to be chosen during the last 2.5 seconds prior to the response. FIGURE 7 shows the gaze cascade plots of the two groups and their average.\n\nAs depicted in the \u201cAll\u201d plot in FIGURE 7, participants spent more time on inspecting\n\nside that they were about to choose near the end of a trial. The proportion of time spent chosen side went from chance level 0.5 steadily up until it reached around 0.87. The probability that each participant looked at the chosen stimuli at each time point was estimated a 100-millisecond (ms) interval. To test the hypothesis that across the time intervals there was significant difference in the probability of looking at the chosen side, and that the two groups of participants differed in the time interval effect, a mixed AVOVA was performed probability of looking at the chosen side with time interval as a within-subject variable and group as a between-subject variable. The results showed a significant main effect of time interval, F(3.043, 66.938) = 52.163, p < .001, Np = 0.703, a significant main effect of group, F(1, 22) = 5.481, p = 0.029, np = 0.199, and a marginal interaction between time interval and group, F(3.043, 66.938) = 2.307, p = 0.084, np = 0.095 In addition, there was significant linear trend, F(1, 22) = 126.657, p < 0.001, np = 0.852, and quadratic trend, F(1, = 19.609, p < 0.001, Np = 0.471, across time intervals. These results demonstrated that during the last 2.5 seconds before response, participants had a significant increase probability of looking at the chosen side. In addition, group A had higher a probability\n\nthe\n\nthe\n\nat\n\nthe\n\n22)\n\nlooking at the chosen side than group B, indicating a stronger cascade effect.\n\nPost-hoc f-test showed that participants started to look at the chosen item significantly above chance level at around 1100 ms before the end of the trial (7.e., the beginning of the gaze cascade effect), t(23) = 2.27, p = 0.033, d = 0.46 (Here d refers to Cohen\u2019s d, an effect size measure to indicate standardized difference between two means), until the end of the trial. Within this time period, the probability to look at the chosen face rose from 58% to 87%.\n\n\n\nGreenhouse-Geisser correction was applied whenever the assumption of sphericity was not\n\nmet.\n\non\n\non\n\na\n\nin\n\nof\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n35\n\nFurthermore, there was a short period from 2200 ms and 1600ms before the end of the trial, where the participants also looked at the chosen side with slightly higher probability than chance level according to f-test (~55%; FIGURE 7) .\n\nThe plots of the two participant groups showed\n\nsome interesting differences. The\n\nparticipants in group A showed a stronger gaze cascade effect. The probability that group A looked at the chosen item reached 94.5% at the end. A /-test was conducted to examine when their probability of viewing the chosen stimulus was above chance level. The result showed that this occurred at around 1000 ms before the end of a trial, t(10) = 2.44, p = 0.035, d = 0.74, at which time point they spent about 66% of their time on the chosen item. In contrast, participants in group B had a weaker cascade effect, looking at the chosen item about 81% of the time at the end. A \u00a2-test indicated that the proportion of time spent on the chosen item was significantly above the chance level at 900 ms before the end of a trial, t(12) = 2.29, p = 0.041, d = 0.64, at which point the probability to view the chosen side increased from 59% to 81%. Group B also exhibited a short period of slightly higher than chance viewing (54%) of the chosen side between 2100ms and 1600ms before the end of the trial (FIGURE 7). The proportions at each time point were also compared between the two groups using independent sample f-tests. It was found that during 700 ms before the end of a trial to the end of the trial, group A spent significantly more time looking at the to-be-chosen item than group B (FIGURE 7). Thus, although both groups exhibited the gaze cascade effect, they differed in both magnitude and onset time, suggesting substantial individual differences in the gaze cascade effect. These results indicated that the EMSHMM method of the instant invention allowed the detection of these individual differences in the gaze cascade effect through\n\nclustering participants\u2019 eye movement patterns according to their similarities.\n\nIn addition, with the HMM based approach the similarity of each participant\u2019s eye movement pattern during the preference-biased period to the representative pattern of group A or group B could be quantitatively assessed by calculating the log likelihood of the participant\u2019s eye movement pattern being generated by the representative model. To quantify participant\u2019s eye movement pattern along the continuum between the representative patterns of group A and group B, the A-B scale was defined as (LA \u2014 LB)/(|LA| + |LB)), where LA is the log likelihood of the eye movement pattern being generated by the group A model, and LB is the log likelihood of the eye movement pattern being generated by the group B model (Chan et al., 2018). The larger the A-B scale, the more similar the pattern is to the representative pattern of group A, and vice versa for the representative pattern of group B.\n\na\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n36\n\nAmong the participants, a significant positive correlation was observed between A-B scale and gaze cascade effect as measured by the average probability looking at the chosen item from the onset of the effect (1000 ms prior to the response) to the end (r = .50, p = .012). In other words, the more similar participants\u2019 eye movement patterns during the preference- biased period to the representative pattern of group A, the stronger their gaze cascade effects.\n\nEXAMPLE 14\u2014Transition between exploration and preference-biased periods To examine whether the two participant groups differed in their transition behavior between the two cognitive states throughout a trial, the trial duration was normalized by dividing each trial into 21 time segments and for each participant the percentage of trials, or the frequency that the participant was in the preference-biased period during each time segment was examined. FIGURE 8A shows the average probability of the two groups being in the preference-biased period throughout a trial. FIGURE 8B shows the probability distribution of number of fixations in the exploration period, and number of fixations in the preference- biased period. The vertical bars indicate standard errors.\n\nThe results showed that for both\n\nthe probability of being in the preference-\n\ngroups, biased period increased soon after a trial began. To test the hypothesis that group A and group B had different probabilities across the time segments, a mixed ANOVA on probability of being in the preference-biased period with time segment as the within-subject variable and group as the between-subject variable was conducted. The results showed a significant time segment effect, F(1.384, 30.445) = 339.091, p < .001, mp = 0.939, a significant group effect, F(1 22) = 7.770, p = 0.011, and a significant interaction between group and time segment, F(1.384, 30.445) = 10.064, p = .001, np = 0.314. These results indicated that in general group B had higher probability to be in preference-biased period than group A, especially in the beginning time segments of a trial (FIGURE 8A). Although group A entered the preference- biased period later, group A had a stronger gaze cascade effect. In contrast, group B entered the but had weaker cascade effect.\n\npreference-biased period earlier,\n\na\n\ngaze\n\nA t-test was used to test the hypothesis that group A and group B differed in average number of fixations per trial. It was found that group A (M = 29.0) had a significantly larger average number of fixations in a trial than group B (M = 13.3), t(22) = 4.44, p< .001, d= 1.75. Group A also had a larger average number of fixations in the exploration period (group A: M = 3.38; group B: M = 1.88), t(22) = 8.46, p < .001, d = 3.37, and in the preference-\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n37\n\nbiased state (group A: M = 25.61; group B: M = 11.5), t(22) = 4.10, p < .001, d = 1.62 (Figure 8B). However, after normalizing for the total number of fixations, group A (M = 0.147) and group B (M = 0.173) did not differ in the fraction of fixations in the exploration or preference-biased period, t(22) = -1.11; p= 0.28; d =0.45.\n\n<\n\nFurther, the hypothesis was tested that group A and group B differed in the number of\n\neye gaze switches between stimuli during a trial. Group A (M = 5.66) had more switches than group B (M = 4.10) on average, t(22) = 2.81, p = 0.01, d = 1.12. However, this effect was mainly due to group A having more fixations In a trial in general. After normalizing for the total number of fixations in a tnal, group A (M = 0.211) had a smaller fraction of eye fixations that involved switching between stimuli than group B (M = 0.328), t(22) = -7.20, 001, d = 2.97. This effect suggested that group A tended to explore a stimulus longer before switching to the other stimulus than group B. Note that since the clustering was only based on participants\u2019 cognitive (high-level) state transitions, these differences in number fixations per trial and frequency of switch between stimuli emerged naturally as a result\n\np\n\nof\n\nof\n\nthe clustering.\n\nEXAMPLE 15\u2014Inference of participants\u2019 preference choices\n\nIt\n\nwas\n\nalso explored whether the models\n\ncould be used\n\nto\n\ninfer\n\nan\n\nindividual\u2019\n\npreference choice in each trial given partial eye movement data. The fixations from the first 10% of the trial according to normalized duration ere used and were gradually increased at step of 5%. FIGURE 9A shows the average inference accuracies of the two groups using partial fixations in a trial, selected as a percentage of each trial\u2019s duration from the beginning. FIGURE 9B shows the average inference accuracies of the two groups using different window lengths starting from the beginning of the trial (note that participants had different trial lengths, with the average length 5.65+2.57 seconds; Group A had average trial length 7.07+2.32 seconds, while Group B had average trial length of 3.9341.13 seconds). The red, blue, and green stars on the top indicated the data points at which the accuracy was significantly higher than the chance level based on \u00a2-test, for each group and all participants\n\na\n\nof\n\nrespectively.\n\nAs shown in FIGURE 9A, when using the first 75% to 90% of the fixations in a trial, the average inference accuracy was higher for group B than for group A. In contrast, when the first 95% or all fixations of a trial were used, the inference accuracy for group A was higher than that for group B. To test the hypothesis that group B had higher inference\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n38\n\naccuracy when 75% to 90% of the fixations were used whereas group A had higher inference\n\naccuracy when 95% to 100% of fixations were used, a mixed ANOVA on inference accuracy was conducted with amount of fixations (75% to 100% at a step of 5%) as the within-subject variable and group as the between-subject variable. The results showed a main effect amount of fixations, F(1.356, 29.840) = 30.778, p < 0.001, Np = 0.583, and an interaction between amount of fixations and group, F(1.356, 29.840) = 5.657, p =0.016, Np = 0.205. The main effect of group was not significant, F(1, 22) = 0.077, n.s. These results indicated that, shown in FIGURE 9A, group B had higher inference accuracy when a smaller amount fixations were used (75% to 90%), whereas group A had higher inference accuracy when larger amount of fixations were used. When the inference accuracies were compared against the chance-level (0.5) using /-test, it was found that when using the first 75% to 85% of the fixations, group B\u2019s inference accuracy was significantly above the chance-level, whereas group A\u2019s inference accuracy was not. When using the first 90% to 100% of the fixations, inference accuracy was significantly above the chance-level for both groups (FIGURE 9A). other words, participants ln group B revealed their biases to the preferred stimulus earlier\n\nIn\n\nthan group A.\n\nTo examine the actual time when a group\u2019s accuracy became significantly above chance, FIGURE 9B plots the average inference accuracy using time windows (in seconds) starting from the beginning of the trial. After the start of the trial, Group B\u2019s inference accuracy increased more rapidly than Group A, and became significantly above chance at around 3 seconds, and then saturated at around 4 seconds. In contrast, Group A\u2019s inference accuracy increased slowly, became above chance-level at around 6 seconds, and saturated at around 10 seconds. Although Group A\u2019s accuracy increased more slowly, it saturated at a value than B.\n\nhigher\n\nGroup\n\nEXAMPLE 16\u2014Last 2 seconds before response\n\nThe gaze cascade effect suggested that eye movement patterns immediately before a preference decision response may provide a strong cue for inferring the preference. According to FIGURE 7, participants started to show a tendency of looking at the chosen side more often at around 2 seconds before response. Thus, the accuracy of inferring participants\u2019 preferences was examined using the fixations during the last 2 seconds before the response. FIGURE 10A shows the average inference accuracies of the two groups using fixations ln the\n\nof\n\nas\n\nof\n\na\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n39\n\nlast 2 seconds until the response. FIGURE 10B shows the more similar participants\u2019 eye movement patterns to the representative pattern of group A (7.e. the further to the right of the X-axis), the higher the inference accuracy.\n\nFirst, the hypothesis was tested that for participants in both group A and group B, the\n\naverage accuracy of inferring their preference decisions using the final 2 seconds was significantly above the chance level, and results supported this hypothesis (FIGURE 104A): croup A, M = 0.93, t(10) = 15.89, p < .001, d = 4.79; group B, M = 0.71, t(12) = 3.79, .003, d= 1.05. In addition, the hypothesis was tested that the inference accuracy of group was significantly higher than that of group B. The result supported the hypothesis, t(22) 3.26, p = .004, d = 1.33. In addition, the more similar participants\u2019 eye movement patterns during the preference biased period to the representative pattern of group A, as opposed that of group B (as measured in A-B scale), the higher the inference accuracy (r = .47, p = FIGURE 10B). This result was consistent with the observation that group A exhibited stronger gaze cascade effect (FIGURE 7). The clustering of the two groups was completely based on the eye movement data alone, and thus the group difference in inference accuracy\n\n=\n\nemerged naturally as the result of the clustering.\n\nIt was also tested whether a regular HMM without inferring participants\u2019 cognitive\n\nstate transition, such as that used in the previous EMHMM approach (Chuk et al., 2014), would be able to reveal participants\u2019 preference choices. To this end, the same inference task was performed using regular HMMs in the EMHMM approach. As shown in FIGURE 11, the average inference accuracy of SHMMs (M = 0.81) was higher than HMMs (M = 0.64) for subjects, t(46) = 2.71, p = 0.009, d = 0.78. A two-way ANOVA on inference accuracy was conducted with group and model (SHMM/HMM) as the independent variables. The results showed a significant main effect of group, F(1, 44) = 5.04, p = 0.03, Np = 0.096 and significant main effect of model, F(1, 44) = 8.75, p = 0.005, Np = 0.166. There was interaction between group and model, F(1, 44) = 1.91, p= .17. This result demonstrated again the advantage of EMSHMM for modeling eye movement patterns in tasks that involve cognitive state changes. FIGURE 11 shows the average inference accuracy for all participants\n\nusing SHMMs and regular HMMs when using the last 2 seconds of the trials.\n\nEXAMPLE 17- Relevance of the study results\n\np\n\nA\n\n=\n\nto\n\n.02;\n\na\n\nall\n\na\n\nno\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n40\n\nThe instant invention presents\n\nnovel method, EMSHMM, for modeling eye\n\nmovement\n\na\n\npatterns in tasks that involve cognitive state changes. Similar to the previous hidden Markov modeling approach to eye movement data analysis EMHMM, the EMSHMM approach has several advantages over traditional eye movement data analysis methods such as ROI or fixation heat map analysis, including the ability to account for individual differences In both spatial and temporal dimensions of eye movements (1.e., through discovering personalized ROIs and transition probabilities among the ROIs) and to quantitatively assess these differences. In contrast to EMHMM, which uses a single regular HMM to model eye movements and assumes a participant\u2019s strategy is consistent throughout a trial, the EMSHMM_\u00a7 approach uses multiple low-level HMMs corresponding to different strategies/cognitive states, and a higher-level state sequence to capture the transitions among different strategies/cognitive states. Thus, it is especially suitable for analyzing eye movement data in complex tasks that involve cognitive state changes such as decision-\n\nmaking tasks.\n\nEMSHMM approach,\n\ndemonstrate the advantages of using the\n\npreference\n\nTo\n\na\n\ndecision-making task was conducted, in which participants viewed two faces with similar attractiveness ratings and decided which one they preferred. Previously two different eye movement patterns at different states of a trial had been observed; they usually began with exploring both alternatives and then focused on the one preferred by the end of the trial and these two eye movement patterns were associated with different cognitive states. In the instant EMSHMM approach, it was assumed that the two eye movement patterns were associated with two cognitive states, exploration period and preference-biased period, respectively. A switching HMM (SHMM) was used to summarize a participant\u2019s eye movement pattern in the preference decision-making task. The SHMM contained two ROIs that corresponded to the two faces of choice; two low-level HMMs that summarized the eye movement patterns during the exploration and preference-biased periods, respectively; and\n\nhigh-level state sequence that captured the transitions between the two cognitive states.\n\nA summary of all participants\u2019 high-level/cognitive state transitions showed that on average they had a 55% probability to remain ln the exploration period, and 45% probability to transition to the preference-biased period, and remained there until the end of the trial (Table 2). When all participants\u2019 exploration period HMMs were summarized in one representative model, it was found that participants had a bias to start from looking at the stimulus on the left side and remain exploring there, and then switch to the right side (Table\n\na\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n41\n\n3a). In contrast, when all participants\u2019 preference-biased period HMMs were summarized in one representative model, it was found that participants looked more often at the to-be-chosen, preferred stimulus (Table 3b). When the percentage of time that participants were looking at the to-be-chosen stimulus before the end of a trial was plotted, 1t showed a steady increase at about 1.5 seconds before the end, demonstrating a gaze cascade effect (FIGURE 7).\n\nWhen participants\u2019\n\nSHMMs\n\nwere\n\nclustered\n\ninto\n\ntwo\n\ngroups\n\naccording to\n\ntheir\n\ncognitive state transitions, one group (group A) showed a stronger and earlier gaze cascade\n\neffect than the other group (group B; FIGURE 7). The two groups also showed interesting differences ln the temporal dynamics of eye movement patterns throughout a trial. More specifically, participants in group A entered the preference-biased period later than group B (FIGURE 8A) but had a stronger cascade effect. In addition, group A\u2019s preference over the two alternatives could not be inferred with above-chance performance using early fixations of a trial until the first 90% of the fixations were used. In contrast, group B\u2019s preference could be inferred with above-chance performance with only the first 75% of the fixations (FIGURE 9A). However, when only the fixations during the final 2 seconds before the decision response were used, group A\u2019s preference was inferred with a higher accuracy than group B. This phenomenon showed that although participants ln group A revealed their preference in the eye movement patterns later in a trial than group B, their eye movement patterns contained more information for inferring their preferences. Recent research has suggested that indecisiveness, or decisional procrastination, is associated with informational tunnel vision: indecisive individuals tend to gather more information about the item that is eventually chosen while ignoring information about other alternatives. Accordingly, in the instant study participants in group B have exhibited a more \u2018indecisive\u2019 eye movement pattern than group A, since they entered the preference-biased period earlier, spent proportionally less time in the exploration period, and switched more frequently between the two stimuli for choice, which may be characteristics of informational tunnel vision. Thus, there exists between movement assessed the\n\na relationship\n\neye\n\npattern similarity (as\n\nusing\n\nEMHMM/EMSHMM approach) to the representative pattern of group B and participants\u2019\n\npersonality measures related to indecisiveness.\n\nThese individual differences ln eye movement pattern and cognitive style during decision making have not been reported before in the literature. More specifically, previous studies only observed that decisions were related to the final fixations ln a trial as revealed in the gaze cascade effect. However, using the instant method it can be shown that participants\u2019\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n42\n\npreference can be inferred significantly earlier than the exhibition of the\n\ncascade effect,\n\ngaze and for some participants (e.g., group B) this inference could achieve above-chance level performance with only the first 75% of the fixations. Interestingly, these participants also tended to show a weaker gaze cascade effect. These findings demonstrate the importance taking individual differences into account in the understanding of human decision-making behavior. Importantly, while regular HMMs without cognitive state transitions in EMHMM approach could also account for individual differences in eye movement patterns, the accuracy in inferring participants\u2019 preference choices using EMSHMM is surprisingly and significantly higher than that using EMHMM. Advantageously, EMSHMM can better capture the cognitive processes involved in the task and consequently lead to higher inference\n\nof\n\nan\n\naccuracy.\n\nFurther advantageously, the instant methods can deduce participants\u2019 preference during a decision making process from eye gaze transition information alone. In contrast, previous methods needed to combine eye movement measures with other information such as additional physiological measures or attended visual features including integrated skin conductance, blood volume pulse, pupillary response.\n\nFurthermore, while previous methods reached an average accuracy of 81%, the instant methods deduce participants\u2019 preference from eye gaze transition information alone using EMSHMM with more than 90% accuracies.\n\nIn\n\naddition,\n\nEMSHMM\n\nprovides\n\nquantitative\n\nmeasures\n\nof\n\nsimilarities\n\namong\n\nindividual eye movement patterns by calculating the log-likelihood of one\u2019s eye movement data being generated by a representative HMM. For example, it was shown that the similarity of participants\u2019 eye movement patterns during the preference-biased period to the representative pattern of group A as opposed to that of group B (as measured in A-B scale) was positively correlated with the gaze cascade effect and inference accuracy using fixations during the final 2 seconds before response. In addition to examining the relationship between eye movement patterns and other psychological measures, using methods of the instant invention it can also be determined how the eye movement pattern similarity measure 1s modulated by factors related to decision making styles, such as gender, cultural, sleep loss, etc. Using EMHMM, it was shown that eye movement pattern similarity to an eye-centered, analytic pattern during face recognition 1s associated with better recognition performance whereas similarity to a nose-centered, holistic patterns 1s correlated with cognitive decline in\n\nolder adults (see e.g., Chuk, Chan, et al., 2017; Chuk, Crookes, et al., 2017; Chan et al., 2018),\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n43\n\nthat individuals with insomnia symptoms exhibit eye movement patterns more similar to a representative nose-mouth pattern during facial expression judgments as compared with healthy controls (Zhang, Chan, Lau, & Hsiao, 2019). Advantageously, the EMSHMM of the instant invention can be used to examine how eye movement patterns are associated with other psychological measures and factors that may affect eye movement patterns 1n more complex tasks that involve cognitive state changes.\n\nWhile the analysis using methods of the instant invention so far focused on the eye\n\ngaze transition behavior between two stimuli of choice in the preference decision-making task by using only two ROIs, with each corresponding to a stimulus, further methods are provided to analysis eye movement pattern, e.g., ROIs (low-level states) and transition probabilities among the ROIs, on each stimulus to capture individual differences in information extraction in addition to gaze transition in decision-making behavior. Previous studies showed that participants have preferred fixated features or fixation locations during subjective decision-making. For example, it was found that attractive and unattractive features received more attention than those with intermediate attractiveness and brands located at the center of a shelf in shops were more likely to be chosen. Because individuals differ in how they obtain information from the stimuli of choice during decision making, or in a cognitive task in general, the methods of the instant invention using an EMSHMM toolbox (see Chuk et al., 2014) capture these individual differences by inferring personalized ROIs on each stimulus using a Gaussian mixture model approach and enable the determination of the optimal number of ROIs for each participant trough the Bayesian method. Advantageously, using a larger number of high-level states, the EMSHMM method is able to discover more fine-grained cognitive states in between the exploration and preference-biased periods in terms of similarity. For example, in more complex cognitive tasks such as driving or cooking,\n\nwith a large number of high-level states the instant method enables the detection of more\n\ndiscrete cognitive states essential to the task and their associated eye movement patterns.\n\nThe SHMM can represent differences in transition matrices within a trial (intra-trial differences), while other methods, e.g., the mixed HMM by Altman (2007), add random effects to the HMM. In particular, random effects are added to the emission density means and to the log-probabilities of the transition matrix and prior. This allows inter-subject or inter-trial differences to be represented in a single model. Thus, In some embodiments of the invention the SHMM are extended to add random effects to model inter-subject differences in single model.\n\na\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n44\n\nIn summary, in some embodiments, the novel EMSHMM-based methods of the instant invention analyze eye movement data in tasks that involve cognitive state changes where for each participant an SHMM is used to capture between cognitive state transitions during the task, with eye movement patterns during each cognitive state being summarized using an HMM.\n\nembodiments, the EMSHMM of the invention 1s applied to\n\nface preference\n\nIn\n\nsome a decision-making task. In specific embodiments, the EMSHMM of the invention identifies two common eye movement patterns from the participants, where one pattern entered the preference-biased cognitive state later, showed a stronger gaze cascade effect immediately before the decision response, and allowed the determination of the preference decision later a trial and the other pattern revealed the preference decision much earlier in a trial, spent more time in the preference-biased cognitive state, had a weaker gaze cascade effect in the and led lower decision inference\n\nin\n\nend,\n\nto a\n\nresponse\n\naccuracy.\n\nThese surprising differences emerged naturally as the result of clustering based on\n\neye movement data alone, and were not revealed by any existing methods in the literature. As compared with previous approaches, the EMSHMM method of the invention is unexpectedly superior at capturing eye movement behavior in the task and infers participants\u2019 decision responses with higher accuracy. In addition, EMSHMM provides quantitative measures of similarities among individual eye movement patterns, and thus is particularly suitable for studies using eye movements to examine individual differences in cognitive processes, making a significant impact on the use of eye tracking to study cognitive behavior across\n\ndisciplines.\n\nIt should be understood that the examples and embodiments described herein are for\n\nillustrative purposes only and that various modifications or changes in light thereof will be suggested to persons skilled in the art and are to be included within the spirit and purview of this application and the scope of the appended claims. In addition, any elements or limitations of any invention or embodiment thereof disclosed herein can be combined with any and/or all other elements or limitations (individually or in any combination) or any other invention or embodiment thereof disclosed herein, and all such combinations are with the of the invention without limitation thereto.\n\ncontemplated\n\nscope\n\n10\n\n15\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n45\n\nREFERENCES\n\nAltman, R. (2007). Mixed hidden Markov models: An extension of the hidden Markov model to the longitudinal data setting. Journal of the American Statistical Association, 102, 201-210.\n\nAndrews, T. J, & Coppola, D. M. (1999). Idiosyncratic characteristics of saccadic eye movements when viewing different visual environments. Vision research, 39(17), 2947-2953.\n\nCastelhano, M. S., & Henderson, J. M. (2008). Stable individual differences across images in human saccadic eye movements. Canadian Journal of Experimental Psychology,62(1), 1-14.\n\nChan, C. Y. H., Chan, A. B., Lee, T.-M. C., & Hsiao, J. H. (2018). Eye movement patterns in face recognition are associated with cognitive decline in older adults, Psychonomic Bulletin & Review, 23(6), 2200-2207.\n\nChuk, T., Chan, A. B., & Hsiao, J. H. (2014). Understanding eye movements in face recognition using hidden Markov models. Journal of Vision, /4(11):8, 1-14.\n\nChuk, T., Chan, A. B., & Hsiao, J. H. (2017). Is having similar eye movement patterns during face learning and recognition beneficial for recognition performance? Evidence from hidden Markov modeling. Vision Research, 141, 204-216.\n\nChuk, T., Crookes, K., Hayward, W. G., Chan, A. B., & Hsiao, J. H. (2017). Hidden Markov model analysis reveals the advantage of analytic eye movement patterns in face recognition across cultures. Cognition, 169, 102-117.\n\nCoviello, E., Chan, A. B., & Lanckriet, G. R. (2014). Clustering hidden Markov models with variational HEM. The Journal of Machine Learning Research, 15(1), 697-747.\n\nGovaert, G., & Nadif, M. (2013). Co-clustering: models, algorithms and applications. ISTE, Wiley. ISBN 978-1-84821-473-6.\n\nKanan, C., Bseiso, \u4e86 N., Ray, N. A., Hsiao, J. H., & Cottrell, G. W. (2015). Humans have idiosyncratic and task-specific scanpaths for judging faces. Vision research, 108, 67-76.\n\nLau, E.Y.Y., Eskes G.A., Morrison, D.L., Rajda, M., Spurr, K.F. (2010). Executive function in patients with obstructive sleep apnea treated with continuous positive airway pressure. J. /nt. Neuropsych. Sac., 16, 1077-1088.\n\nMacQueen, J. (1967). Some methods for classification and analysis of multivariate observations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability (pp. 281-297). Berkeley, CA: University of California Press.\n\n10\n\n15\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n46\n\nPhillips, L-H., Wynn, V.E., McPherson, S., & Gilhooly, K.J. (2001). Mental planning and the Tower of London task. O. J. iexp. Psychol. ~ A, 84, 579-597\n\nPoynter, W., Barber, M., Inman, J.. & Wiggins, C. (2013), Individuals exhibit idiosyncratic eye-movement behavior profiles across tasks. Vision research, 89, 32-38.\n\nRidderinkhof, K.R., Band, G-P., & Logan, D. (1999). A study of adaptive behavior: effects of age and irrelevant information on the ability to inhibit one's actions. Acta Psychol., 101, 315-337.\n\nShimojo, S., Simion, C., Shimojo, E., & Scheier, C. (2003). Gaze bias both reflects and influences preference. Nature neuroscience, 6(12), 1317-1322.\n\nWu, D. W. L., Bischof, W. F., Anderson, N. C., Jakobsen, T., & Kingstone, A. (2014). The influence of personality on social attention. Personality and Individual Differences, 60, 25-29.\n\nYeung, P. Y., Wong, L. L., Chan, C. C., Leung, J. L., & Yung, C. Y. (2014). A validation study of the Hong Kong version of Montreal Cognitive Assessment (HK-MoCA) in Chinese older adults in Hong Kong. Hong Kong Med. J., 20(6), 504-510.\n\nZhang, J., Chan, A. B., Lau, E. Y. Y., & Hsiao, J. H. (2019). Individuals with insomnia misrecognize angry faces as fearful faces while missing the eyes: An eye-tracking study. Sleep, 42(2), zsy220.\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n47\n\nCLAIMS\n\nWe claim:\n\nl. An eye movement analysis with hidden Markov model (EMHMM) with co- clustering comprising:\n\na vector of prior values of hidden states, a transition matrix of the hidden states, and\n\na Gaussian emission for each hidden state; wherein the prior values indicate the probabilities of time-series data starting from a corresponding hidden state; the transition matrix indicates the transition probabilities between any two hidden states; and the Gaussian emissions indicate the probabilistic associations between an observed time-series data and a hidden state; and\n\na co-clustering data mining technique;\n\nwherein the EMHMM with co-clustering analyzes eye movement data involving stimuli with different feature layouts.\n\n2. An eye movement analysis with switching hidden Markov model (EMSHMM) comprising:\n\na vector of prior values of hidden states, a transition matrix of the hidden states, and\n\na Gaussian emission for each hidden state; wherein the prior values indicate the probabilities of time-series data starting from a corresponding hidden state; the transition matrix indicates the transition probabilities between any two hidden states; and the Gaussian emissions indicate the probabilistic associations between an observed time-series data and a hidden state; and\n\ntwo levels of hidden state sequences comprising: several low-level hidden state sequence models that are used to learn the eye movement pattern of several cognitive states and a high-level hidden state sequence model that acts like a switch that captures the transitions between cognitive states;\n\nwherein the EMSHMM analyzes eye movement data in complex tasks that state changes.\n\ncognitive\n\ninvolve\n\n3. A method for determining a cognitive style and/or cognitive ability in a subject, the method comprising:\n\ncollecting eye movement data of a subject;\n\n10\n\n] >\n\n20\n\n25\n\n30\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n48\n\ncomputing regions of interest (ROIs) based on the eye movement data;\n\nmeasuring transition times between the ROIs;\n\ncalculating transition probabilities among the ROIs;\n\nclustering eye movements into groups using co-clustering:\n\nassessing each individual\u2019s data log-likelihoods using the co-clustering eye movement models\n\ngroup\n\nmeasuring executive function using TOL, visual attention using Flanker Task, working memory using Verbal and Visuospatial Two-Back Task;\n\ndetermining the relationship between data log-likelihood measures and measured executive function, visual attention, and working memory; and\n\ndetermining a cognitive style and/or cognitive ability of the subject based on the data log-likelihood measures using the co-clustering eye movement group modesl, or the eye movement group to which the subject\u2019s eye movement was co-clustered.\n\n4. A method for determining the cognitive state in a subject, the method comprising:\n\ncollecting eye movement data of a subject;\n\ncomputing regions of interest (ROIs) based on the eye movement data;\n\nmeasuring transition times between the ROIs;\n\ncalculating transition probabilities using the EMSHMM according to claim 2; and determining the cognitive state of the subject based on the EMSHMM results.\n\n5.\n\nA method for inferring a subject\u2019s choice, the method comprising: providing two image choices;\n\ncollecting eye movement data of a subject looking at the two image choices;\n\none\n\ntraining one switching hidden Markov model (SHMM) for a first chosen image SHMM for a second chosen image;\n\ngenerating an aggregated SHMM from the two trained SHMMs;\n\ncalculating the probability of a last fixation based on the aggregated SHMM; and inferring a choice based on the calculated probability and the subject\u2019s last fixation.\n\n6. An electronic device for determining a cognitive style in a subject, the device comprising:\n\nand\n\n10\n\n] >\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n49\n\na camera configured to capture a facial image; and\n\na processor configured to detect a center position of an eye within a facial image via the camera, to determine an eye gaze position based on the center position and a pupil position, to analyze the eye gaze position in consecutively captured facial images, and to measure an eye movement patterns based on the eye gaze positions of the consecutively captured facial images;\n\nwherein the processor is further configured to calculate a log likelihood of an eye movement pattern and to assign a certain cognitive style to the measured eye movement pattern if it matches the eye movement pattern of a representative group of subjects having the certain cognitive style.\n\n7. The device according to claim 6, wherein the processor is further configured to receive external user-entered data that assign certain eye movement patterns with certain external task criteria and/or cognitive state criteria; and the processor is configured to assign a cognitive state to the subject based on the external task criteria and/or cognitive state criteria and the subject\u2019s eye movement patterns.\n\n3\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n1/11\n\nFIGURES IA\u20141C\n\nchiot fixations Fixation Begins. Analytic (Eye-morement\n\nTaRat\n\nTochees\n\nMS: . a \u2018Pram Pt . tet Tye\n\n.\n\now. EYEE SONS:\n\nyg \u201cah:\n\n; ER.\n\noe Lays. a CRI\n\nPRM ore\n\na\n\n. Feo rater\n\nBios te oe a eee eee\n\n_ Fron alee\n\nBice 3 MO ao Analytic\n\n\u4e8c a sty\u201d die\n\n\u2018participants.\n\nUFixation heatmap oungf articipants\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n2/11\n\nFIGURE 2\n\n_ Clustenng Results a 8} ava uv * . . Pan ee ARR eens SSS Ss WEARS a ee SS SS tt tt +t .\n\n4 is \u7531\n\nFIGURE 3\n\nSoy\n\nee EE ee \u8981 \u3002 \u673a \u5c0d ' ' vehicles (mean=0.280, sided \u4e8e ' mm gammy o ad, a \u5168 x) _ | GR AINE RN De ec ay ee ee re f. \u4e86 wweweewe \u4e2d eeewewweewe hd Qk a8 3 the o4 48 oF SEL SKE 094) animats {mean=0.374, std=0.173) hn \u5230 1 ' SN ES RS ? * \u5b57 wank, a eh EE LLP Sk Al eo Vee ZZ i rn 2 ee ; ea a a iii eee eee eee ee eee ee ee eer ere re eee a\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n3/11\n\nFIGURE 4A\n\nsoy ate ata \u4e86 e's ee be ee \u3002 . uy. ate. ae : tase ee) \u3002 . tenet een: wee . tet ae. . oa x \u5340 se \u2018= se tae! \u3002 : : : : \u3002 oe \u3002 ee ee woe aw ae m8, ec et; . . see tet a he 4, en 2 2 & \u00a9 \u548c Dee te . . \u751f \u751f ss \" ke . vs eee a a . = a eee * \u3002 \u6709 plat oe a te aa + \u3002 \u672c arenes \", WWW 2, \u201c \u201cs wa ma ee MA wa wu ae a 4444. bar te teeta a ate ath mae Co ae | Cee ie a! a, a8, 4, . ., \u904e \u54c1 op wa he ahs SS NS\n\n2 \u65f6 f, , hips Ataf ere = . es va + . Ons * + ty 5 2 LE, 9 Hs\n\n.\n\n|\n\n\u3002.\n\n\u4eba 2\n\na = \u00e9 2 Giggs 2 Bee a! a\n\nthe\n\n\u00a5 4 rat O ty\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n4/11\n\nFIGURES 4b1 \u2014 4b4\n\n. = Sa SESS SS SEES RH SSS : mo 4 AA We SN WA www ts te AG AM aN A S RSS Bares nage \" SN ': \u9084 \u548c \\ 140 ii ES NE 4 am] \u672c \u5168 \\ ~~ 3 SN Shes LER A hI ED we ee oem LEP OP EEL aA ta oy 2 \" i ee ee ee ww ee Saat tye a age + \u2018oS Te ty \u4eba SAM al AA vs as \u201cpts eat w we \u591a COE EEE EN 4 a8 CM eee oe\u201d 4 a4 ay 44 ++ \u56db . . . . . os =. . 8 8 8 8 8 8 SS\n\nGroup 2: Focused\n\n\\ NS ES es \u6c5f =e : ae 4, ws . Ca . a a,\n\nft Fal J \u4eba # Ae r Pa a! a \u904e area Pde AY ee , \u6709 vey 7 \u4e86 i ieee (tgs y ged a Peat ied pe ceded 2 Sa es\n\na\n\nPaes\n\ni ee 444 . ah RS AN SNS NS oS RSE SSSR Oa SSS SA SRS a4. 2\u201d \u540e . . . . SN SS\n\n= \u5716\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n6/11\n\nFIGURES 5A \u2014 5F\n\npe0.00010, O0}<0.477\n\n20001, r160}=0.544 pat\n\neR \u4e86 - \u201d. nian ee a ee eee tae cat\u201d eh. i <r a a as ck a: ea. sea a ae | , TP RRARARARARARA AAA RAAB 3 \u00a9 \u751f \u54c1 es en \u2018ale! 5 iy AS 5 \u548c Fo Te Se & 4 \u6709 4 \u6709 \u6709 \u6709 4 \u6709 \u6709 \u6709 4 4 J as - 4 w 3 \u6709 \u6709 4 4 \u6709 \u6709 4 4 \u6709 \u6709 4 \\ 4 \u6709 \u6709 \u6709 4 \u6709 \u6709 \u6709 4 \u6709 \u6709 \u6709 Ny \u548c 3 \u6709 4 \u6709 \u6709 \u6709 \u6709 4 \u6709 \u6709 \u6709 4 8 : Fa Sa Sa Sa a Se Soe Se ee Wena eee eee eee To Fa Fa To Fa To, Fa To So Ten Se Ten Se Te We we 1 ' ye ii OR a \u201ces ees. Sell + eye ig tripe Gary Forused < BF Saale <> Focused < EF Scale --> Explorative L_P_P_P_P PP PP PP RAAT AAA AAR \u5230 G8 p=0.00243, r(60): cn oS. SN AAA a ee ew ee ee PB. a we ee ee \u9084 \u201cA | (RRR a os\u201d . one ERTS TO RI (P80 PBS Be ey \\ \u4e86 Nee ee eh hehe he he he he hehehe he he he hee he he he hehehehe he he hp \u3002 \u3002 7. ps0.09825, (60)=-0.901 Raye ee eee 5 ee ee a ee! p=0,08737, 159)=0.223 Fovusad <-- EF Scale --> E . a oy \u540c Ra ' . \u4e0b \u4e8c \u4e8c \u4e8c ae SCO \uff0c SRR -_ Focused <-- EF Scale ~> Explorative pxO. 01859, r{BO}=-0.914 a SOO \u5eec \u9580\u4eba \u4eba \u7ad9 \u4eba \u4eba \u7ad9 \u8a08 \u4eba \u4eba \u4eba Cte ee ee ee ee \u7ad9 \u5168 \u4eba Pe be Wee Wee Hee Hee He ee i of as 4 hE \u9ede \u4eba \u53ea \u201d \u6709 ee \u540c . \u2018 oye ees os oe an \u201cDay juenBuer somes ; acy ve : a - wee \u7f51 pty LO. \u5b57 i iat Lh: 2 \u7ad9\n\nFocused! <\n\nEF Seats -> Explorative\n\nFocused <-- EF Scale ~-> Explorstive\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n7/11\n\nFIGURE 6\n\nWG WA NWI GIL IRL SSG SN CX \u3002 ~ WWW AN \u6240. SS SS NS Be kk we + + 4 4 4 4 4 + 4 4 + + + + nn + + + + + + + + 4+ 4+ + + + + Oe fe 8 oe ee ke ee + + + + + tt + + + + + ee : + \u00a2 + \u00a2 + \u00a2 + \u00a2 + + + + = a Rk ee ot Bee ee ee ee 60L\u2014ss\u2014i\u2018(\u2018\u201c\u2018\u201c\u00e9t Te ee CCC SMO mY x SS SS eS wi mate SEQ SN : oo Ex Ee... = \u2018 BR RRR SS \u6cd5 vee ee Ne eee RRR RRR RRR RRR eee ee lll eS 0 tt te , OOO ee can a ea D REE EEE SO RAR yy yy ein eh hh hh hhh hk a oh) Se ers / a he Ae ek eh Ce hh Ss oe hh eh ee ee or ee ee ah ee + ey Cee ee ee ee ee ee Oe eR ek ee th et te . \u56fe 2 . . a a \u3002 + \u2018a we \u3002 + \u3002 ae \u2122~ . 2 \" + ws + sw : + + \u3002 + . ++ : ss + ott g \u2018 w \u673a + +s e . . . + +t ee . : + + were . - \u3002 Wo CC \u3002 \u4e5f \u673a \u3002 + a, + ve \u3002 Oh . . \u3002 + rc : < ++ . \u3002 arate +t \u3002 + 4 + et tee +e eee ees\n\nFIGURE 7\n\nrade 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 \u00ab \u2014 anc xx \u201cWAL. ake er aw . ) ) } ) ) ) ) ) ) ) ) ! : >. 1 . : \uff0c + a sae a an\u3002 \u201c - ae, \uff0c : ee ka weet? Loans -_ eR | \uff0c : . wie welt aa wae, oes } : : : ur : wo Pee as a H 2 vo ) ) ) ) ) ) ) ) } | \u4e00 | tee \uff0c We 1 \uff0c \u2014 \u672c \uff0c | AA 1 \uff0c \u5168 0.8 \u4e8c \u673a a AA \u672c Ra \u673a t \uff0c oe. oe. \u540c AAS \u672c \u2027 an sg rr - ae \u2018 1 Lee Le. me ayy. | ve ae ex eEEEEEBEBS SSS ARSE ee mS SA | PC \u529e | GT Posi PO Eo AAS: } 1 | \u2018 pms ) 1 ~The | \u53ea 84 \u7531 All ! Nae - \uff0c ) \u201cAdy ! * * * * * * KR \u00ae ' Qo be a je \u300d 28 \u201c2? a. \u4e00 \u201cOS 3 time befure responee ises}\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n8/11\n\nFIGURE 8A\n\nrere \u300c \u672c ve tee wit ee + e . \u4e00 \u4e00 \u4e00 \u4e00 \u7ae0 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 orobaliity in preference period Ra 1.5. a8 trial duration (Horna tele ele ee ee 8 a ee ete ele teeta ete ele ete te mutetoreeeene were ree eee ee ee ee a ee ee\n\nFIGURE 8B\n\nprobability (bj distribution of number of fixations \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 \u4e00 soe \u751f Group 18 Pee oye iat 2Q 2d 30 : 38 pumiber of fixations in oreference-biased period 4\n\na accuracy\n\naccuiracy\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n9/11\n\nFIGURE 9A\n\n\uff0c . \uff0c : . \u540c : ats \u3002 . . : a \u540c \u201c 4 . . . . \u52a0 1 . . . . . | \u4eba \u4eba : * at. . : | 4 1 ? 4 : | \u3001 Lanne j.--+--- Gi FOOD A ! : re 1 . . : a 1 : : \u65f6 \u548c \u54c1 | \u4eba > 1 \"vv \u201d\u4e0a Se OO I I I I | Em gag tt rrr rrr rrr rere I I I a I a a \u00a9 : | Group 8 | | S 1 : : | ! ! : | All ! 1 : : | wes \u548c 1 : : | \u3002 1 . . . . 1 \u3002 1 . . \u570b \u52a0 . . . 1 . . \uff0c \u540c . 1 . . \uff0c . . 1 : : \uff0c : : 1 : . ' : . 1 . . . . . 1 \u3002 . . . 1 : : : : : aye . ! \uff0c : : 4 8) . | | , | | a fra rrr rrr rrr rrr rrr reer rer rrr reer errr rrr rrr reer rrr rrr rere rrr ree em vv vv aoe : : | | : Me, 1 : : : wa. | 1 1 \u5168 wot . . \u2018 . . \u2018 \u201d ! 4 ee . + : . \u548c \u672c 1 : ; \uff0c . _ matte\u201d ,' kw \u3002 \u2027 we, 1 : . \uff0c : a Ma were ~a\" 1 : \uff0c : \u5168 \u533a \u4e00 \u5427 \u4e8c 1 : : - == eer ho ied : ; \uff0c : 4 ee at * . : : \u2018 . \u2014a as wa \u2018ae : \uff0c : al : 1 : : , at Rd 1 : : \uff0c : aa eee an weet. ae 1 : \u958b 1 ~ ad H 8 re ee | ee \u540c die MOTT eee en oe Gee = 1 : : ~~ ~ \u4e00 \u8aaa a , 1 : . awa : 7 \u672c Sr \u4e00 ~ 1 : : ; am\u2019 . sy 1 _ . \uff0c a) es = . \u56db awh ia * \u4e00 ee oe ett lee ee [ ~ ae oe \u4e00 1 * CAR ETT LLL ! 7? \u201ca ov a a ~ \uff0c_ dee \u4e00 _ . we sais : eS a eT awe ------- Ae a ae a ee + aera ~ rr wa\" ww \u9084 W eh. : ~ ~~ ~ secr-- OT mh tt \u4eba [ YY rw 1 ve \u2018 \u2018 o. Ne - \u4e8c \u4e00 \u4e00 \u4e00 \u4eba : ~ Mee ty eee ate ie + 4 : ' 4 YY ae wh 1 4 eae . a ee : *.\u00b0 aus awh\u2019 \u5fc3 \uff0c aa \u2018 : \uff0c ahs, . aan us, wet. : a \uff0c : \u672c 1 8 Ooh SO% 4096 SOM BOM. FO% BON rer FOG%G percentage of inal uss from the begh Ay\n\nFIGURE 9B\n\n\u4e00 vs ss > \u4eba ste \u201c \u3001* : pao aca ' \u548c 1 ' \u548c \u4e86 \u4eba \u4eba Bre ee er be rr le eee ee Be ee pe en be ee ne de ee ee er rt re et fe ee bee re ee eee eee rece eee . 4 \u2018 At aH N a ' . 4 ' \u201d wee , nr L_--------Y---------- L'_--~--------v--------- \u5c0f --------- or L \u4eba \u4eba \u4eba \u4fe1\u4eba \u4eba \u4eba \u4fe1\u4eba \u4eba \u4eba \u5168 \u4eba \u4eba \u4eba \u4eba \u4eba \u4eba \u4eba \u4eba \u4eba \u8aaa re veh \u65f6 av \u672c \u7684 _ 34 \u3001 yee ett * vee eee me \" \u4e00 i. ' =>\" ee ee Aaah rays ~ vv aad - bb te el ~~ ~~ > xt ~ > aS a ++ 1 \u3002\u201d \u56db \u3002 +e ~~ a wes . . At , \u672c \u540c a a a \u2014\u00a5 ae aan \u7ad9 = mm 1 \u570b \u4e86 2 = | 1 1 1 1 1 1 1 I 1 1 1 1 i. 1 ANA enna = = aan wa aan awe ewe 2 2 2 20\u00b0 meee: awe aan wale tua 255 = \u201cos | = Group A oS | | = a arOUp Bp. windaw from be in ingt of tal\n\nlength starting f\n\nag)\n\n(aac)\n\naccuracy\n\nWO 2021/087651\n\nPCT/CN2019/115288\n\n10/11\n\nFIGURE 10A\n\n4 \u4e0a oa fb 9.8 - a. o.4 7 0.2 + g\n\n>\n\n5 &\n\nGroup A\n\nGroup 8\n\nFIGURE 10B\n\nWO 2021087651\n\nPETICN2M19118\n\nwn\n\nFIGURE 11\n\n1\n\n:\n\n0.008\n\nSHRM\n\nINTERNATIONAL SEARCH REPORT\n\nInternational application No.\n\nPCT/CN2019/115288\n\nA.\n\nCLASSIFICATION OF SUBJECT MATTER\n\nA61B 3/113(2006.01)1; GOON 20/00(2019.01)1; A61B 5/00(2006.01)1\n\nAccording to International Patent Classification (IPC) or to both national classification and IPC\n\nB.\n\nFIELDS SEARCHED\n\nMinimum documentation searched (classification system followed by classification symbols)\n\nA61B; GO6N Documentation searched other than minimum documentation to the extent that such documents are included in the fields searched Electronic data base consulted during the international search (name of data base and, where practicable, search terms used) EPODOC, WPI, CNPAT, IEEE, CNKI: eye movement?, hidden markov model?, cluster+, switching, stimuli, high, low, level, cognitive, gaze, ROI DOCUMENTS CONSIDERED TO BE RELEVANT Category* Citation of document, with indication, where appropriate, of the relevant passages Relevant to claim No. \u53ca CHUK, Tim et al.,. \"Hidden Markov model analysis reveals the advantage of analytic eye 1, 6-7 movement patterns In face recognition across cultures,\u201d Cognition, Vol. 169, 03 September 2017 (2017-09-03), pages 102-112 X CHUK, Tim et al.,. \"Mind reading: Discovering individual preferences from eye movements using switching hidden Markov models,\u201d CogSci 2016, 31 December 2016 (2016-12-31), pages 182-186 Xx CHAN, Cynthia Y.H.et al.,. \"Eye-movement patterns In face recognition are associated with 3 cognitive decline In older adults,\" Psychonomic Bulletin & Review, 08 January 2018 (2018-01-08), pages 2200-2207 A CHUK, Tim et al.,. \"Is having similar eye movement patterns during face learning and recognition beneficial for recognition performance? Evidence from hidden Markov modeling,\u201d Vision Research, Vol. 140, 04 May 2017 (2017-05-04), pages 204-216 \u5716 Further documents are listed in the continuation of Box C. \u5716 See patent family annex. Special categories of cited documents: \u201d document defining the general state of the art which is not considered to be of particular relevance \u2019 earlier application or patent but published on or after the international filing date > document which may throw doubts on priority claim(s) or which is cited to establish the publication date of another citation or other special reason (as specified) \u2019 document referring to an oral disclosure, use, exhibition or other \u201cXK 35 \u201coy\u201d * later document published after the international filing date or priority date and not in conflict with the application but cited to understand principle or theory underlying the invention document of particular relevance; the claimed invention cannot considered novel or cannot be considered to involve an inventive when the document is taken alone document of particular relevance; the claimed invention cannot considered to involve an inventive step when the document combined with one or more other such documents, such combination means \u2019 document published prior to the international filing date but later than being obvious to a person skilled in the art \u201d document member of the\n\nC.\n\nthe priority date claimed\n\nsame patent family\n\nDate of the actual completion of the international search\n\nDate of mailing of the international search report\n\n10 July 2020\n\n28 July 2020\n\nName and mailing address of the ISA/CN\n\nAuthorized officer\n\nNational Intellectual Property Administration, PRC 6, Xitucheng Rd., Jimen Bridge, Haidian District, Beijing 100088\n\nLI,Duo\n\nChina\n\nFacsimile No. (86-10)62019451\n\nTelephone No. 86-(10)-53961393\n\nForm PCT/ISA/210 (second sheet) (January 2015)\n\nthe\n\nbe step\n\nbe\n\nis", "type": "Document"}}