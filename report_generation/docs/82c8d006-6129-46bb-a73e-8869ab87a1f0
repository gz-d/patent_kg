{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/751-US20210102527(Pending) re (Done on website already).pdf"}, "page_content": "IN\n\nIN\n\nUS 20210102527A1\n\n( 19 ) United States\n\n( 12 ) Patent Application Publication ( 10 ) Pub . No .: US 2021/0102527 A1\n\nApr. 8 , 2021\n\n( 43 ) Pub . Date :\n\nLiu et al .\n\n( 54 ) SYSTEM AND METHOD FOR MONITORING\n\nGO6N 3/04\n\n( 2006.01 )\n\n( 2006.01 )\n\nA DEVICE\n\nG05B 23/02\n\n( 52 ) U.S. CI .\n\n( 71 ) Applicant : City University of Hong Kong ,\n\nF03D 17700 ( 2016.05 ) ; GO6N 3/08\n\nCPC\n\nKowlonn ( HK )\n\n( 2013.01 ) ; F05B 2260/84 ( 2013.01 ) ; G05B\n\n23/0254 ( 2013.01 ) ; F05B 2260/80 ( 2013.01 ) ;\n\n( 72 ) Inventors : Xin Liu , Kowloon ( HK ) ; Zijun Zhang ,\n\nGO6N 3/0454 ( 2013.01 )\n\nKowloon Tong ( HK )\n\n( 57 )\n\n( 21 ) Appl . No .: 16 / 592,815\n\nABSTRACT\n\n( 22 ) Filed :\n\nA system and a method for monitoring a device including the\n\nOct. 4 , 2019\n\nsteps of obtaining operation information from a device ,\n\nPublication Classification\n\nwherein the operation information is associated with the\n\ncondition of the device in operation ; and processing the\n\n( 51 ) Int . Ci .\n\noperation information with a device modelling engine to\n\n( 2006.01 )\n\nFO3D 17/00\n\ndetermine one or more operation conditions of the device .\n\n( 2006.01 )\n\nGOON 3/08\n\nod\n\nof\n\nod\n\nUS 20210102527A1\n\nas) United States a2) Patent Application Publication\n\nao Pub. No.: US 2021/0102527 Al\n\n(43) Pub. Date: Apr. 8, 2021\n\nLiu et al.\n\nSYSTEM AND METHOD FOR MONITORING A DEVICE\n\nGO6N 3/04 (2006.01) GO5SB 23/02 (2006.01)\n\nUS. CL CPC veces F03D 17/00 (2016.05); GO6N (2013.01); FOSB 2260/84 (2013.01): GOSB 23/0254 (2013.01); FOSB 2260/80 (2013.01); GO6N 3/0454 (2013.01)\n\n_ \u7ad9 (71) Applicant: City University of Hong Kong, Kowlonn (HK)\n\nInventors: Xin Liu, Kowloon (HK); Zijun Zhang, Kowloon Tong (HK)\n\n(57) ABSTRACT\n\n(21) Appl. No.: 16/592,815\n\nAsystem and a method for monitoring a device including the steps of obtaining operation information from a device, wherein the operation information is associated with the condition of the device in operation; and processing the operation information with a device modelling engine to determine one or more operation conditions of the device.\n\n(22) Filed: Oct. 4, 2019\n\nPublication Classification\n\n(51) Int. Cl.\n\nFO03D 17/00 GO6N 3/08 (2006.01) (2006.01)\n\n122 Conditions Renorting ee Aopargtus Device Comitionas Gateway Homme TGR\n\n(54)\n\n\n\n(52)\n\n3/08\n\n(72)\n\n100\n\nPlants Equipment\u2019\n\n164\n\nPatent Application Publication\n\nApr. 8 , 2021 Sheet l of 9\n\nUS 2021/0102527 A1\n\n\n\n?\n\n?\n\ntrantarrid\n\n|\n\nUS 2021/0102527 Al\n\nApr. 8, 2021 Sheet 1 of 9\n\nPatent Application Publication\n\nOOF Agmaieg suOTIDUO oar aryeinddy peed Asean\n\ny Binds\n\nPOT\n\nAwe\n\nPatent Application Publication\n\nApr. 8 , 2021 Sheet 2 of 9\n\nUS 2021/0102527 A1\n\nConditions Monitoring Engine\n\n106\n\n206\n\n208\n\n204\n\n202\n\nPreprocessing\n\nConditions\n\nReport\n\nDetection\n\nAbnormal\n\nModule\n\nDevice\n\nModelling\n\nEngine\n\nFigure 2\n\n1\n\n1\n\n1\n\nI Preprocessing bypass\n\n} I { {\n\nUS 2021/0102527 Al\n\nApr. 8, 2021 Sheet 2 of 9\n\nPatent Application Publication\n\nalnpolIN uofrl5919d jeuuouqV aul3ud auillspoiIWN solAedq 90T aul8u3 z0Z \u2014\u2014> | 3ulss95oJdaJd ssedAd 3ulolluoIAN 3ulssaooJda4d suorllpuo9\n\nZ 9Jn \u5f15\n\n80Z\n\n\u4e00 \u4e00 \u4e00 >\n\n\u201c14ods suoHipuoD\n\nPatent Application Publication\n\nApr. 8 , 2021 Sheet 3 of 9\n\nUS 2021/0102527 A1\n\n.....\n\nI\n\nUS 2021/0102527 Al\n\nApr. 8, 2021 Sheet 3 of 9\n\nPatent Application Publication\n\nBuses ee por 3 DG \u4eba 893 \u516d \u548c \u7528 \u4e2d \u516b \u516d \u548c \u7576 [ds Me f MACeRGeN {EEA | Buea} PUES PRE. yOueRuERE oil Bugeedg\n\nPatent Application Publication\n\nApr. 8 , 2021 Sheet 4 of 9\n\nUS 2021/0102527 A1\n\nFigure 4\n\n400\n\nCondition\n\nMonitoring\n\nModel\n\ntraining\n\nData\n\ngrouping\n\ncollection\n\nData\n\nStart\n\nEnd\n\nData pre - processing\n\n408\n\n404\n\n410\n\n406\n\n402\n\nUS 2021/0102527 Al\n\nApr. 8, 2021 Sheet 4 of 9\n\nPatent Application Publication\n\napolmo uORIpuo) | < \u4e00 \u4e00 OTP SMUD [Poy | \u4e00 \u4e00 80y | Burdnoss ee | \u4e00 \u4e00 \u4e00 90 BassoooxdHoad ed \u2014\u2014 por Wonoolloo meq = <\u2014\u2014\u2014\u2014__ 70\n\np ainsiy\n\nPatent Application Publication\n\nApr. 8 , 2021 Sheet 5 of 9\n\nUS 2021/0102527 A1\n\n1500\n\n28 %\n\n+\n\n%\n\nFine\n\ntuning & D 518\n\n150\n\nY w\n\n60\n\n????????????? ;:\n\n????\n\n*****\n\n1909\n\nMAS\n\nWY\n\nApr. 8, 2021 Sheet 5 of 9\n\nUS 2021/0102527 Al\n\nPatent Application Publication\n\nG anky BL ee Medes ne 7 # Hing deere pee exec oats\n\nPER\n\nPatent Application Publication\n\nApr. 8 , 2021 Sheet 6 of 9\n\nUS 2021/0102527 A1\n\n06\n\nWWWWWWWWWWWWWWW\n\nTun\n\ntu\n\nApr. 8, 2021 Sheet 6 of 9\n\nUS 2021/0102527 Al\n\nPatent Application Publication\n\nEg as tn an oa\n\nget\n\nee\n\nMe\n\neae\n\nee\n\nnee\n\note:\n\n9 windy\n\nPatent Application Publication\n\nApr. 8 , 2021 Sheet 7 of 9\n\nUS 2021/0102527 A1\n\n700\n\nFigure 7\n\n710\n\n712\n\n708\n\n704\n\n702\n\n706\n\nf\n\nrecoustnction\n\nATOR O validation\n\nIncreases 23 times .\n\nset\n\nlayers\n\nthe unber of hidden\n\nrodes of\n\nStep # : Initialize a neural network\n\nwith no hidden\n\nStep 1 : Insert a hidden layer\n\nhidden des to the\n\nseveral\n\nwith\n\nthe\n\nlayo\n\nThe RE continuously\n\nnework\n\nbest\n\nthe\n\nStep 4 : Select\n\nStep 2 : Defemine\n\nUS 2021/0102527 Al\n\nApr. 8, 2021 Sheet 7 of 9\n\nPatent Application Publication\n\nangi\n\n: Bi BGS 2) GOR Wowjag 1 san em yoajag 1p days SA Sout {9 sssua ray {snoneyUEs PY MLL OTL \u4e00 > : 395 HOTT 0 20 \u751f odtS AL ou 907 > spon aagpprtyo mq ayy ourumaiay -z dag co SOME OY 0 SapOT pol \u4e00 > \u4e0a opbpItieESAoS HR ak | WeppIB vasay 77 dais co : SEA 1 0 \u9ede 0 YA \u548c OAAIStI 3\n\n00/\n\n/\n\nPatent Application Publication\n\nApr. 8 , 2021 Sheet 8 of 9\n\nUS 2021/0102527 A1\n\nm\n\nWww\n\n***********\n\nUS 2021/0102527 Al\n\nApr. 8, 2021 Sheet 8 of 9\n\nPatent Application Publication\n\nVay eH pomeerary t0L i \u65e5 t \u65e5 i i t i i \u4e0a : \u548c Me Seca: eb one en See a Sak ant ag ones og Brg ue FOARY \u4e86 | et OAT aes MO \u5340 \u7576 03 0\n\ngames\n\nPOE\n\nae a\n\nPatent Application Publication\n\nApr. 8 , 2021 Sheet of 9\n\nUS 20210102527A1\n\n?\n\n?\n\nApr. 8, 2021 Sheet 90f9\n\nUS 2021/0102527 Al\n\nPatent Application Publication\n\n& andy ee HOMO jeu porwainy | 2 ra as\n\n|\n\n|\n\nI\n\n|\n\n|\n\nI\n\n|\n\n|\n\nI\n\n|\n\n|\n\nI\n\n| pe i |\n\nApr. 8 , 2021\n\nUS 2021/0102527 A1\n\n1\n\n[ 0014 ]\n\nIn an embodiment of the first aspect , the based\n\nSYSTEM AND METHOD FOR MONITORING\n\nmatching network is a deep neural network .\n\nA DEVICE\n\n[ 0015 ]\n\nIn an embodiment of the first aspect , the structure\n\nof the deep neural network is adjusted based on a stopping\n\nTECHNICAL FIELD\n\ncriterion .\n\n[ 0001 ] The present invention relates to a system and\n\n[ 0016 ]\n\nIn an embodiment of the first aspect , the stopping\n\nmethod for monitoring a device , and particularly , although\n\ncriterion is determined based on a reconstruction error\n\nnot exclusively , to a system and method which uses machine\n\nobtained from the deep neural network .\n\nlearning to monitor the operation conditions of one or more\n\n[ 0017 ]\n\nIn an embodiment of the first aspect , the recon\n\ndevices .\n\nstruction error is obtained by inputting the domain data set\n\ninto the deep neural network .\n\nBACKGROUND\n\n[ 0018 ]\n\nIn an embodiment of the first aspect , the structure\n\n[ 0002 ] The monitoring of plant , equipment and various\n\nof the deep neural network is adjusted by inserting one or\n\ndevices has been a challenge to control systems engineers in\n\nmore hidden layers into the deep neural network .\n\nrecent times as such large scale plants and equipment have\n\n[ 0019 ]\n\nIn an embodiment of the first aspect , the one or\n\nbecome much more widespread . In addition to the coordi\n\nmore hidden layers inserted by adding one or more hidden\n\nnate efforts in operating such equipment efficiently , there is\n\nnodes into the hidden layers .\n\nalso a need to operate these plants and equipment safely as\n\n[ 0020 ]\n\nIn an embodiment of the first aspect , weights or\n\nsafety standards have improved over time .\n\nbiases of the deep neural network are manipulated when the\n\n[ 0003 ]\n\nMonitoring systems have been developed for vari\n\ndeep neural network is adjusted .\n\nous plants and equipment , but often , such systems are\n\n[ 0021 ]\n\nIn an embodiment of the first aspect , the device is\n\ncomplex and require many sensors , monitoring equipment\n\na wind turbine .\n\nand expert personal so as to diagnose and respond to\n\n[ 0022 ]\n\nIn accordance with a second aspect of the present\n\nproblems or failures . In turn , these monitor systems itself are\n\ninvention there is provided a system for monitoring a device\n\nbecoming more complex and expensive which as systems\n\ncomprising :\n\nbecome more and more complex , they are in turn more\n\n[ 0023 ]\n\na device conditions gateway arranged to obtain\n\nprone to failure . This has adversely affecting the efficient of\n\noperation information from a device , wherein the\n\nsuch systems perform its core functions in the first place .\n\noperation information is associated with the condition\n\nof the device in operation ; and\n\nSUMMARY OF THE INVENTION\n\n[ 0024 ]\n\na conditions monitoring engine arranged to pro\n\ncess the operation information with a device modelling\n\n[ 0004 ]\n\nIn accordance with a first aspect of the present\n\nengine to determine one or more operation conditions\n\ninvention , there is provided a method for monitoring a\n\ndevice comprising the steps of :\n\nof the device .\n\n[ 0005 ]\n\nobtaining operation information from a device ,\n\n[ 0025 ]\n\nIn an embodiment of the second aspect , the device\n\nmodelling engine includes a plurality of matching networks\n\nwherein the operation information is associated with\n\nfor processing the operation information of a plurality of\n\nthe condition of the device in operation ; and\n\n[ 0006 ]\n\nprocessing the operation information with a\n\ndevices .\n\ndevice modelling engine to determine one or more\n\n[ 0026 ]\n\nIn an embodiment of the second aspect , each of the\n\nplurality of matching networks is arranged to be associated\n\noperation conditions of the device .\n\nwith an individual device of the plurality of devices .\n\n[ 0007 ]\n\nIn an embodiment of the first aspect , the device\n\nmodelling engine includes a plurality of matching networks\n\n[ 0027 ]\n\nIn an embodiment of the second aspect , the plu\n\nrality of matching networks are generated for each of the\n\nfor processing the operation information of a plurality of\n\nassociated devices by training a base matching network with\n\ndevices .\n\n[ 0008 ]\n\nIn an embodiment of the first aspect , each of the\n\na domain data set .\n\nplurality of matching networks is arranged to be associated\n\n[ 0028 ]\n\nIn an embodiment of the second aspect , the domain\n\ndata set includes operation information from the plurality of\n\nwith an individual device of the plurality of devices .\n\n[ 0009 ]\n\nIn an embodiment of the first aspect , the plurality\n\ndevices .\n\nof matching networks are generated for each of the associ\n\n[ 0029 ]\n\nIn an embodiment of the second aspect , the base\n\nmatching network is further processed by training the base\n\nated devices by training a base matching network with a\n\nmatching network with a target data set associated with the\n\ndomain data set .\n\nindividual device so as to generate each of the matching\n\n[ 0010 ]\n\nIn an embodiment of the first aspect , the domain\n\ndata set includes operation information from the plurality of\n\nnetworks for each of the individual devices .\n\n[ 0030 ]\n\nIn an embodiment of the second aspect , the target\n\ndevices .\n\ndata set includes operation information for each of the\n\n[ 0011 ]\n\nIn an embodiment of the first aspect , the base\n\nmatching network is further processed by training the base\n\nindividual devices .\n\nmatching network with a target data set associated with the\n\n[ 0031 ]\n\nIn an embodiment of the second aspect , the target\n\nindividual device so as to generate each of the matching\n\ndata is used to fine tune the base matching network into the\n\neach of the matching networks for each of the individual\n\nnetworks for each of the individual devices .\n\n[ 0012 ]\n\nIn an embodiment of the first aspect , the target data\n\ndevices .\n\nset includes operation information for each of the individual\n\n[ 0032 ]\n\nIn an embodiment of the second aspect , the based\n\nmatching network is a deep neural network .\n\ndevices .\n\n[ 0013 ]\n\nIn an embodiment of the first aspect , the target data\n\n[ 0033 ]\n\nIn an embodiment of the second aspect , the struc\n\nture of the deep neural network is adjusted based on a\n\nis used to fine tune the base matching network into the each\n\nstopping criterion .\n\nof the matching networks for each of the individual devices .\n\nUS 2021/0102527 Al\n\nApr. 8, 2021\n\n[0014] In an embodiment of the first aspect, the based matching network is a deep neural network.\n\nSYSTEM AND METHOD FOR MONITORING A DEVICE\n\n[0015] In an embodiment of the first aspect, the structure the deep neural network is adjusted based on a stopping criterion.\n\nTECHNICAL FIELD\n\n[0001] The present invention relates to a system and method for monitoring a device, and particularly, although not exclusively, to a system and method which uses machine learning to monitor the operation conditions of one or more devices.\n\n[0016] In an embodiment of the first aspect, the stopping criterion is determined based on a reconstruction error obtained from the deep neural network.\n\n[0017] In an embodiment of the first aspect, the recon- struction error is obtained by inputting the domain data the deep neural network.\n\nBACKGROUND\n\n[0018] In an embodiment of the first aspect, the structure the deep neural network is adjusted by inserting one more hidden layers into the deep neural network.\n\n[0002] The monitoring of plant, equipment and various devices has been a challenge to control systems engineers in recent times as such large scale plants and equipment have become much more widespread. In addition to the coordi- nate efforts in operating such equipment efliciently, there is also a need to operate these plants and equipment safely as safety standards have improved over time.\n\n[0019] In an embodiment of the first aspect, the one or more hidden layers inserted by adding one or more hidden nodes into the hidden layers.\n\n[0020] In an embodiment of the first aspect, weights biases of the deep neural network are manipulated when neural network is adjusted.\n\n[0003] Monitoring systems have been developed for vari- ous plants and equipment, but often, such systems are complex and require many sensors, monitoring equipment and expert personal so as to diagnose and respond to problems or failures. In turn, these monitor systems itself are becoming more complex and expensive which as systems become more and more complex, they are in turn more prone to failure. This has adversely affecting the efficient of such systems perform its core functions in the first place.\n\n[0021] In an embodiment of the first aspect, the device wind turbine.\n\n[0022] In accordance with a second aspect of the present invention there is provided a system for monitoring a device comprising:\n\n[0023] a device conditions gateway arranged to obtain operation information from a device, wherein the operation information is associated with the condition of the device in operation; and\n\nSUMMARY OF THE INVENTION\n\n[0024] a conditions monitoring engine arranged to pro- cess the operation information with a device modelling engine to determine one or more operation conditions of the device.\n\n[0004] In accordance with a first aspect of the present invention, there is provided a method for monitoring device comprising the steps of:\n\n[0025] In an embodiment of the second aspect, the device modelling engine includes a plurality of matching networks for processing the operation information of a plurality of devices.\n\n[0005] obtaining operation information from a device, wherein the operation information is associated with the condition of the device in operation; and\n\n[0006] processing the operation information with a device modelling engine to determine one or more operation conditions of the device.\n\n[0026] In an embodiment of the second aspect, each of the plurality of matching networks is arranged to be associated with an individual device of the plurality of devices.\n\n[0007] In an embodiment of the first aspect, the device modelling engine includes a plurality of matching networks for processing the operation information of a plurality of devices.\n\n[0027] In an embodiment of the second aspect, the plu- rality of matching networks are generated for each of the associated devices by training a base matching network with domain data set.\n\n[0008] In an embodiment of the first aspect, each of the plurality of matching networks is arranged to be associated with an individual device of the plurality of devices.\n\n[0028] In an embodiment of the second aspect, the domain set includes operation information from the plurality devices.\n\n[0009] In an embodiment of the first aspect, the plurality of matching networks are generated for each of the associ- ated devices by training a base matching network with domain data set.\n\n[0029] In an embodiment of the second aspect, the base matching network is further processed by training the base matching network with a target data set associated with the individual device so as to generate each of the matching networks for each of the individual devices.\n\n[0010] In an embodiment of the first aspect, the domain data set includes operation information from the plurality of devices.\n\n[0030] In an embodiment of the second aspect, the target data set includes operation information for each of individual devices.\n\n[0011] In an embodiment of the first aspect, the matching network is further processed by training the base matching network with a target data set associated with the individual device so as to generate each of the matching networks for each of the individual devices.\n\n[0031] In an embodiment of the second aspect, the target data is used to fine tune the base matching network into each of the matching networks for each of the individual devices.\n\n[0012] In an embodiment of the first aspect, the target data includes operation information for each of the individual devices.\n\n[0032] In an embodiment of the second aspect, the network is a deep neural network.\n\n[0013] In an embodiment of the first aspect, the target data used to fine tune the base matching network into the each of the matching networks for each of the individual devices.\n\n[0033] In an embodiment of the second aspect, the struc- ture of the deep neural network is adjusted based on stopping criterion.\n\na\n\na\n\nbase\n\nset\n\nis\n\nof\n\nset\n\ninto\n\nof\n\nor\n\nor\n\nthe\n\ndeep\n\nis\n\na\n\na\n\ndata\n\nof\n\nthe\n\nthe\n\nbased\n\nmatching\n\na\n\nApr. 8 , 2021\n\nUS 2021/0102527 A1\n\n2\n\n[ 0055 ]\n\na device conditions gateway arranged to obtain\n\n[ 0034 ]\n\nIn an embodiment of the second aspect , the stop\n\noperation information from a device , wherein the\n\nping criterion is determined based on a reconstruction error\n\noperation information is associated with the condition\n\nobtained from the deep neural network .\n\nof the device in operation ; and\n\n[ 0035 ]\n\nIn an embodiment of the second aspect , the recon\n\n[ 0056 ]\n\na conditions monitoring engine arranged to pro\n\nstruction error is obtained by inputting the domain data set\n\ninto the deep neural network .\n\ncess the operation information with a device modelling\n\n[ 0036 ]\n\nIn an embodiment of the second aspect , the struc\n\nengine to determine one or more operation conditions\n\nture of the deep neural network is adjusted by inserting one\n\nof the device .\n\nor more hidden layers into the deep neural network .\n\n[ 0057 ]\n\nIn this example embodiment , the system for moni\n\n[ 0037 ]\n\nIn an embodiment of the second aspect , the one or\n\ntoring a device 100 includes at least a device conditions\n\nmore hidden layers inserted by adding one or more hidden\n\ngateway 104 and a conditions monitoring engine 106. The\n\nnodes into the hidden layers .\n\ndevice conditions gateway 104 is arranged to obtain opera\n\n[ 0038 ]\n\nIn an embodiment of the second aspect , weights or\n\ntion information for one or more devices 102 whilst the\n\nbiases of the deep neural network are manipulated when the\n\nconditions monitoring engine 106 is arranged to process this\n\ndeep neural network is adjusted .\n\noperation information for the one or more devices 102 so as\n\n[ 0039 ]\n\nIn an embodiment of the second aspect , the device\n\nto determine one or more operation conditions for the device\n\nis a wind turbine .\n\nor devices 102. These operation conditions in turn can be\n\n[ 0040 ]\n\nIn an embodiment of the second aspect , the system\n\nused for further processing or user review so as to determine\n\nfurther comprises a pre - processor arranged to process the\n\nthe status and condition of the device itself to diagnose ,\n\noperation information obtained from the device conditions\n\npredict or solve problems for the device 108. Preferably , the\n\ngateway .\n\nconditions monitoring engine 106 includes a device model\n\nling engine 204 which is arranged to process the operation\n\n[ 0041 ]\n\nIn an embodiment of the second aspect , the opera\n\ntion information is pre - processed before the information is\n\ninformation by use of a generated model associated with the\n\ninputted to the device modelling engine .\n\ndevice 102 being monitored so as to devise one or more\n\n[ 0042 ]\n\nIn an embodiment of the second aspect , the opera\n\noperations of the device 102 .\n\ntion information is pre - processed to remove fault data from\n\n[ 0058 ] Example embodiments of the system for monitor\n\nthe operation information .\n\ning a device 100 may be used to monitor the conditions of\n\n[ 0043 ]\n\nIn an embodiment of the second aspect , the device\n\na device 102. Preferably , although not necessarily , the device\n\nconditions gateway is a SCADA system .\n\n102 would operate with a plurality of other associated\n\ndevices 102 to perform a certain task or goal . Such devices\n\nBRIEF DESCRIPTION OF THE DRAWINGS\n\n102 may include any equipment , fixture , plant , vehicle or\n\napparatus that may be arranged to operate , individually ,\n\n[ 0044 ] Embodiments of the present invention will now be\n\npartially or completely together to perform one or more\n\ndescribed , by way of example , with reference to the accom\n\ntasks . Examples of such devices may include wind turbines\n\npanying drawings in which :\n\nin a wind farm , as illustrated in some of the example\n\n[ 0045 ] FIG . 1 is a block diagram illustrating a system for\n\nembodiments described below . Other devices , such as coal\n\nmonitoring a device in accordance with one embodiment of\n\npowered generators , nuclear power generators , trains , drill\n\nthe present invention ;\n\ning / boring machines , transportation systems , signalling sys\n\n[ 0046 ] FIG . 2 is a block diagram of an example conditions\n\ntems , oil rigs , pumps , radar arrays , radio transmitters , lifts /\n\nmonitoring engine as used in the system of FIG . 1 ;\n\nescalators , air conditioning systems , telecommunication\n\n[ 0047 ] FIG . 3 is a block diagram of an example device\n\nequipment and exchanges , computer servers , may also oper\n\nmodelling engine of the monitoring engine of FIG . 2 ;\n\nate with examples of the systems for monitoring a device\n\n[ 0048 ] FIG . 4 is a data flow diagram of an example\n\n100 .\n\nmethod for monitoring a device in accordance with another\n\n[ 0059 ]\n\nPreferably , the various examples of systems for\n\nembodiment of the present invention ;\n\nmonitoring a device 100 may be arranged to receive condi\n\n[ 0049 ]\n\nFIG . 5 is a diagram illustrating an example group\n\ntion information from each of the devices 102 that are being\n\ning of operation information into a source domain set and a\n\nmonitored . Such condition information may include any\n\ntarget domain set for training the conditions monitoring\n\ninformation that relates to or otherwise associated with the\n\nengine of FIG . 2 ;\n\ncondition of the device before , during and after operation or\n\n[ 0050 ] FIG . 6 is a block diagram illustrating an example\n\nthe performance of any tasks . Such condition information or\n\nmethod of training one or more learning networks ;\n\ndevice data may be specific to the device itself , including\n\n[ 0051 ] FIG . 7 is a flow diagram illustrating an example\n\noperations status , health checks , operation duration , operat\n\ntraining process of a learning network for use with the\n\ning rates , temperature , surrounding conditions etc or any\n\ndevice modelling engine of FIG . 3 ;\n\ninformation that is associated with the status or health of the\n\n[ 0052 ] FIG . 8 is a diagram illustrating method steps of the\n\ndevice itself . As the person skilled in the art would appre\n\ntraining process of the learning network of FIG . 7 ; and ,\n\nciate , such condition information will depend on the specific\n\n[ 0053 ] FIG . 9 is a diagram illustrating method steps of the\n\ndevice that is being monitored as condition information will\n\ntraining process of the learning network of FIG . 7 .\n\nbe different for a range of different devices .\n\n[ 0060 ]\n\nIn the examples described below , the system 100 is\n\nDETAILED DESCRIPTION OF THE\n\narranged to operate with wind turbines of a wind farm , and\n\nPREFERRED EMBODIMENT\n\nthus the condition information for wind turbines ( the\n\n[ 0054 ]\n\nReferring to FIG . 1 , there is provided an example\n\ndevices ) are relevant to wind turbines generally . Such infor\n\nembodiment of a system for monitoring a device compris\n\nmation may include , without limitations , turbine rotation\n\ning :\n\nrates , gear box rotation rates , gearbox temperature , power\n\nApr. 8, 2021\n\nUS 2021/0102527 Al\n\n[0034] In an embodiment of the second aspect, the stop- ping criterion is determined based on a reconstruction error obtained from the deep neural network.\n\n[0055] a device conditions gateway arranged to operation information from a device, wherein the operation information is associated with the condition of the device in operation; and\n\n[0035] In an embodiment of the second aspect, the recon- struction error is obtained by inputting the domain data the deep neural network.\n\n[0056] a conditions monitoring engine arranged to pro- cess the operation information with a device modelling engine to determine one or more operation conditions of the device.\n\n[0036] In an embodiment of the second aspect, the struc- of the deep neural network is adjusted by inserting one more hidden layers into the deep neural network.\n\n[0057] In this example embodiment, the system for moni- toring a device 100 includes at least a device conditions gateway 104 and a conditions monitoring engine 106. The device conditions gateway 104 is arranged to obtain opera- tion information for one or more devices 102 whilst the conditions monitoring engine 106 is arranged to process this operation information for the one or more devices 102 so as to determine one or more operation conditions for the device or devices 102. These operation conditions in turn can be used for further processing or user review so as to determine the status and condition of the device itself to diagnose, predict or solve problems for the device 108. Preferably, the conditions monitoring engine 106 includes a device model- ling engine 204 which is arranged to process the operation information by use of a generated model associated with the device 102 being monitored so as to devise one or more operations of the device 102.\n\n[0037] In an embodiment of the second aspect, the one or more hidden layers inserted by adding one or more hidden nodes into the hidden layers.\n\n[0038] In an embodiment of the second aspect, weights biases of the deep neural network are manipulated when neural network is adjusted.\n\n[0039] In an embodiment of the second aspect, the device is a wind turbine.\n\n[0040] In an embodiment of the second aspect, the system further comprises a pre-processor arranged to process the operation information obtained from the device conditions gateway.\n\n[0041] In an embodiment of the second aspect, the opera- tion information is pre-processed before the information is inputted to the device modelling engine.\n\n[0042] In an embodiment of the second aspect, the opera- information is pre-processed to remove fault data from operation information.\n\n[0058] Example embodiments of the system for monitor- ing a device 100 may be used to monitor the conditions of device 102. Preferably, although not necessarily, the device 102 would operate with a plurality of other associated devices 102 to perform a certain task or goal. Such devices 102 may include any equipment, fixture, plant, vehicle or apparatus that may be arranged to operate, individually, partially or completely together to perform one or more tasks. Examples of such devices may include wind turbines in a wind farm, as illustrated in some of the example embodiments described below. Other devices, such as coal powered generators, nuclear power generators, trains, drill- ing/boring machines, transportation systems, signalling sys- tems, oil rigs, pumps, radar arrays, radio transmitters, lifts/ escalators, air conditioning systems, telecommunication equipment and exchanges, computer servers, may also oper- ate with examples of the systems for monitoring a device 100.\n\n[0043] In an embodiment of the second aspect, the device conditions gateway is a SCADA system.\n\nBRIEF DESCRIPTION OF THE DRAWINGS\n\n[0044] Embodiments of the present invention will now be described, by way of example, with reference to the accom- panying drawings in which:\n\n[0045] FIG. 1 is a block diagram illustrating a system for monitoring a device in accordance with one embodiment present invention;\n\n[0046] FIG. 2 is a block diagram of an example conditions monitoring engine as used in the system of FIG. 1;\n\n[0047] FIG. 3 is a block diagram of an example device engine of the monitoring engine of FIG. 2;\n\n[0048] FIG. 4 is a data flow diagram of an example method for monitoring a device in accordance with another embodiment of the present invention;\n\n[0059] Preferably, the various examples of systems for monitoring a device 100 may be arranged to receive condi- tion information from each of the devices 102 that are being monitored. Such condition information may include any information that relates to or otherwise associated with the condition of the device before, during and after operation or the performance of any tasks. Such condition information or device data may be specific to the device itself, including operations status, health checks, operation duration, operat- ing rates, temperature, surrounding conditions ete or any information that is associated with the status or health of the device itself. As the person skilled in the art would appre- ciate, such condition information will depend on the specific device that is being monitored as condition information will be different for a range of different devices.\n\n[0049] FIG. 5 is a diagram illustrating an example group- ing of operation information into a source domain set and a target domain set for training the conditions monitoring engine of FIG. 2;\n\n[0050] FIG. 6 is a block diagram illustrating an example method of training one or more learning networks;\n\n[0051] FIG. 7 is a flow diagram illustrating an example training process of a learning network for use with the device modelling engine of FIG. 3;\n\n[0052] FIG. 8 is a diagram illustrating method steps of the training process of the learning network of FIG. 7; and,\n\n[0053] FIG. 9 is a diagram illustrating method steps of the training process of the learning network of FIG. 7.\n\n[0060] In the examples described below, the system 100 is arranged to operate with wind turbines of a wind farm, and thus the condition information for wind turbines (the devices) are relevant to wind turbines generally. Such infor- mation may include, without limitations, turbine rotation rates, gear box rotation rates, gearbox temperature, power\n\nDETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT\n\n[0054] Referring to FIG. 1, there is provided an example embodiment of a system for monitoring a device compris-\n\nset\n\ninto\n\nture\n\nor\n\nor\n\nthe\n\ndeep\n\ntion the\n\nof\n\nthe\n\nmodelling\n\ning:\n\nobtain\n\na\n\nApr. 8 , 2021\n\nUS 2021/0102527 A1\n\n3\n\ngeneration rates , surrounding wind speeds , surrounding\n\nthat a conditions report regarding the device can be provided\n\nwind deviations , detectable vibrations , tower accelerations ,\n\nto a user or for further processing 208 .\n\ndrive train accelerations , torque change rates , blade pitch\n\n[ 0065 ] As shown in FIG . 2 , the pre - processing module 202\n\nangles , torque measured at turbine and gearbox , etc. These\n\nis arranged to filter out any fault data , noise or any unnec\n\nvariables may be measured periodically and stored over a\n\nessary data . As it will be described with reference to FIGS .\n\ntime period for further analysis and processing to determine\n\n3 to 9 , as the device modelling engine 204 is preferably\n\nor estimate the wind turbine's health , operation status or for\n\nimplemented by use of a machine learning architecture ,\n\nthe prediction of operation characteristics or forecasts of\n\nincluding a plurality of learning networks , the removal of\n\ncertain operation results or problems . As it will be described\n\nany zero ( null ) data , fault data or noise by the pre - processing\n\nbelow , embodiments of the system for monitoring a device\n\nmodule is optional , but may improve the ability to train or\n\nmay be used to process these variables of the condition\n\npattern match within these machine learning processes . This\n\ninformation for a wind turbine so as to report and to report\n\nwill in turn improve the accuracy of the device modelling\n\nand to predict the wind turbine's operation condition .\n\nengine 204 to acquire the necessary knowledge to perform\n\na suitable pattern matching or classification process in its\n\n[ 0061 ] As shown in FIG . 1 , the operation information of\n\nsubsequent processing of the operation information to deter\n\neach of the one or more devices is collected by a device\n\nmine operation conditions of the device .\n\nconditions gateway 104. The device conditions gateway 104\n\nmay be a centralised system arranged to communicate with\n\n[ 0066 ] Once the operation information has been pre - pro\n\nvarious sensors that are operating with each of the one more\n\ncessed , the information is in turn inputted into the device\n\ndevices to take various measurements relevant to the opera\n\nmodelling engine 204 which is arranged to model the device\n\ntion condition of the device . Preferably , an example of a\n\nto which the information is associated with . The device\n\ndevice conditions gateway 104 may include part or all of a\n\nmodelling engine 204 may include a machine learning\n\nSupervisory Control and Data Acquisition ( SCADA ) system\n\nsystem which uses machine learning to recognize the param\n\nwhich is used to monitor and control various devices that are\n\neters and operations of a particular device after training as a\n\nin operation . Such devices may include plant , equipment ,\n\nparticular state or classification , and thus the different con\n\ndevices or appliances in various industries or applications .\n\nditions of the device , after training , can be recognized within\n\nThe SCADA system may be implemented to obtain various\n\na level of accuracy based on operation information that is fed\n\noperation parameters which are specific to the devices that\n\nwithin it .\n\nare being monitored and controlled by the SCADA systems .\n\n[ 0067 ]\n\nPreferably , the device modelling engine 204\n\n[ 0062 ] As the device conditions gateway 104 obtains\n\nincludes a plurality of learning networks , each specifically\n\nvarious parameters collectively as condition information of\n\nstructured and trained for each individual devices of a\n\nplurality of devices . Thus in the examples described below ,\n\nthe devices , the condition information is then communicated\n\nto the conditions monitoring engine for processing . In this\n\nwhere the device is a wind turbine in a wind farm which has\n\nexample embodiment , the conditions monitoring engine 106\n\nmultiple devices ( wind turbines ) , each of the plurality of\n\nis arranged to process the condition information obtained by\n\nlearning networks may be individually structured and\n\nthe conditions gateway so to devise and report a specific\n\ntrained to operate with a specific wind turbine of the wind\n\ncondition for the device . This specific condition could be a\n\nfarm . In turn , when operation information obtained for a\n\npresent condition as well as a future condition that may be\n\nspecific wind turbine is fed into an associated learning\n\npredicted based on the condition information that is obtained\n\nnetwork , the operation conditions of the specific wind tur\n\nby the device conditions gateway and processed by the\n\nbine can be modelled by the learning network and thus any\n\ncondition information engine 106 .\n\nabnormal functionalities or status updates of each wind\n\nturbine can be determined or predicted based on the match\n\n[ 0063 ] The example embodiments of the system for moni\n\ning of the operation information against the modelling of the\n\ntoring a device 100 may be advantageous as it allows for the\n\nwind turbine by the learning network .\n\ndetection and prediction of any operation conditions of a\n\ndevice . By processing the operation information , any abnor\n\n[ 0068 ] With reference to FIG . 3 , there is illustrated a block\n\ndiagram of an example device modelling engine 204. In this\n\nmal conditions for a device may be identified quickly or\n\nembodiment , the engine 204 includes a plurality of learning\n\npredicted such that suitable actions can be undertaken to\n\nreduce the risk of damage or to maintain the safe operation\n\nnetworks 304 ( network 1 , network i and network N ) , denot\n\nof the device .\n\ning the first network , the ith network and the Nth network .\n\nThe number of learning networks 304 that are included\n\n[ 0064 ] With reference to FIG . 2 , there is illustrated a block\n\nwithin the modelling engine 204 may preferably be the\n\ndiagram of the conditions monitoring engine 106 which is\n\nnumber of devices that are being monitored by the system .\n\narranged to process the operation information of the devices\n\nThese learning networks are arranged to model an individual\n\nwhen it is obtained by the device conditions gateway . In this\n\ndevice which is being monitored and thus when operation\n\nexample embodiment , the device modelling engine includes\n\ninformation 302 are fed into each of these learning networks\n\nan optional pre - processing module 202 arranged to pre\n\n304 , the learning networks 304 are arranged to identify\n\nprocess the conditions information before inputting the\n\nspecific patterns which have matched the combination of the\n\npre - processed conditions information into the device mod\n\noperation information that has been fed into the network and\n\nelling engine 204 where the conditions information is\n\nin turn , provide an output that may be one or more classi\n\napplied to a previously generated model representative of\n\nfications of a particular condition of the device itself .\n\nthe device so as to determine or predict the operation\n\n[ 0069 ] Once these matches are found , existing knowledge\n\nconditions of the device as based on the condition informa\n\nstored within each network 304 as generated by past training\n\ntion obtained for the device . In turn , when this determination\n\nor prediction is made , the abnormality detection module 206\n\nmay classify one or more conditions of the device . Such\n\nis arranged to process this determination or prediction such\n\nconditions may in turn infer the existing or forecast condi\n\nUS 2021/0102527 Al\n\nApr. 8, 2021\n\na conditions report regarding the device can be provided a user or for further processing 208.\n\ngeneration rates, surrounding wind speeds, surrounding wind deviations, detectable vibrations, tower accelerations, drive train accelerations, torque change rates, blade pitch angles, torque measured at turbine and gearbox, etc. These variables may be measured periodically and stored over a time period for further analysis and processing to determine or estimate the wind turbine\u2019s health, operation status or for the prediction of operation characteristics or forecasts of certain operation results or problems. As it will be described below, embodiments of the system for monitoring a device may be used to process these variables of the condition information for a wind turbine so as to report and to report and to predict the wind turbine\u2019s operation condition.\n\n[0065] As shown in FIG. 2, the pre-processing module 202 is arranged to filter out any fault data, noise or any unnec- essary data. As it will be described with reference to FIGS. 3 to 9, as the device modelling engine 204 is preferably implemented by use of a machine learning architecture, including a plurality of learning networks, the removal of any zero (null) data, fault data or noise by the pre-processing module is optional, but may improve the ability to train or pattern match within these machine learning processes. This will in turn improve the accuracy of the device modelling engine 204 to acquire the necessary knowledge to perform a suitable pattern matching or classification process in its subsequent processing of the operation information to deter- mine operation conditions of the device.\n\n[0061] As shown in FIG. 1, the operation information of each of the one or more devices is collected by a device conditions gateway 104. The device conditions gateway 104 may be a centralised system arranged to communicate with various sensors that are operating with each of the one more devices to take various measurements relevant to the opera- tion condition of the device. Preferably, an example of a device conditions gateway 104 may include part or all of a Supervisory Control and Data Acquisition (SCADA) system which is used to monitor and control various devices that are in operation. Such devices may include plant, equipment, devices or appliances in various industries or applications. The SCADA system may be implemented to obtain various operation parameters which are specific to the devices that are being monitored and controlled by the SCADA systems.\n\n[0066] Once the operation information has been pre-pro- cessed, the information is in turn inputted into the device modelling engine 204 which is arranged to model the device to which the information is associated with. The device modelling engine 204 may include a machine learning system which uses machine learning to recognize the param- eters and operations of a particular device after training as a particular state or classification, and thus the different con- ditions of the device, after training, can be recognized within a level of accuracy based on operation information that is fed. within it.\n\n[0067] Preferably, the device modelling engine 204 includes a plurality of learning networks, each specifically structured and trained for each individual devices of a plurality of devices. Thus in the examples described below, where the device is a wind turbine in a wind farm which has multiple devices (wind turbines), each of the plurality of learning networks may be individually structured and trained to operate with a specific wind turbine of the wind. farm. In turn, when operation information obtained for a specific wind turbine is fed into an associated learning network, the operation conditions of the specific wind tur- bine can be modelled by the learning network and thus any abnormal functionalities or status updates of each wind. turbine can be determined or predicted based on the match- ing of the operation information against the modelling of the\n\n[0062] As the device conditions gateway 104 obtains various parameters collectively as condition information of the devices, the condition information is then communicated to the conditions monitoring engine for processing. In this example embodiment, the conditions monitoring engine 106 is arranged to process the condition information obtained by the conditions gateway so to devise and report a specific condition for the device. This specific condition could be a present condition as well as a future condition that may be predicted based on the condition information that is obtained by the device conditions gateway and processed by the condition information engine 106.\n\n[0063] The example embodiments of the system for moni- toring a device 100 may be advantageous as it allows for the detection and prediction of any operation conditions of a device. By processing the operation information, any abnor- mal conditions for a device may be identified quickly or predicted such that suitable actions can be undertaken to reduce the risk of damage or to maintain the safe operation of the device.\n\n[0068] With reference to FIG. 3, there is illustrated a block diagram of an example device modelling engine 204. In this embodiment, the engine 204 includes a plurality of learning networks 304 (network 1, network i and network N), denot- ing the first network, the ith network and the Nth network. The number of learning networks 304 that are included within the modelling engine 204 may preferably be the number of devices that are being monitored by the system. These learning networks are arranged to model an individual device which is being monitored and thus when operation information 302 are fed into each of these learning networks 304, the learning networks 304 are arranged to identify specific patterns which have matched the combination of the operation information that has been fed into the network and in turn, provide an output that may be one or more classi- fications of condition of the device itself.\n\n[0064] With reference to FIG. 2, there is illustrated a block diagram of the conditions monitoring engine 106 which is arranged to process the operation information of the devices when it is obtained by the device conditions gateway. In this example embodiment, the device modelling engine includes an optional pre-processing module 202 arranged to pre- process the conditions information before inputting the pre-processed conditions information into the device mod- elling engine 204 where the conditions information is applied to a previously generated model representative of the device so as to determine or predict the operation conditions of the device as based on the condition informa- tion obtained for the device. In turn, when this determination or prediction is made, the abnormality detection module 206 is arranged to this determination or prediction such\n\n[0069] Once these matches are found, existing knowledge stored within each network 304 as generated by past training may classify one or more conditions of the device. Such conditions may in turn infer the existing or forecast condi-\n\nprocess\n\nthat\n\nto\n\nwind turbine by the learning network.\n\na particular\n\nApr. 8 , 2021\n\nUS 2021/0102527 A1\n\n4\n\nturbines , may not need to be carefully selected but prefer\n\ntions of the device in operation and thus a report on the\n\nconditions of the device can be generated .\n\nably , the features which are filtered out may include those\n\nparameters which remain constant in all records of a wind\n\n[ 0070 ]\n\nIn one example embodiment , a reconstruction error\n\nturbine . The data may then be scaled to the range in [ 0 , 1 ]\n\n( RE ) 310 can be obtained by comparing the output of the\n\nlearning networks 304 against measured conditions of the\n\nor the like based on the maximum and minimal value of each\n\nfeature such that they can be consistently represented when\n\ndevice in operation . This comparison , which can be pre\n\nprocessed by the learning networks .\n\nsented as a RE value 304 or vector can in turn be an\n\nindication of any abnormalities in the operation of the device\n\n[ 0077 ] Once the data has been scaled to the ranges , the\n\nand can be processed by a RE processor 306. This is further\n\ndata is then grouped 406 for model training 408. The\n\ngrouping process 406 is arranged to separate the operation\n\ndescribed below with reference to FIGS . 4 and 7 where the\n\nRE is compared with an upper limit and a lower limit , and\n\ndata into a source domain set and a target domain set . The\n\nsource domain set may comprise of a portion of the opera\n\nwhere the RE falls outside of these limits , would therefore\n\nindicate an abnormal operation condition with the device .\n\ntion information from a plurality of wind turbines and thus\n\nform a common training set that would train a learning\n\n[ 0071 ] With reference to FIG . 4 , there is illustrated another\n\nexample of a method for monitoring a device . This method\n\nnetwork with operation information that is shared amongst\n\na plurality of wind turbines . This is advantageous as a\n\nwill be described with reference to FIGS . 4 to 9 and will\n\nplurality of wind turbines are likely to collectively provide\n\nrefer to the usage of the method for monitoring wind\n\nturbines , although as it would be appreciated by a person\n\na much larger data set for training , although the disadvan\n\nskilled in the art , any other devices , plant , equipment or\n\ntage is that such a training set would be for wind turbines\n\napparatus may also be monitored .\n\ngenerally , and not specific to an individual turbines .\n\n[ 0078 ] The target domain data set may then be used to\n\n[ 0072 ] As shown , this method includes the process of\n\ntraining the plurality of learning networks , which once\n\nfurther train the learning network after the training is per\n\ntrained , may no longer be necessary to operate the method\n\nformed with the source domain set . In this process , the\n\nfor monitoring the devices . However , periodic or on - going\n\nsource domain set trained learning network is then \u201c fine\n\ntuned \u201d individually to each wind turbine as the target\n\ntraining may increase further knowledge within the machine\n\nlearning structure of the system and method for monitoring\n\ndomain data is obtained from each individual wind turbine .\n\ndevices and therefore increasing the accuracy of the method\n\nIn this way , the source domain set trained learning network\n\nin operation .\n\nis then adapted or transferred into an individual learning\n\n[ 0073 ]\n\nIn this example embodiment , the method includes\n\nnetwork for each individual wind turbine . This is advanta\n\ngeous as the domain adaptation or transferred learning\n\nfive steps 402 , 404 , 406 , 408 , 410. These are operation\n\nprocess allows a trained learning network with knowledge\n\ninformation collection ( data collection ) 402 , operation infor\n\ndeveloped by training with a larger set of data ( many wind\n\nmation pre - processing ( data pre - processing ) 404 , operation\n\nturbines ) to be specifically adapted or transferred to work\n\ninformation grouping ( data grouping ) 406 , device model\n\ntraining 408 and condition monitoring 410 .\n\nwith an individual wind turbine that would on its own have\n\na much small set of training data . These processes , including\n\n[ 0074 ]\n\nInitially , when the method is started for the moni\n\nthe grouping of operation information 406 into source\n\ntoring of devices , operation information or operation data\n\ndomain data set and target data set as well as the training\n\nfor each of the devices are first collected . This information\n\nprocess 408 is further described below with reference to\n\nmay then be processed by the device modelling engine\n\nFIGS . 5 to 9 .\n\nmentioned earlier with reference to FIGS . 1 to 3 should the\n\n[ 0079 ] With reference to FIG . 5 , there is shown an\n\nmodels have already been generated . However , in the first\n\nexample embodiment 500 of data grouping 406 for the\n\ninstance when these models have not yet been generated ,\n\nmethod of monitoring a device as shown in FIG . 4. In this\n\nsteps 402 , 404 , 406 and 408 are therefore undertaken so as\n\nexample , the method is arranged to monitor a plurality of\n\nto generate the models of the devices which will be used to\n\nwind turbines 502 of a commercial wind farm having K\n\nperform the processing of the operation information .\n\nwind turbines ( WT ) .\n\n[ 0075 ]\n\nThis step of generating the models of the devices is\n\npreferably performed by training a machine learning system\n\n[ 0080 ] Accordingly , the SCADA data of K WTs 504 are\n\ncollected 402 and pre - processed 404 through the mentioned\n\nhaving learning networks such as deep neural networks\n\nprocess above which in this embodiment , the data 504 can\n\n( DNN ) . In a first instance , operation information or opera\n\nbe represented as D { i } , i = 1 , 2 , ... , K. Suppose that D { i } , i = 1 ,\n\ntion data of each of the devices may be firstly obtained from\n\n2 , ... , K , has the identical r - dimensional feature space ( a\n\na control or monitor system such as SCADA systems of the\n\nset of SCADA parameters ) and can be further expressed as\n\nwind farm ( 402 ) . The data may then be transformed and\n\nD { i } = { X , { i } , x { i } ,\n\nXx { } } , where M means the total\n\nprocessed into files , such as spreadsheet files that may\n\nnumber of data points in D { i } .\n\nenable the data pre - processing 404 and further model train\n\ning 408 which follows .\n\n[ 0081 ] As shown in FIGS . 5 and 6 , an example of the\n\n[ 0076 ] The operation data may then be pre - processed 404 ,\n\nfacilitation of a transfer learning process is shown . The\n\nalthough as it would be appreciated , since a machine learn\n\nprocess includes two phases : a source domain learning phase\n\ning system is used to process the data , the pre - processing of\n\n602 and a target domain learning phase 604. The source\n\ndata 404 is optional and thus data which are not pre\n\ndomain learning phase 602 is arranged to pre - train a model\n\nprocessed may nonetheless operate with a machine learning\n\nthat extracts the common data patterns existing among all\n\nsystems . The pre - processing process 404 may include a step\n\nWTs 502 , while the target domain learning phase can\n\nof filter out any fault operating data in accordance with\n\ncustomize the unique characteristics of each WT by fine\n\ntuning the pre - trained model using the individual dataset and\n\nrecords as found in an associated fault log and thus leaving\n\nonly the data under normal condition . As an example ,\n\nthus allowing the individual pre - trained model , which has\n\nSCADA parameters , e.g. features for modelling the wind\n\nbeen fine tuned to a specific WT to model that specific WT .\n\nUS 2021/0102527 Al\n\nApr. 8, 2021\n\nturbines, may not need to be carefully selected but prefer- ably, the features which are filtered out may include those parameters which remain constant in all records of a wind turbine. The data may then be scaled to the range in [0, 1] or the like based on the maximum and minimal value of each feature such that they can be consistently represented when processed by the learning networks.\n\nof the device in operation and thus a report on conditions of the device can be generated.\n\n[0070] In one example embodiment, a reconstruction error (RE) 310 can be obtained by comparing the output of the learning networks 304 against measured conditions of the device in operation. This comparison, which can be pre- sented as a RE value 304 or vector can in turn be an indication of any abnormalities in the operation of the device and can be processed by a RE processor 306. This is further described below with reference to FIGS. 4 and 7 where the RE is compared with an upper limit and a lower limit, and where the RE falls outside of these limits, would therefore indicate an abnormal operation condition with the device.\n\n[0077] Once the data has been scaled to the ranges, the data is then grouped 406 for model training 408. The grouping process 406 is arranged to separate the operation data into a source domain set and a target domain set. The source domain set may comprise of a portion of the opera- tion information from a plurality of wind turbines and thus form a common training set that would train a learning network with operation information that is shared amongst a plurality of wind turbines. This is advantageous as a lurality of wind turbines are likely to collectively provide a much larger data set for training, although the disadvan- tage is that such a training set would be for wind turbines generally, and not specific to an individual turbines.\n\n[0071] With reference to FIG. 4, there is illustrated another example of a method for monitoring a device. This method will be described with reference to FIGS. 4 to 9 and will refer to the usage of the method for monitoring wind turbines, although as it would be appreciated by a person skilled in the art, any other devices, plant, equipment or apparatus may also be monitored.\n\n[0072] As shown, this method includes the process o: training the plurality of learning networks, which once trained, may no longer be necessary to operate the method for monitoring the devices. However, periodic or on-going training may increase further knowledge within the machine learning structure of the system and method for monitoring devices and therefore increasing the accuracy of the method in operation.\n\n[0078] The target domain data set may then be used to further train the learning network after the training is per- formed with the source domain set. In this process, the source domain set trained learning network is then \u201cfine tuned\u201d individually to each wind turbine as the target domain data is obtained from each individual wind turbine. in this way, the source domain set trained learning network is then adapted or transferred into an individual learning network for each individual wind turbine. This is advanta- geous as the domain adaptation or transferred learning process allows a trained learning network with knowledge developed by training with a larger set of data (many wind turbines) to be specifically adapted or transferred to work with an individual wind turbine that would on its own have much small set of training data. These processes, including the grouping of operation information 406 into source domain data set and target data set as well as the training process 408 is further described below with reference to FIGS. 5 to 9.\n\n[0073] In this example embodiment, the method includes five steps 402, 404, 406, 408, 410. These are operation information collection (data collection) 402, operation infor- mation pre-processing (data pre-processing) 404, operation information grouping (data grouping) 406, device model training 408 and condition monitoring 410.\n\n[0074] Initially, when the method is started for the moni- toring of devices, operation information or operation data for each of the devices are first collected. This information may then be processed by the device modelling engine mentioned earlier with reference to FIGS. 1 to 3 should the models have already been generated. However, in the first instance when these models have not yet been generated, steps 402, 404, 406 and 408 are therefore undertaken so as to generate the models of the devices which will be used to perform the processing of the operation information.\n\n[0079] With reference to FIG. 5, there is shown an example embodiment 500 of data grouping 406 for the method of monitoring a device as shown in FIG. 4. In this example, the method is arranged to monitor a plurality of wind turbines 502 of a commercial wind farm having K wind turbines (WT).\n\n[0075] This step of generating the models of the devices is preferably performed by training a machine learning system having learning networks such as deep neural networks (DNN). In a first instance, operation information or opera- tion data of each of the devices may be firstly obtained from a control or monitor system such as SCADA systems of the wind farm (402). The data may then be transformed and processed into files, such as spreadsheet files that may enable the data pre-processing 404 and further model train- ing 408 which follows.\n\n[0081] As shown in FIGS. 5 and 6, an example of the facilitation of a transfer learning process is shown. The process includes two phases: a source domain learning phase 602 and a target domain learning phase 604. The source domain learning phase 602 is arranged to pre-train a model that extracts the common data patterns existing among all WTS 502, while the target domain learning phase can customize the unique characteristics of each WT by fine- tuning the pre-trained model using the individual dataset and thus allowing the individual pre-trained model, which has been fine tuned to a specific WT to model that specific WT.\n\n[0076] The operation data may then be pre-processed 404, although as it would be appreciated, since a machine learn- ing system is used to process the data, the pre-processing of data 404 is optional and thus data which are not pre- processed may nonetheless operate with a machine learning systems. The pre-processing process 404 may include a step of filter out any fault operating data in accordance with records as found in an associated fault log and thus leaving only the data under normal condition. As an example, SCADA parameters, e.g. features for modelling the wind\n\ntions\n\nthe\n\na\n\n[0080] Accordingly, the SCADA data of K WTs 504 are collected 402 and pre-processed 404 through the mentioned process above which in this embodiment, the data 504 can be represented as pb} i=1,2,...,K. Suppose that pD\u00ae i=, 2,...,K, has the identical r-dimensional feature space (a set of SCADA parameters) and can be further expressed as D&=(x,, x, . 2. X,\"}, where M means the total number of data points in Dt.\n\nApr. 8 , 2021\n\nUS 2021/0102527 A1\n\n5\n\nHowever as the learning process begins , a hidden layer with\n\n[ 0082 ] To enable the two - phase training scheme , the\n\nsource domain 602 and target domain 604 can be con\n\nseveral hidden nodes is inserted to the network 704 , refer\n\nstructed as shown in FIGS . 5 and 6 , where D { i } is divided\n\nring to the step 1. The number of hidden nodes on the\n\ninserted hidden layer is next determined 706 at the step 2 .\n\ninto two parts :\n\nThe step 1 704 and step 2 706 will be explained in detail later\n\n[ 0083 ] one for constructing the source domain 510 , and\n\n[ 0084 ]\n\nthe other for its own target domain 512 .\n\nwith reference to FIGS . 8 and 9 .\n\n[ 0085 ] The source domain 510 may contain the parts of\n\n[ 0098 ] Afterwards , the reconstruction error ( RE ) on vali\n\ndata from all WTs is further divided to source domain\n\ndation set ,\n\ntraining set Dst 514 and source domain validation set Ds\n\n516. The target domain of the i - th WT 512 is split into\n\nfine - tuning set D , { i } 518 and test set Deli } 520 .\n\nR = . | X ; \u2013 8 ; 17\n\n[ 0086 ] Once the data grouping is performed , the model\n\ntraining process is ready to begin . This step is to train each\n\nof the learning networks such that they can model each\n\n[ 0099 ] where \u00d1 , denotes the j - th reconstructed data point\n\nindividual WT . Various types of machine learning networks\n\nof Ds , and M , means the data size of Dev , is computed at step\n\ncan be used but preferably , a deep neural network ( DNN ) is\n\n3 to evaluate model performance 708. The network expan\n\ntrained for each WT with both the source domain 602 and\n\nsion procedure from step 1 through 3 may then be repeated ,\n\nthe target domain data 604. In this example embodiment as\n\nuntil R , continuously increases for 0 times 710. Finally , the\n\nshown in FIGS . 5 and 6 , the DNNs 606 may consist of an\n\nnetwork causing the minimal R , in search history will be\n\ninput layer , one or more hidden layers and an output layer\n\nselected as the base DNN 712 .\n\nwith the purpose of reconstructing the input data itself .\n\n[ 0100 ] The manipulation of the DNN is represented by the\n\n[ 008 ]\n\nSolving such an example of a modelling problem is\n\ndiagrams of FIGS . 8 and 9 , which shows a visual represen\n\nequivalent to minimizing a reconstruction loss ( RL ) ,\n\ntation of the method of inserting a hidden layer , i.e. the step\n\n1 702 in FIG . 7. Consider a DNN containing hidden layers\n\nj { i } = [ { Y { i } - $ { i } | 2\n\n802 , whose weights and biases can be represented as W , and\n\n[ 0088 ] where \u00ca { i } denotes the reconstructed feature space\n\ndenote the i - th weight and bias\n\nBi , while W\n\nand B ( 1,1 )\n\nby the DNN model .\n\n( 1 , 1 )\n\nmatrices in W , and B? , i = 1 , 2 , ... , 1 + 1 . A new hidden layer\n\n[ 0089 ] The DNN for the i - th WT may therefore be param\n\n804 may be inserted between the hidden layer 1 802 and\n\neterized by 608 :\n\noutput layer 806. In such operation , all elements of W , and\n\n{ L , N , wit , Bit )\n\nB , except W ( 1 , 2 + 1 )\n\nare firstly inherited to W and\n\nand B ( 1,2 + 1 )\n\n1 + 1\n\nB2-1 . Meanwhile , weights , { We , Wa } and biases , { Be , Ba } ,\n\n[ 0090 ] where L refers to the total number of hidden layers ,\n\nproduced due to the insertion of a new hidden layer are also\n\n10091 ]\n\nN = { No , N1 , ... , NZ + 1 } means the number of nodes\n\nincluded into W + 1 and B2 + 1 , where e stands for the encoder\n\nof all layers with\n\nand d stands for the decoder .\n\n[ 0092 ] No = N2 + 1 Fr corresponding to the input layer and\n\noutput layer . W { i } = { W , { i } , w , { i } ,\n\nW1 + 1 { i } } and\n\n[ 0101 ] The initial number of hidden nodes on this layer\n\nmay be determined based on Principle Component Analysis\n\n2\n\nB { i } = { B , { i } , B , { i }\n\nBL + 1 { } } present weights and bias\n\n( PCA ) technique , which is a classical dimension reduction\n\ndistributed across connections among all layers of the DNN\n\ntechnique . A preferred embodiment of PCA in this invention\n\nmodels 606 .\n\nis as follows :\n\n[ 0093 ] Such parameterization is motivated by an assump\n\n[ 0102 ]\n\n1 - compute the output of the hidden layer 1 by\n\ntion that the homogeneous patterns and heterogeneous char\n\nfeeding Dst into the original neural network using feedfor\n\nacteristics among K WTs can be modelled by a compro\n\nward algorithm .\n\nmised ( L , N } and differentiated { W { } ,\n\n} ,\n\n[ 0103 ]\n\n2 \u2014 Set the initial number of hidden nodes of the\n\n[ 0094 ] FIG . 6 shows a general framework of an example\n\nsaid layer which may be initialized as the number of the\n\nmodel training process 600. In source domain learning , Dst\n\ngreatest eigenvalues that can make up 80 % of the sum of all\n\n514 and Ds , 516 according to FIG . 5 are used to train a base\n\neigenvalues obtained by PCA .\n\nDNN 606. The DNN structure may be self - organizing\n\n[ 0104 ] 3 \u2014 Determining the dimension of { We , Wa } and\n\nwithout a prior specification , in which details will be elabo\n\n{ B , BC ) .\n\nrated below with FIGS . 7 to 9 .\n\n[ 0105 ] FIG . 9 shows the method of determining the num\n\n[ 0095 ] Hyper - parameters of the base DNN 606 , { L , N ,\n\nW { 0 } , B { C } } 608 , may be automatically determined by two\n\nber of hidden nodes , denoted as N2 + 1 , after inserting a\n\ncompact hidden layer 1 + 1 referring to the step 2 706 in FIG .\n\npre - specified thresholds , e , and 0 , 612. Then , in target\n\n{ i } , B { } i ,\n\n7. The expansion of the inserted hidden layer contains three\n\nK , can\n\nbe finally obtained by optimizing the network parameters\n\nsteps :\n\nusing backpropagation ( BP ) method based on { W { 0 } , B { 0 } }\n\n[ 0106 ]\n\nStep 1. Initialize 802 : add one or some hidden\n\nnodes on the inserted hidden layer . Denote the weights\n\nas the initial values .\n\nconnecting the hidden layer 1 and these new hidden nodes as\n\n[ 0096 ] FIG . 7 shows an example process of the source\n\nWeu , and the weights connecting the new hidden nodes and\n\ndomain training 700. In this example , the DNN structure in\n\nsource domain learning is determined by self - expanding an\n\nthe output layer as W\n\nand the biases of the new hidden\n\ndu '\n\ninitial network through continuously stacking hidden layers\n\nnodes as Beu . At each update , Weu Wdu and Beu will be\n\nrandomly initialized and aggregated to the We , W. and Be ,\n\nand units until a suitable size is achieved . The hyperparam\n\nrespectively .\n\neter 0 , and 62 612 referred to in FIG . 6 may assist in deciding\n\nwhen to stop adding new hidden layers or hidden nodes .\n\n[ 0107 ]\n\nStep 2. Update 804 : train the weights and biases of\n\nthe augmented partial network noted in FIG . 9. We , W. Be\n\n[ 0097 ] At the beginning of source domain learning , a\n\nneural network may start with no hidden layers inside 702 .\n\nand B , will be trained using the gradient descent method\n\nUS 2021/0102527 Al\n\nApr. 8, 2021\n\nHowever as the learning process begins, a hidden layer with several hidden nodes is inserted to the network 704, refer- ring to the step 1. The number of hidden nodes on the inserted hidden layer is next determined 706 at the step 2. The step 1 704 and step 2 706 will be explained in detail later with reference to FIGS. 8 and 9.\n\n[0082] To enable the two-phase training scheme, source domain 602 and target domain 604 can be con- structed as shown in FIGS. 5 and 6, where DB is divided into two parts:\n\n[0084] the other for its own target domain 512.\n\n[0085] The source domain 510 may contain the parts of data from all WTs is further divided to source domain training set D,, 514 and source domain validation set D,,, 516. The target domain of the i-th WT 512 is split into fine-tuning set Di 518 and test set D,,\u201c 520.\n\n[0098] Afterwards, the reconstruction error (RE) on vali- dation set,\n\n1 \u958b Ry = Fp Dale RIP\n\n[0086] Once the data grouping is performed, the model training process is ready to begin. This step is to train each of the learning networks such that they can model each individual WT. Various types of machine learning networks can be used but preferably, a deep neural network (DNN) is trained for each WT with both the source domain 602 and the target domain data 604. In this example embodiment as shown in FIGS. 5 and 6, the DNNs 606 may consist of an input layer, one or more hidden layers and an output layer with the purpose of reconstructing the input data itself.\n\n[0099] where x, denotes the j-th reconstructed data point of D,,, and M,, means the data size of D,,, is computed at step 3 to evaluate model performance 708. The network expan- sion procedure from step 1 through 3 may then be repeated, until R, continuously increases for gl times 710. Finally, the network causing the minimal R,, in search history will be selected as the base DNN 712.\n\n[0100] The manipulation of the DNN is represented by the diagrams of FIGS. 8 and 9, which shows a visual represen- tation of the method of inserting a hidden layer, i.e. the step 1 702 in FIG. 7. Consider a DNN containing hidden layers 802, whose weights and biases can be represented as Wi and B, while Wy, , and Bg, \u00bb denote the i-th weight and bias matrices in W, and B,, i=1, 2,... , 1+1. Anew hidden layer 804 may be inserted between the hidden layer 1 802 and output layer 806. In such operation, all elements of W, and B, except Wu \u00bb41) and By, 241) are firstly inherited to W,,, and B,,1- Meanwhile, weights, {W,, W,} and biases, {B,, B,}. produced due to the insertion of a new hidden layer are also included into W,,, and B;,,, where e stands for the encoder and d stands for the decoder.\n\n[0087] Solving such an example of a modelling problem equivalent to minimizing a reconstruction loss (RL),\n\ngta?\n\n[0088] where \u6587 \u54c1 denotes the reconstructed feature space by the DNN model.\n\nThe DNN for the i-th WT may therefore be by 608:\n\n{LN, WAY Bray\n\n[0090] to layers, [0091] N={N,,N,,...,N,,,} means the number of nodes of all layers with\n\n[0092] N,=N;,,=1 corresponding to the input layer and output layer. Weetw, 2, wef, Wy} and \u4eba ={Bi 4, BL, 2... B,,, } present weights and bias distributed across connections among all layers of the DNN models 606.\n\n[0101] The initial number of hidden nodes on this layer may be determined based on Principle Component Analysis (PCA) technique, which is a classical dimension reduction technique. A preferred embodiment of PCA in this invention is as follows:\n\n[0102] 1 \u4e00 compute the output of the hidden layer 1 feeding D,, into the original neural network using feedfor- ward algorithm.\n\n[0094] FIG. 6 shows a general framework of an example model training process 600. In source domain learning, D,, \u00a714 and D,,, 516 according to FIG. 5 are used to train a base DNN 606. The DNN structure may be self-organizing without a prior specification, in which details will be elabo- rated below with FIGS. 7 to 9.\n\n[0103] 2\u2014Set the initial number of hidden nodes of the said layer which may be initialized as the number of the greatest eigenvalues that can make up 80% of the sum of all eigenvalues obtained by PCA.\n\n[0104] 3\u2014Determining the dimension of {W,, W,} and {B,, By}.\n\n[0106] Step 1. Initialize 802: add one or some hidden nodes on the inserted hidden layer. Denote the weights connecting the hidden layer | and these new hidden nodes as W,,,, and the weights connecting the new hidden nodes and the output layer as Wu and the biases of the new hidden nodes as B,,,. At each update, W,,,, Wz, and B,,, will be randomly initialized and aggregated to the W,, Wu and Be respectively.\n\n[0096] FIG. 7 shows an example process of the source domain training 700. In this example, the DNN structure in source domain learning is determined by self-expanding an initial network through continuously stacking hidden layers and units until a suitable size is achieved. The hyperparam- eter 01 and 0, 612 referred to in FIG. 6 may assist in deciding when to stop adding new hidden layers or hidden nodes.\n\n[0107] Step 2. Update 804: train the weights and biases of the augmented partial network noted in FIG. 9. WwW\u3002 Wz, B\u3002 and B, will be trained using the gradient descent method\n\n[0097] At the beginning of source domain learning, a neural network may start with no hidden layers inside 702.\n\nthe\n\n[0083]\n\none for constructing the source domain 510, and\n\nis\n\nithayyytih_\n\n[0089] eterized\n\nparam-\n\nwhere L refers\n\nthe total number of hidden\n\nB\n\n[0093] Such parameterization is motivated by an assump- tion that the homogeneous patterns and heterogeneous char- acteristics among K WTs can be modelled by a compro- mised {L, N} and differentiated {W\u2122, BO}.\n\n[0095] Hyper-parameters of the base DNN 606, {L, N, Ww}, BYP) 608, may be automatically determined by two pre-specified thresholds, 0, and 0, 612. Then, in target domain learning 604, the {w\", Bia}, i=l, 2,...,K, can be finally obtained by optimizing the network parameters using backpropagation (BP) method based on {W?}, BO as the initial values.\n\nby\n\n[0105] FIG. 9 shows the method of determining the num- ber of hidden nodes, denoted as N,,;, after inserting a compact hidden layer 1+1 referring to the step 2 706 in FIG. 7. The expansion of the inserted hidden layer contains three steps:\n\nApr. 8 , 2021\n\nUS 2021/0102527 A1\n\n6\n\nroutines , programs , objects , components and data files\n\nwhile fixing the parameters from input layer to the hidden\n\nassisting in the performance of particular functions , the\n\nlayer 1 , i.e. { W1 ,\n\nW ; } and { B1 , .. B , } . Then , RE on\n\n1 ...\n\nvalidation set , R , { 2 + 1,0 } , where n means the number of\n\nskilled person will understand that the functionality of the\n\ncurrent hidden nodes , will be computed . When R , { 1 + 1 , n }\n\nsoftware application may be distributed across a number of\n\ncontinuously increases for 02 times with increasing n , no\n\nroutines , objects or components to achieve the same func\n\ntionality desired herein .\n\nmore hidden nodes will be added and move to step 3 ; else\n\n[ 0116 ]\n\nIt will also be appreciated that where the methods\n\nmove to step 1 to add more hidden nodes .\n\n[ 0108 ]\n\nStep 3. Prune 806 : determine N2-1 and prune the\n\nand systems of the present invention are either wholly\n\nimplemented by computing system or partly implemented\n\nredundant hidden nodes . Set N2 + 1 as the n causing the\n\nminimal R , { 2 + 1 , n } . Retain the first N241 hidden nodes and\n\nby computing systems then any appropriate computing\n\nprune the rest . Update We Wa and B , to keep the corre\n\nsystem architecture may be utilised . This will include stand\n\nsponding rows or columns of the matrices . Finally , apply BP\n\nalone computers , network computers and dedicated hard\n\noptimization to tune { W , Wa } and { Be , Bd .\n\nware devices . Where the terms \" computing system \u201d and\n\n[ 0109 ]\n\nAfter the source domain learning , conduct the\n\n\u201c computing device \u201d are used , these terms are intended to\n\ntarget domain learning according to FIG . 6. { W { i ) , B { i } } of\n\ncover any appropriate arrangement of computer hardware\n\nthe i - th WT DNN model are obtained by minimizing J { i }\n\ncapable of implementing the function described .\n\nwith the fine - tuning set D. } . The BP method is applied to\n\n[ 0117 ]\n\nIt will be appreciated by persons skilled in the art\n\nproduce K DNN based WT models using {\n\n{ 0 } , BO } } .\n\nthat numerous variations and / or modifications may be made\n\n[ 0110 ] The fine - tuned models then can be used for con\n\nto the invention as shown in the specific embodiments\n\ndition monitoring of a wind turbine population . In a pre\n\nwithout departing from the spirit or scope of the invention as\n\nferred embodiment of the invention , e.g. to monitor the\n\nbroadly described . The present embodiments are , therefore ,\n\ncondition of the i - th WT , the real - time SCADA data of the\n\nto be considered in all respects as illustrative and not\n\nsaid WT will be fed into its DNN model to compute RE ,\n\nrestrictive .\n\nwhich can be seen as an indicator of wind turbine health\n\n[ 0118 ] Any reference to prior art contained herein is not to\n\nstatus . Let R , denote the RE at time t . We can use the\n\nbe taken as an admission that the information is common\n\nexponentially weighted moving average ( EWMA ) chart to\n\ngeneral knowledge , unless otherwise indicated .\n\ndo condition monitoring . The control variable Z , can be\n\ncomputed by\n\n1. A method for monitoring a device comprising the steps\n\nof :\n\nz , = AR , + ( 1-1 ) 2-1\n\nobtaining operation information from a device , wherein\n\n[ 0111 ] where \u00e0 E ( 0,1 ] is a weight of the historical RE , and\n\nthe operation information is associated with the con\n\nZo can be the mean of REs for a period of monitoring . The\n\ndition of the device in operation ; and\n\nmean and variance can be computed by :\n\nprocessing the operation information with a device mod\n\nelling engine to determine one or more operation\n\nconditions of the device .\n\nUz = MR\n\n2. A method for monitoring a device in accordance with\n\n0 } = ( ok 1 - ) [ 1 \u2013 ( 1 \u2013 2 ) 2 ' ]\n\nclaim 1 , wherein the device modelling engine includes a\n\nplurality of matching networks for processing the operation\n\ninformation of a plurality of devices .\n\n3. A method for monitoring a device in accordance with\n\n[ 0112 ] The upper and lower control limits can be com\n\nclaim 2 , wherein each of the plurality of matching networks\n\nputed by :\n\nis arranged to be associated with an individual device of the\n\nplurality of devices .\n\n4. A method for monitoring a device in accordance with\n\nLCL ( t ) = Mz - Loza A [ 1 \u2013 ( 1 - 1 ) 2t ]\n\nclaim 3 , wherein the plurality of matching networks are\n\n( 2 - A ) n\n\ngenerated for each of the associated devices by training a\n\nbase matching network with a domain data set .\n\nLCL ( t ) = Mz - Loz A [ 1 \u2013 ( 1 - 1 ) 2 ]\n\n5. A method for monitoring a device in accordance with\n\n( 2 - a ) n\n\nclaim 4 , wherein the domain data set includes operation\n\ninformation from the plurality of devices .\n\n[ 0113 ] where L can be empirically set to 3 or any other\n\n6. A method for monitoring a device in accordance with\n\nsuitable value .\n\nclaim 5 , wherein the base matching network is further\n\n[ 0114 ] When the control variable Z , exceeds the upper or\n\nprocessed by training the base matching network with a\n\nlower control limit , the monitored wind turbine can be\n\ntarget data set associated with the individual device so as to\n\nconsidered as off - normal . The wind turbine should be iden\n\ngenerate each of the matching networks for each of the\n\ntified as normal when Zy is between the lower and upper\n\nindividual devices .\n\nlimits .\n\n7. A method for monitoring a device in accordance with\n\n[ 0115 ] Although not required , the embodiments described\n\nclaim 6 , wherein the target data set includes operation\n\nwith reference to the Figures can be implemented as an\n\napplication programming interface ( API ) or as a series of\n\ninformation for each of the individual devices .\n\nlibraries for use by a developer or can be included within\n\n8. A method for monitoring a device in accordance with\n\nanother software application , such as a terminal or personal\n\nclaim 7 , wherein the target data is used to fine tune the base\n\ncomputer operating system or a portable computing device\n\nmatching network into the each of the matching networks\n\noperating system . Generally , as program modules include\n\nfor each of the individual devices .\n\nUS 2021/0102527 Al\n\nApr. 8, 2021\n\nwhile fixing the parameters from input layer to the hidden layer |, ie. {W,,..., W,} and {B,,.... B,}. Then, RE on validation set, Res where n means the number of current hidden nodes, will be computed. When R,{*1\u201d3 continuously increases for 0, times with increasing n, no more hidden nodes will be added and move to step 3; else move to step 1 to add more hidden nodes.\n\nroutines, programs, objects, components and data files assisting in the performance of particular functions, the skilled person will understand that the functionality of the software application may be distributed across a number of routines, objects or components to achieve the same func- tionality desired herein.\n\n[0116] It will also be appreciated that where the methods and systems of the present invention are either wholly implemented by computing system or partly implemented by computing systems then any appropriate computing system architecture may be utilised. This will include stand alone computers, network computers and dedicated hard- ware devices. Where the terms \u201ccomputing system\u201d and. \u201ccomputing device\u201d are used, these terms are intended to cover any appropriate arrangement of computer hardware capable of implementing the function described.\n\n[0108] Step 3. Prune 806: determine N,,, and prune the redundant hidden nodes. Set N,,, as the n causing the minimal Re Retain the first N,,, hidden nodes and prune the rest. Update W,, W, and B, to keep the corre- sponding rows or columns of the matrices. Finally, apply BP optimization to tune {W,, W,} and {B,, B,}.\n\n[0109] After the source domain learning, conduct the target domain learning according to FIG. 6. {W\", BM} of the i-th WI DNN model are obtained by minimizing J with the fine-tuning set Df. The BP method is applied to produce K DNN based WT models using {W\u2018!, B&H}.\n\n[0117] It will be appreciated by persons skilled in the art that numerous variations and/or modifications may be made to the invention as shown in the specific embodiments without departing from the spirit or scope of the invention as broadly described. The present embodiments are, therefore, to be considered in all respects as illustrative and not restrictive.\n\n[0110] \u2018The fine-tuned models then can be used for con- dition monitoring of a wind turbine population. In a pre- ferred embodiment of the invention, e.g. to monitor the condition of the i-th WT, the real-time SCADA data of the said WT will be fed into its DNN model to compute RE, which can be seen as an indicator of wind turbine health status. Let R, denote the RE at time t. We can use the exponentially weighted moving average (EWMA) chart to do condition monitoring. The control variable z, can be computed by\n\n[0118] Any reference to prior art contained herein is not taken as an admission that the information is common knowledge, unless otherwise indicated.\n\n1. Amethod for monitoring a device comprising the\n\n2 =ARHMI-A)Z, 4\n\nobtaining operation information from a device, wherein the operation information is associated with the con- dition of the device in operation; and\n\n[0111] where \u548c E(0.1] is a weight of the historical RE, and can be the mean of REs for a period of monitoring. The mean and variance can be computed by:\n\nprocessing the operation information with a device mod elling engine to determine one or more operation conditions of the device.\n\n2. A method for monitoring a device in accordance with claim 1, wherein the device modelling engine includes plurality of matching networks for processing the operation information of a plurality of devices.\n\nots a A -U-4\"]\n\n3. A method for monitoring a device in accordance with aim 2, wherein each of the plurality of matching networks arranged to be associated with an individual device of urality of devices.\n\n[0112] The upper and lower control limits can be by:\n\nMA ZLCLD = wz \u2014 Lo, Gn ALL -(1-Ay*] ECL) = pz - Loy Gdn\n\n4. A method for monitoring a device in accordance with aim 3, wherein the plurality of matching networks generated for each of the associated devices by training base matching network with a domain data set.\n\n5. A method for monitoring a device in accordance with claim 4, wherein the domain data set includes operation information from the plurality of devices.\n\nwhere L can be empirically set to 3 or any value.\n\n6. A method for monitoring a device in accordance with claim 5, wherein the base matching network is further processed by training the base matching network with a target data set associated with the individual device so as to generate each of the matching networks for each of the individual devices.\n\n[0114] When the control variable z, exceeds the upper lower control limit, the monitored wind turbine can considered as off-normal. The wind turbine should be iden- tified as normal when z, is between the lower and upper limits.\n\n[0115] Although not required, the embodiments described with reference to the Figures can be implemented as an application programming interface (API) or as a series of libraries for use by a developer or can be included within another software application, such as a terminal or personal computer operating system or a portable computing device operating system. Generally, as program modules include\n\n7. A method for monitoring a device in accordance with claim 6, wherein the target data set includes operation information for each of the individual devices.\n\n8. A method for monitoring a device in accordance claim 7, wherein the target data is used to fine tune the matching network into the each of the matching networks each of the individual devices.\n\nZo\n\nHe = HR\n\ncom-\n\nputed\n\n[0113] suitable\n\nother\n\nor\n\nbe\n\nto\n\nbe general\n\nsteps\n\nof:\n\na\n\nis\n\nthe\n\nare\n\na\n\nce\n\n3\n\na\n\nwith\n\nbase\n\nfor\n\nApr. 8 , 2021\n\nUS 2021/0102527 A1\n\n7\n\nprocessed by training the base matching network with a\n\n9. A method for monitoring a device in accordance with\n\nclaim 8 , wherein the based matching network is a deep\n\ntarget data set associated with the individual device so as to\n\ngenerate each of the matching networks for each of the\n\nneural network .\n\nindividual devices .\n\n10. A method for monitoring a device in accordance with\n\n23. A system for monitoring a device in accordance with\n\nclaim 9 , wherein the structure of the deep neural network is\n\nclaim 22 , wherein the target data set includes operation\n\nadjusted based on a stopping criterion .\n\ninformation for each of the individual devices .\n\n11. A method for monitoring a device in accordance with\n\n24. A system for monitoring a device in accordance with\n\nclaim 10 , wherein the stopping criterion is determined based\n\nclaim 23 , wherein the target data is used to fine tune the base\n\non a reconstruction error obtained from the deep neural\n\nmatching network into the each of the matching networks\n\nnetwork .\n\nfor each of the individual devices .\n\n12. A method for monitoring a device in accordance with\n\n25. A system for monitoring a device in accordance with\n\nclaim 11 , wherein the reconstruction error is obtained by\n\nclaim 24 , wherein the based matching network is a deep\n\ninputting the domain data set into the deep neural network .\n\nneural network .\n\n13. A method for monitoring a device in accordance with\n\n26. A system for monitoring a device in accordance with\n\nclaim 12 , wherein the structure of the deep neural network\n\nis adjusted by inserting one or more hidden layers into the\n\nclaim 25 , wherein the structure of the deep neural network\n\nis adjusted based on a stopping criterion .\n\ndeep neural network .\n\n27. A system for monitoring a device in accordance with\n\n14. A method for monitoring a device in accordance with\n\nclaim 26 , wherein the stopping criterion is determined based\n\nclaim 13 , wherein the one or more hidden layers inserted by\n\non a reconstruction error obtained from the deep neural\n\nadding one or more hidden nodes into the hidden layers .\n\nnetwork .\n\n15. A method for monitoring a device in accordance with\n\n28. A system for monitoring a device in accordance with\n\nclaim 14 , wherein weights or biases of the deep neural\n\nclaim 27 , wherein the reconstruction error is obtained by\n\nnetwork are manipulated when the deep neural network is\n\ninputting the domain data set into the deep neural network .\n\nadjusted .\n\n29. A system for monitoring a device in accordance with\n\n16. A method for monitoring a device in accordance with\n\nclaim 28 , wherein the structure of the deep neural network\n\nclaim 1 , wherein the device is a wind turbine .\n\nis adjusted by inserting one or more hidden layers into the\n\n17. A system for monitoring a device comprising :\n\ndeep neural network .\n\na device conditions gateway arranged to obtain operation\n\n30. A system for monitoring a device in accordance with\n\ninformation from a device , wherein the operation infor\n\nclaim 29 , wherein the one or more hidden layers inserted by\n\nmation is associated with the condition of the device in\n\nadding one or more hidden nodes into the hidden layers .\n\noperation , and\n\n31. A system for monitoring a device in accordance with\n\na conditions monitoring engine arranged to process the\n\nclaim 30 , wherein weights or biases of the deep neural\n\noperation information with a device modelling engine\n\nnetwork are manipulated when the deep neural network is\n\nto determine one or more operation conditions of the\n\nadjusted\n\ndevice .\n\n32. A system for monitoring a device in accordance with\n\n18. A system for monitoring a device in accordance with\n\nclaim 31 , wherein the device is a wind turbine .\n\nclaim 17 , wherein the device modelling engine includes a\n\n33. A system for monitoring a device in accordance with\n\nplurality of matching networks for processing the operation\n\nclaim 17 , further comprising a pre - processor arranged to\n\ninformation of a plurality of devices .\n\nprocess the operation information obtained from the device\n\n19. A system for monitoring a device in accordance with\n\nconditions gateway .\n\nclaim 18 , wherein each of the plurality of matching networks\n\n34. A system for monitoring a device in accordance with\n\nis arranged to be associated with an individual device of the\n\nclaim 33 , wherein the operation information is pre - pro\n\nplurality of devices .\n\ncessed before the information is inputted to the device\n\n20. A system for monitoring a device in accordance with\n\nmodelling engine .\n\nclaim 19 , wherein the plurality of matching networks are\n\n35. A system for monitoring a device in accordance with\n\ngenerated for each of the associated devices by training a\n\nclaim 34 , wherein the operation information is pre - pro\n\nbase matching network with a domain data set .\n\ncessed to remove fault data from the operation information .\n\n21. A system for monitoring a device in accordance with\n\n36. A system for monitoring a device in accordance with\n\nclaim 20 , wherein the domain data set includes operation\n\nclaim 17 , wherein the device conditions gateway is a\n\ninformation from the plurality of devices .\n\nSCADA system .\n\n22. A system for monitoring a device in accordance with\n\nclaim 21 , wherein the base matching network is further\n\nUS 2021/0102527 Al\n\nApr. 8, 2021\n\n9. A method for monitoring a device in accordance with claim 8, wherein the based matching network is a deep neural network.\n\nprocessed by training the base matching network with target data set associated with the individual device so as generate each of the matching networks for each of individual devices.\n\n10. A method for monitoring a device in accordance 9, wherein the structure of the deep neural network based on a stopping criterion.\n\n23. A system for monitoring a device in accordance claim 22, wherein the target data set includes operation information for each of the individual devices.\n\n11. A method for monitoring a device in accordance with claim 10, wherein the stopping criterion is determined based a reconstruction error obtained from the deep neural network.\n\n24. A system for monitoring a device in accordance claim 23, wherein the target data is used to fine tune the base matching network into the each of the matching networks each of the individual devices.\n\n12. A method for monitoring a device in accordance with claim 11, wherein the reconstruction error is obtained by inputting the domain data set into the deep neural network.\n\n25. A system for monitoring a device in accordance with aim 24, wherein the based matching network is a deep neural network.\n\n13. A method for monitoring a device in accordance with claim 12, wherein the structure of the deep neural network adjusted by inserting one or more hidden layers into deep neural network.\n\n26. A system for monitoring a device in accordance claim 25, wherein the structure of the deep neural networl adjusted based on a stopping criterion.\n\n27. A system for monitoring a device in accordance claim 26, wherein the stopping criterion is determined based a reconstruction error obtained from the deep neural network.\n\n14. A method for monitoring a device in accordance with claim 13, wherein the one or more hidden layers inserted one or more hidden nodes into the hidden layers.\n\n15. A method for monitoring a device in accordance with claim 14, wherein weights or biases of the deep neural network are manipulated when the deep neural network adjusted,\n\n28. A system for monitoring a device in accordance claim 27, wherein the reconstruction error is obtained inputting the domain data set into the deep neural network.\n\n29. A system for monitoring a device in accordance claim 28, wherein the structure of the deep neural network adjusted by inserting one or more hidden layers into neural network.\n\n16. A method for monitoring a device in accordance with claim 1, wherein the device is a wind turbine.\n\n17. A system for monitoring a device comprising:\n\na device conditions gateway arranged to obtain operation information from a device, wherein the operation infor- mation is associated with the condition of the device in operation; and\n\n30. A system for monitoring a device in accordance with aim 29, wherein the one or more hidden layers inserted one or more hidden nodes into the hidden layers.\n\n31. A system for monitoring a device in accordance witl aim 30, wherein weights or biases of the deep neural network are manipulated when the deep neural network adjusted.\n\na conditions monitoring engine arranged to process the operation information with a device modelling engine to determine one or more operation conditions of the device.\n\n32. A system for monitoring a device in accordance with aim 31, wherein the device is a wind turbine.\n\n18. A system for monitoring a device in accordance with claim 17, wherein the device modelling engine includes plurality of matching networks for processing the operation information of a plurality of devices.\n\n33. A system for monitoring a device in accordance with aim 17, further comprising a pre-processor arranged to process the operation information obtained from the device conditions gateway.\n\n19. A system for monitoring a device in accordance claim 18, wherein each of the plurality of matching networks arranged to be associated with an individual device of plurality of devices.\n\n34. A system for monitoring a device in accordance claim 33, wherein the operation information is pre-pro- cessed before the information is inputted to the device modelling engine.\n\n20. A system for monitoring a device in accordance with claim 19, wherein the plurality of matching networks are generated for each of the associated devices by training base matching network with a domain data set.\n\n35. A system for monitoring a device in accordance claim 34, wherein the operation information is pre-pro- cessed to remove fault data from the operation information.\n\n21. A system for monitoring a device in accordance with claim 20, wherein the domain data set includes operation information from the plurality of devices.\n\n36. A system for monitoring a device in accordance with claim 17, wherein the device conditions gateway is SCADA system.\n\n22. A system for monitoring a device in accordance with claim 21, wherein the base matching network is further\n\nwith is\n\nclaim adjusted\n\non\n\nis\n\nthe\n\nby\n\nadding\n\nis\n\na\n\nwith\n\nis\n\nthe\n\na\n\na\n\nto the\n\nwith\n\nwith\n\nfor\n\nwith\n\nis\n\nwith\n\non\n\nwith by\n\nwith\n\nis deep\n\nthe\n\nby\n\nadding ec\n\nis\n\nwith\n\nwith\n\na\n\nce\n\nce\n\n5\n\nec\n\nce", "type": "Document"}}