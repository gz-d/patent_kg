{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/1361_US11822732B1.pdf"}, "page_content": "US011822732B1\n\n\u00b0 az) United States Patent\n\n(10) Patent No.: US 11,822,732 B1\n\nZhu et al.\n\n(45) Date of Patent: Nov. 21, 2023\n\n(54)\n\nINTERACTIVE WEARABLE DEVICE AND METHOD OF MACHINE LEARNING BASED TRAINING THEREOF\n\n(71) Applicant: City University of Hong Kong, Hong Kong (HK)\n\nOTHER PUBLICATIONS\n\nYilin Liu et al. NeuroPose: 3D Hand Pose Tracking Using EMG Wearables. The Web Conference 202 1\u2014Proceedings of the World Wide Web Conference, WWW 2021 (2021), 1471-1482.\n\nArnold M Lund, Measuring Usability with the USE Questionnaire. Usability interface 8, 2 (2001), 3-6.\n\n(72) Inventors: Kening Zhu, Hong Kong (HK); Taizhou Chen, Hong Kong (HK); Tianpei Li, Hong Kong (HK)\n\nScott MacKenzie. 1992. Fitts\u2019 Law as a Research and Design Human-Computer Interaction. Human-computer interaction 7, 91-139.\n\nI in (1992),\n\nTool |\n\n(Continued)\n\n(73) Assignee: City University of Hong Kong, Hong Kong (HK)\n\nPrimary Examiner \u2014 Christopher J Kohlman\n\n(*) Notice: Subject to any disclaimer, the term of this patent is extended or adjusted under 35 U.S.C. 154(b) by 0 days.\n\n(74) Attorney, Agent, or Firm \u2014 Idea Intellectual Limited; Margaret A. Burke; Sam T. Yip\n\n(21) Appl. No.: 18/169,223\n\n(57) ABSTRACT\n\nra (22) Filed: Feb. 15, 2023\n\n(51) Int. Cl.\n\nChen ne 2 (2023.01) GO6F 3/033 (2013.01)\n\n(52) U.S. Cl. CPC ee G06F 3/017 (2013.01); GO6F 3/014 (2013.01); GO6F 3/033 (2013.01); GO6N 5/022 (2013.01); GO6F 2203/0331 (2013.01\n\n(58) Field of Classification Search See application file for complete search history.\n\n(56) References Cited\n\nU.S. PATENT DOCUMENTS\n\nAn interactive wearable device includes a ring body and a B doy detector. The ring body includes a top insulating layer, a bottom insulating layer, and an intermediate insulating layer disposed in between the top and the bottom insulating layers. The detector includes a receiving electrode layer disposed in between the top and the intermediate insulating layers, a transmitting electrode layer disposed in between the inter- diate and the bottom insulating 1 1 1 \u548c electrode layer embedding in the bottom insulating layer and electrically coupled to an electrical ground. The receiving electrode layer has a plurality of receiving electrode portions separated from each other and arranged in a matrix and along a curve path. The interactive wearable device is configured to analyze the movement event at least according to a variation of a data set in response to the movement\n\n, GO6F 3/014 GOGF 3/0236 GO06V 40/1365 382/124 6,859,141 Bl 2/2005 Van Schyndel et al. 10,139,906 BL* 11/2018 Bai .. 10,310,632 B2* 6/2019 Nirjon . 2016/0034742 Al* 2/2016 Kim .\n\nevent, measured by receiving electrode portions.\n\n19 Claims, 18 Drawing Sheets\n\n\u548c\n\n\u4e0a\n\nUS 11,822,732 BT\n\nPage 2\n\n(56) References Cited\n\nOTHER PUBLICATIONS\n\nDenys JC Matthies et al. CapGlasses: Untethered Capacitive Sens- with Smart Glasses. In Augmented Humans Conference 2021. 121-130.\n\ning\n\nMiranda McClellan et al. Deep Learning at the Mobile Edge: Opportunities for 5G Networks. Applied Sciences 10, 14 (2020), 4735.\n\nDevices. Association for Computing Machinery, (2021), 710-723. Robert Y. Wang et al. Real-Time Hand-Tracking with a Color Glove. ACM Transactions on Graphics 28, 3 (2009), 1-8. Saiwen Wang et al. Interacting with Soli: Exploring Fine-Grained Dynamic Gesture Recognition in the Radio-Frequency Spectrum. UIST 2016\u2014Proceedings of the 29th Annual Symposium on User Interface Software and Technology (2016), 851-860.\n\nEric\n\nWhitmire et al. DigiTouch: Reconfigurable Thumb-to-Finger Input and Text Entry on Head-mounted Displays. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technolo- gies 1, 3 (2017), 1-21.\n\nMicroChip. 2022. ATUSB-GESTIC-PCB. Retrieved Aug. 10, 2022 from https://www.microchip.com/en-us/development-tool/ EV91M41A.\n\nMathias Wilhelm et al. PeriSense: Ring-Based Multi-Finger Gesture Interaction Utilizing Capacitive Proximity Sensing. Sensors (Swit- zerland) 20, 14 (2020), 1-23.\n\nMicroChip. 2022. MGC3130 Datasheet. Retrieved Aug. 10, 2022 from https:/Awww.microchip.com/en-us\u2018product/MGC3 130.\n\nFranziska Mueller et al. Real-time Pose and Shape Reconstruction Two Interacting Hands With a Single Depth Camera. ACM Transactions on Graphics 38, 4 (2019).\n\nof\n\nAdiyan Mujibiya et al. Mirage: Exploring Interaction Modalities Using Off-Body Static Electric Field Sensing. UIST 2013\u2014 Proceedings of the 26th Annual ACM Symposium on User Interface Software and Technology (2013), 211-220.\n\nArshad Nasser et al. FingerTalkie: Designing A Low-cost Finger- worn Device for Interactive Audio Labeling of Tactile Diagrams. Human-Computer Interaction. Multimodal and Natural Interaction, Masaaki Kurosu (Ed.). Springer International Publishing, Cham, 475-496.\n\nIn\n\nMathias Wilhelm et al. eRing: Multiple Finger Gesture Recognition one Ring Using an Electric Field. ACM International Confer- Proceeding Series Jun. 25-26, 2015, 1-6.\n\nwith\n\nence\n\nErwin Wu et al. Back-Hand-Pose: 3D Hand Pose Estimation for a Wrist-Worn Camera via Dorsum Deformation Network. UIST 2020\u2014\n\nProceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology (2020), 1147-1160.\n\nXuhai Xu et al. EarBuddy: Enabling On-Face Interaction via Wireless Earbuds. Association for Computing Machinery, (2020), 1-14.\n\nZheer Xu et al. 2020. BiTipText: Bimanual Eyes-Free Text Entry on Fingertip Keyboard. Conference on Human Factors in Computing Systems\u2014Proceedings (2020), 1-14.\n\na\n\nViet Nguyen et al. HandSense: Capacitive coupling-based Dynamic, Micro Finger Gesture Recognition. SenSys 2019\u2014Proceedings the 17th Conference on Embedded Networked Sensor Systems (2019), 285-297.\n\nof\n\nZheer Xu et al. TipText: Eyes-Free Text Entry on a Fingertip Keyboard. UIST 2019\u2014Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (2019), 883-889.\n\nRekimoto. GestureWrist and GesturePad: Unobtrusive Wearable Interaction Devices. International Symposium on Wearable Com- Digest of Papers (2001), 21-27.\n\nJ.\n\nputers,\n\nThijs Roumen et al. NotiRing: A Comparative Study of Notification Channels for Wearable Interactive Rings. Conference on Human Factors in Computing Systems\u2014Proceedings (2015), 2497-2500. Shardul Sapkota et al. Ubiquitous Interactions for Heads-Up Com- puting: Understanding Users\u2019 Preferences for Subtle Interaction Techniques in Everyday Settings. In Proceedings of the 23rd International Conference on Mobile Human-Computer Interaction (2021), 36, 15.\n\nYui-Pan Yau et al. How Subtle Can It Get? A Trimodal Study of Ring-sized Interfaces for One-Handed Drone Control. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 2 (2020).\n\nSang Ho Yoon et al. TIMMi: Finger-worn Textile Input Device with Multimodal Sensing in Mobile Interaction. Proceedings of the 9th International Conference on Tangible, Embedded, and Embodied Interaction January (2015), 269-272.\n\n21,\n\nChaoyun Zhang et al. Deep Learning in Mobile and Wireless Networking: A Survey. IEEE Communications Surveys & Tutorials 3 (2019), 2224-2287.\n\nYilei Shi et al. Ready, Steady, Touch!\u2014Sensing Physical Contact with a Finger-Mounted IMU. Proceedings of the ACM on Interac- tive, Mobile, Wearable and Ubiquitous Technologies 4 (2020), 1-25. Karen Simonyan et al. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv preprint arXiv:1409.1556 (2014).\n\nJoshua Smith et al. Electric Field Sensing For Graphical Interfaces. Computer Graphics and Applications 18, 3 (1998), 54-59.\n\nIEEE\n\nChristian Szegedy et al. Rethinking the Inception Architecture for Computer Vision. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (2016), 2818-2826.\n\non\n\nJonathan Tompson et al. Real-Time Continuous Pose Recovery Human Hands Using Convolutional Networks. ACM Transactions Graphics 33, 5 (2014).\n\nof\n\nCheng Zhang et al. FingerSound: Recognizing unistroke thumb gestures using a ring. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, 3 (2017), 1-19. Cheng Zhang et al. FingerPing: Recognizing Fine-grained Hand Poses using Active Acoustic On-body Sensing. Conference on Human Factors in Computing Systems\u2014Proceedings 2018\u2014Apr. 2018, 1-10.\n\nXu\n\nZhang et al. Hand Gesture Recognition and Virtual Game Control Based on 3D Accelerometer and EMG Sensors. Interna- tional Conference on Intelligent User Interfaces, Proceedings IUI (2008), 1-5.\n\nJunhan Zhou et al. AuraSense : Enabling Expressive Around- Smartwatch Interactions with Electric Field Sensing. UIST 2016\u2014\n\nProceedings of the 29th Annual ACM Symposium on User Interface Software and Technology Figure 1 (2016), 81-86.\n\nHsin-Ruey Tsai et al. TouchRing: Subtle and Always-Available Input Using a Multi-touch Ring. In The ACM International Con- ference on Mobile Human-Computer Interaction. (2016), 891-898. Hsin Ruey Tsai et al. ThumbRing: Private Interactions Using One-Handed Thumb Motion Input on Finger Segments. Proceed- ings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services Adjunct, MobileHCI 2016 (2016), 791-798.\n\nHsin-Ruey Tsai et al. SegTouch: Enhancing Touch Input While Providing Touch Gestures on Screens Using Thumb-To-Index- Finger Gestures. Proceedings of the 2017 CHI Conference Extended. Abstracts on Human Factors in Computing Systems (2017), 2164- 2171.\n\nRadu-Daniel Vatavu et al. GestuRING: A Web-based Tool for Designing Gesture Input with Rings, Ring-Like, and Ring-Ready\n\nKening Zhu et al. A sense of ice and fire: Exploring thermal feedback with multiple thermoelectric-cooling elements on a smart ting. International Journal of Human Computer Studies 130, (2019), 234-247.\n\nThomas G. Zimmerman et al. Applying Electric Field Sensing Human-Computer Interfaces. Conference on Human Factors Computing Systems\u2014Proceedings 1, May 1995, 280-287.\n\nKazuyuki Arimatsu et al. Evaluation of Machine Learning Tech- niques for Hand Pose Estimation on Handheld Device with Prox- imity Sensor. Conference on Human Factors in Computing Systems\u2014 Proceedings (2020), 1-13.\n\nSandra Bardot et al. ARO: Exploring the Design of Smart-Ring Interactions for Encumbered Hands. Proceedings of MobileHCI 2021\u2014ACM International Conference on Mobile Human- Computer Interaction: Mobile Apart, MobileTogether (2021).\n\nto\n\nin\n\nUS 11,822,732 BT\n\nPage 3\n\n(56) References Cited\n\nOTHER PUBLICATIONS\n\nChangzhan Gu et al. A Two-Tone Radar Sensor for Concurrent Detection of Absolute Distance and Relative Movement for Gesture Sensing. IEEE Sensors Letters 1, 3 (2017), 1-4.\n\nHymalai Bello et al. MoCapaci: Posture and gesture detection loose garments using textile cables as capacitive antennas. Proceedings International Symposium on Wearable Computers, ISWC (2020), 78-83.\n\nXiaojun Bi et al. FFitts Law: Modeling finger Touch with Fitts\u2019 Law. Conference on Human Factors in Computing Systems\u2014 Proceedings (2013).\n\nin \u2014\n\nGu et al. Accurate and\n\nof Touch\n\nYizheng Low-Latency Sensing Contact on Any Surface with Finger-Worn IMU Sensor. UIST 2019\u2014Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology (2019), 1059-1070.\n\nGa\u00e9l Guennebaud et al. 2010. Eigen v3. http://eigen.tuxfamily.org. Tian Guo. Cloud-based or On-device: An Empirical Study of Mobile Deep Inference. In 2018 IEFE International Conference on Cloud Engineering (IC2E). IEEE, (2018), 184-190.\n\nRoger Boldu et al. Thumb-In-Motion: Evaluating Thumb-to-Ring Microgestures for Athletic Activity. SUI 2018\u2014Proceedings of Symposium on Spatial User Interaction (2018), 150-157.\n\nthe\n\nShangchen Han et al. MEgATrack: Monochrome Egocentric Articu- lated Hand-Tracking for Virtual Reality. ACM Transactions Graphics 39, 4 (2020).\n\non\n\nT. Califski et al. A Dendrite Method for Cluster Analysis. Com- munications in Statistics\u2014Theory and Methods 3, 1 (1974), 1-27. Liwei Chan et al. CyclopsRing: Enabling Whole-Hand and Context- Aware Interactions Through a Fisheye Ring. UIST 2015\u2014 Proceedings of the 28th Annual ACM Symposium on User Interface Software and Technology 1 (2015), 549-556.\n\nKaiming He et al. Deep Residual Learning for Image Recognition. Proceedings of the IEEE Conference on Computer Vision Recognition, (2016), 770-778.\n\nIn\n\nPattern\n\nMikko Heino et al. Recent Advances in Antenna Design and Interference Cancellation Algorithms for In-Band Full Duplex Relays. IEEE Communications Magazine 53, 5 (2015), 91-101.\n\nand\n\nLiwei Chan et al. FingerPad: Private and Subtle Interaction Using Fingertips. (2013), 255-260.\n\nChih-Chung Chang et al. LIBSVM: A Library for Support Vector Machines. ACM Transactions on Intelligent Systems and Technol- ogy 2 (2011), 27:1-27:27. Issue 3.\n\nin\n\nKe-Yu Chen et al. Finexus: Tracking Precise Motions of Multiple Fingertips Using Magnetic Sensing. Conference on Human Factors Computing Systems\u2014Proceedings (2016), 1504-1514.\n\nTaizhou Chen et al. GestOnHMD: Enabling Gesture-based Interac- tion on Low-cost VR Head-Mounted Display. IEEE Transactions on Visualization and Computer Graphics 27, 5 (2021), 2597-2607.\n\nAnuradha Herath et al. Expanding Touch Interaction Capabilities for Smart-rings: An Exploration of Continual Slide and Microroll Gestures. In CHI Conference on Human Factors in Computing Systems Extended Abstracts, (2022), 1-7.\n\nFang Hu et al. FingerTrak: Continuous 3D Hand Pose Tracking by Deep Learning Hand Silhouettes Captured by Miniature Thermal Cameras on Wrist. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 2 (2020).\n\nDa\n\nYuan Huang et al. DigitSpace: Designing Thumb-to-Fingers Touch Interfaces for One-Handed and Eyes-Free Interactions. Con- ference on Human Factors in Computing Systems\u2014Proceedings (2016), 1526-1537.\n\nGabe Cohn et al. Humantenna: Using the Body as an Antenna for Real-Time Whole-Body Interaction. Conference on Human Factors in Computing Systems\u2014Proceedings (2012), 1901-1910.\n\nGao Huang et al. Densely Connected Convolutional Networks. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, (2017), 4700-4708.\n\nIn\n\nRajkumar Darbar et al. RingloT: A Smart Ring Controlling Things in Physical Spaces. May 2019, 2-9.\n\nAlexey Dosovitskiy et al. An Image is Worth 16X16 Words: \u2018Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010. 11929 (2020).\n\nChristoph Endres et al. \u201cGeremin\u201d: 2D Microgestures for Drivers Based on Electric Field Sensing. International Conference on Intel- ligent User Interfaces, Proceedings IUI Jun. 2014 (2011), 327-330. Paul M. Fitts. The Information Capacity of the Human Motor System in Controlling the Amplitude of Movement. Journal of experimental psychology 47, 6 (1954), 381-391.\n\nMasaaki Fukumoto et al. \u201cFingeRing\u201d: A Full-Time Wearable Interface. Conference on Human Factors in Computing Systems\u2014\n\nProceedings 1994\u2014Apr., May 1994, 81-82.\n\nOliver Glauser et al. Deformation Capture via Soft and Stretchable Sensor Arrays. ACM Trans. Graph 38, 2 (2019), 1-16.\n\nOliver Glauser et al. Interactive Hand Pose Estimation using a Stretch-Sensing Soft Glove. SACM Transactions on Graphics (Pro- ceedings of ACM SIGGRAPH) 38, 4 (2019), 15.\n\nSeungwoo Je et al. PokeRing: Notifications by Poking Around Finger. Conference on Human Factors in Computing Systems\u2014 Proceedings (2018), 1-10.\n\nSeungwoo Je et al. tactoRing: A Skin-Drag Discrete Display. Conference on Human Factors in Computing Systems\u2014 2017\u2014May 2017, 3106-3114.\n\nProceedings\n\nRudolph Emil Kalman. A New Approach to Linear Filtering and Prediction Problems. (1960).\n\nWolf Kienzle et al. LightRing: Always-Available 2D Input on Any Surface. UIST 2014\u2014Proceedings of the 27th Annual ACM Sym- posium on User Interface Software and Technology (2014), 157- 160.\n\nWolf Kienzle et al. Electroring: Subtle Pinch and Touch Detection with a Ring. Conference on Human Factors in Computing Systems\u2014 Proceedings (2021).\n\nDavid Kim et al. Digits: Freehand 3D Interactions Anywhere Using Wrist-Worn Gloveless Sensor. UIST\u2019 12\u2014Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technol- (2012), 167-176.\n\na\n\nogy\n\nthe\n\nMathieu Le Goc et al. A Low-cost Transparent Electric Field Sensor for 3D Interaction on Mobile Devices. Conference on Human Factors in Computing Systems\u2014Proceedings Apr. 2014, 3167- 3170.\n\nJun Gong et al. Pyro: Thumb-Tip Gesture Recognition Using Pyroelectric Infrared Sensing. UIST 2017\u2014Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technol- ogy (2017), 553-563.\n\nJonghwa Kim et al. EMG-based Hand Gesture Recognition for Realtime Biosignal Interfacing. International Conference on Intel- ligent User Interfaces, Proceedings IUI (2008), 30-39.\n\nDiederik P. Kingma et al. ADAM: A Method for Stochastic Opti- mization. 3rd. International Conference on Learning Representa- tions, ICLR 2015\u2014Conference Track Proceedings (2015), 1-15.\n\nGeorge Frederick Kunz. 2012. Rings for the Finger. Courier Cor- poration.\n\nTobias Grosse-Puppendahl et al. OpenCapSense: A Rapid Prototyp- ing Toolkit For Pervasive Interaction Using Capacitive Sensing. 2013 IEEE International Conference on Pervasive Computing and. Communications, PerCom 2013 Mar. 2013, 152-159.\n\nTobias Grosse-Puppendahl et al. Honeyfish\u2014a high resolution gesture recognition system based on capacitive proximity sensing. Embedded World Conference 2012 Jan. 2012, 10.\n\nGierad Laput et al. SurfaceSight: A New Spin on Touch, User, and. Object Sensing for IoT Experiences. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (2019). Gierad Laput et al. ViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers. UIST 2016\u2014 Proceedings of the 29th Annual Symposium on User Interface Software and Technology (2016), 321-333.\n\nTobias Grosse-Puppendahl et al. Finding Common Ground: A Survey of Capacitive Sensing in Human-Computer Interaction. Conference on Human Factors in Computing Systems\u2014 Proceedings 2017\u2014May 2017, 3293-3316.\n\nChen Liang et al. DualRing: Enabling Subtle and Expressive Hand. Interaction with Dual IMU Rings. Proceedings of the ACM Interactive, Mobile, Wearable and Ubiquitous Technologies 5, (2021).\n\non\n\n3\n\nUS 11,822,732 BT\n\nPage 4\n\n(56) References Cited\n\nOTHER PUBLICATIONS\n\nJaime Lien et al. Soli: Ubiquitous Gesture Sensing with Millimeter Wave Radar. ACM Transactions on Graphics 35, 4 (2016), 1-19. Hyunchul Lim et al. Touch+Finger: Extending Touch-Based User Interface Capabilities with \u201cIdle\u201d Finger Gestures in the Air In Proceedings of the 3 1st Annual ACM Symposium on User Interface Software and Technology, (2018), 335-346. Guanhong Liu et al. Keep the Phone in Your Pocket: Enabling\n\nSmartphone Operation with an IMU Ring for Visually Impaired People. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 2 (2020).\n\n* cited by examiner\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 1 of 18\n\nUS 11,822,732 B1\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 2 of 18\n\n-\u5341 JOeJaUaT [eUTIS US TOE JOSSIIOId \u54c1 \u5341 \u5316 pou TW oneoytsse|o \u5341 Iapow TA} puodas CIN He japow Fi 5 LTIWN -\u4e00 AIOUusW \uff0c Ngo\n\n99IA9d 8Sul32319d\n\nUS 11,822,732 B1\n\nOOL\n\ndL \u201cSls\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 3 of 18\n\nUS 11,822,732 B1\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 4 of 18\n\nUS 11,822,732 B1\n\nqzll PZbL ez \u4e0a\n\nds eZ \u201ca Bee . \u4eba \u4e00 \u4eba 3 } 5 ZH |\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 5 of 18\n\nUS 11,822,732 B1\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 6 of 18\n\nUS 11,822,732 B1\n\ni i \u570b\n\n\u4e0a\n\nt i t | i i i i\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 7 of 18\n\nUS 11,822,732 B1\n\nOSLs \u4e00 sjes elep Buluien pajyeiauasas BU} JO xepul HD JO SenfeA ay} ol Sulpsos0e GOssavqid ayy Ad \u201cpauiuuaolsp si OrlsS~ | S195 Jo159A SiNeay 1ualel ay} Buisn \u2018s0883201d ay} Aq \u2018payndiuod due SialSnbp JeUsIs [ed14}990}9 indino pajesauagai | 08IS \u4e00 | sjas elep 8ululej peleJaua8a ay] Jo dea Wiojj Jossa504d sl Aq paloeJXa SI | 195 elep Suluses} ed JO yea Jo sjeusis je2Ln2 \u5f15 9 3ndjno sunonsjsuosai/Bunesuasas So4n1s9aoJoluu 0] 3SuodSsal Ul Sjes Bep Suiule ad YUM paule sj9pou 0zLsS ~~ | OLLS saj2ue Sue9A 1Uajisjjip JapuN suasn jo ysea jo syasuly OM} Aq pauojiad 48Ael apoj5ala BUIAIAI91 SY] JO SUOIIOd apoHoala SuIAIadas Sy] Aq \u2018painseaw | pauiuuualap St Sein3se8ojoIu 33845Sip jo Ayjeunjd e Sulipnipuil jes OOLS \u4e00\n\ndS 9\n\nAqossy} 419poo2sp-Japooue paseg NND V 3UeAe JUaWaAOW ge SuNp aJe sjas eyep BuluielL aunjsedousiw V\n\nJas JOPA Bine \u5f15 1Ua3el V\n\nad JO XAPU] HD jo sanlgA\n\naj3ue Sueam jeuuildo uy\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 8 of 18\n\nUS 11,822,732 B1\n\n9 Old 88 \u4e60 89 5 vrs 5Z 5 SZ 9 ET 9 ET 9 ET'9 5T 9 vr'9 ver 05 5 885'5 88' 5 ET 9 Buebl (\u4eba asimyopsaquno apiD (1) asiwojpolD aput2 (4) Dau2 (\u5168 6T 9 5SZ 9 T3\u00b09 S29 83\u00b09 v's T8 9 5SL'9 t6 9 veo 88'S T3\u00b09 sz9 00\u00b0 00\u00b02 OL dr adims (3) ye1edims (p) yeT edims (9) de} ajqnog (q) umog adims (j} anblfed 252VIelDos aseq anbiyey 22ViegipoS aseq\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 9 of 18\n\nUS 11,822,732 B1\n\n(S'0) nodoiq+ (7\u00b0) MayAyeat+ pzpasodsuesj auc} (SO0) jnodoig+ (Zo) MayAyeat+ pzAuoD\n\nSXO6LXL plouualS \u2018TXT @PLIs \u2018TXTXy PZAUOD SXEGTXBPOC aunie94 3ualei SXOGEXT jeusis Indu;\n\n/ 95\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 10 of 18\n\nUS 11,822,732 B1\n\n062 S \\ japow IN JW payajas ad 8no444 S38s aunqyesj 94} 0} asuodsas ul Sain1S83oiolu UMOUY 94} 3zlU80294 ol ajqeua S| aolAep ajqeseam 9AIHDEJ91ul adj edl dns 2os59504d ayy Aq \u2018(x)4 S}as aunjeay feuy audl YM Pauley S| japow TA Payajes ayy 08\u00a2S 7 + 0/CZS a (x}4 285 Sunjzea) ayy jo adA jeuals e ol Buipsoasze 4ossaqoid ayy Aq \u2018paydajas S japow IN 9d1 (X).LP yas aunjeay Juaipesd jesodwiay Buipuodseso9 ad pue (y) Jp Jes ein3eaj quaipeus asim -jauuedo Buipuodsassod ay} JO auo 3Seal Je OF Buipioo2e YUM 40SSaoo04d au Aq \u2018pauiwayap \u4e00 Sl ainysagosoiw UMoUY lpuods9JJo9 ayy ol asuodsay ul (x)4 18S auNjeaj yeuly e + 09zS- Bunl533o0JDIUU (~ Sulpuodsassoo ayy ol asuodsal Jesoduwa} e 1DejJ3Xa 0} Se Os \u2018sjas \u2018 gossaaqid ayy Ag \u2018pauuojied si + 94n3SagoJ9iuu 8uipuodseJJoo ay} 0} asSuodsa4Ul oszs _ \u548c \u4e86 asim-jauueys gelx9 03 Se OS oss99oud ad Aq \u201cpauudojHod $s! A Ores ~~ Sajdwies ainjsed pasiouep oss95o4d ay] Aq \u2018pawiopad st \u672c sayy e Aq \u2018pasiousp sf ajdwes 085CS a yjed aaind e Xe ge ul PABUeIE solAap ajqeseam _ Sposa Suiafasdas ayy Aq \u2018paanseaw 0CCS - JO BUO UMOU By} OF asuodsas ; suasn jo Aieinld e Jo Saduaiajaid | 0 LZS \u4e00 seumsaBorojus uMmouy jo AlleJnld\n\n8 9| J\n\nauoje pu BAIIeIBIU! BY} JO SUOIIOd Ss} siseunysedoqujw asow as Bep Fuses Boul JO sug ol SulpJo95e pauluayap Si e Suipnjoul yes aanysagouiw y\n\nUMOUY ui (x) Ep yas aunjeaj quaipers elep Buluser} Sy} jo yes UO uoQesado juaipesd jesodwaa \u516d UMOUY (X}9p Jes ein3esj quaipesd 'S48s elep SululeJl 9u1 Jo doee UO uoiie49do jUuaipesd asim-jauueyo e Sj} jo Yoee 00 uofesedo duydwies-as 19Hno3 e ainjsad ay} jo U52B3\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 11 of 18\n\n_ - hd \uff0c x 2 es \uff0c So : io \uff0c 3 : 3 ef . Soe \u6b64 \uff0c \u540d : o ar: \uff0c \uff0c Ld + 6 wn o in ow mY ws \u201c \u201c4 o \u3002 \u5de1 o \u8fc7 \u4eba wow w iF Fy\n\nOY 35ION-o3-IeuU6IS\n\nUS 11,822,732 B1\n\n0\n\nUO LL.\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 12 of 18\n\nUS 11,822,732 B1\n\nay POO} 108 Si \"SSE \u4eba 0 al THE \u8aaa SA\n\n4 i st \u4eba\n\n$h\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 13 of 18\n\nUS 11,822,732 B1\n\nWAI BA} isod SY] PUR JUDAD aaiesau oui 0 /CS JO UO se ldaAe JUBLUaAOW e Ahssej o alqeue si eolAap slqdejeaN ahf5oeJalul aAlisod 34} pue aS sJn3eaj aAhe3au ayy ym 4ossaooid ayy Ad \u2018paules silapou i PSUJEIGO S| SIOPAA SINjeas BAIISOd ay} Suipnpul 0 9\u00a2S LG 195 94nle9j Alsod e led YINS do199A aunjeaj shAllsod e 1524Xa 0} Sas elep JO SMOPUIM 9ulplls sd] jo OS CS ss 9Jnje9j Alle89u Be ed JO SMOPUIM 8ulptls ay} Jo yore uo oss99o4d ayy Ad \u2018pew sojied si Uoljesado PSUIEIGS SI SIOPSA SiNjea} SAETSU SU} SUIpNpul yons 40152aA BiNjeay BAWeBaU e 12Bj3Xa O} S1as elep yoea uo \u201cossaz01d ay) Ad \u2018pawuojiad si Uonelado \u672c MOPUIM \u4e2d HPplls5 dilA 5195 Elep Sululezy oAsod ay] JO ydea a JUSAS SATTISOd SUT 088S Lo asuodsai ul Suon4od apouyaja SuiAla994 au} Ad painseaw Si s195 ejep Suiules] \u751f JUSAS SATIESSU SUT OF OZES _V asuodsas ul \u2018suofod apospaya 8uiAleoaJ ay \u2018Aq paunseatu sj sjas eyep Sujules a pauljap ae Juana 9AHHlsod e pue quand anyesau V oles -\u00a3\n\nTE Old\n\naaesau 9jotu JO ouO\n\nSAjisod sid0U Jo auO\n\nad 1ed dpns \u2018Jas ainjzeay TIA uoneollsselp V Buller VANisod Jo yoea JUaipess asim-jauUeL \u00a5 Buiusjen aaiyesau yo yore jUuaipess asim-jauueyd Vy UO pue\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 14 of 18\n\nUS 11,822,732 B1\n\nL- qUaWAaAOW snonuiiuo5 QT ay) SujwuOpad SuuNp saguy Xepul ue quuny} e jo uonlsod e jnoge Buljeuiljsa Jo lnsei uolss9489JE APIAOIC O} alqeue 9Al2eJalul BU} 3e \u7531 dns 195 aunqeaj ad UUM doss99oJd ayy Aq \u2018paules si 09fS \u4e00 09SyS \u2014. paulelqo si sJO}VA 94n1e9 ad Buipnjoul yas Bunjeaj e Jey] dons '\"uoBeiado MOpUIM Sutplls ay} 8ubnp Satu 1Ua4ajHjHp }e O} aulieJj MOPUIM Suiplls ay} UO 340Ssseoo4Jd ad Aq \u2018pauuojed si uonesado \u672c \u4e00 auiejj MOPUIM 3ulpts e YIM S3as Blep Buluiesy ay} Jo yea UO paJOjad si OVS \u4e00 a o\u20acrsS\u2014 JIPYS BUSI9J3s BY} JO BUIAOLW ou BUIMO|JOY JaSN Bulpuodsassco ay} 10 JaSuly OM} UaSMjaq ssAouu Jasn Zulpuodsassoo ay} jo Jadu) duunu ad all Sjas elep Buluies yj JO yDes Yasn Sulpuodsasios ay Aq JUaWdAOW snonuuoo \u672c 0zpS \u4e8c UaaJDS ou UO suoilsod OM} Wl} paAotu si jeplls aouaiejal BY} YOIYM UI! uaoJ9s e UO paads ]ueisuo5 e je PSAOW \u672c 0LyS \u4e8c Ajaaipadsas \u201cSuoll0d apojl9ala SuiAi95949u+ Aq \"Painseat aie siasn juasajIp sjas elep Sulujes} jo Ayyeunjd e \u2018suasn juasayjip ayy Aq 3uauuaAou snonunuod\n\nVCT Old\n\nJo109A an3e \u76f4 32843XB 3uslipeJ3 aslA-jeuueu5 V UoHEJado MAopulm BuIpI{s \u00a5 xepul edl UO SUONISOd 2os5s99oJd ayy Aq \u2018painseawi si QT sd Suluuopiod Buying SI Japl|s seoueJajadV 9ui OF asuodsal Ul QT ad Sujwuopiad Buling\n\nUO Jesu S| 991A9p ajqeieam japow UO!SsaIZas [YI e\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 15 of 18\n\nVLDS ZLPS\u2014\n\n\u2018uoHeJado MOPUIM 3ulplls ay) SuLNp syas eyep Suiules 1uatuaAouu-snonulluo9 -QT ay} JO yoea ssod5e paAouu sl auie4d MOpUIM 8uiplls 84} YIUM Ul \u2018Wes MOPUIM s}es elep Bululesy luaouueAouu-snonuiiuo9-dt adl jo Yoee UO paujuojlod st uoeieado | \u4e00 JOPIS 82Uajejei BU] JO BUIAOW ad 3uUiMAoioj Josn Suipuodsasi0d std jo JaBUly xopul ed UO SUOITSOd OM] UTAaMjaq SeAOU JasN SY} JO JaSuy quinuy} ed alldN \u4eba Joss99oJd ay} Ad \u2018painseaw si sjas elep Buluies} -GT 84] JO yea 295n 8uipuodseoo ay] Ag JUaWaAOW Snonuuoo QT sd SuimUopiad\n\nBuipuodsai09 juasuueAouu-snonuiluo Bung\n\nBUIDIS e YM MOpUIM 8utplls \u00a5\n\nUS 11,822,732 B1\n\nact\n\nOld\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 16 of 18\n\nvonised\n\npayeunisg \u548c 9 \u5916 4 URRY ASION :9AITeaSN <emmm ode ae SAAC PUA SP 39 3 \u4eba 4 \u4eba \u4eba \u5168 SA\n\nSSedaA071 jeubig nduy\n\nUS 11,822,732 B1\n\n5T \u2018Old\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 17 of 18\n\nUS 11,822,732 B1\n\np-OE xX ETE 401 x PLT 501x978 * OANBALIOC %WCO'b WET MOE TAS dIW NAS-3 NNY slspoJA \u2018sjapow pezieuosied oul uo Snsel uolss9469 \u5c0d (\u4e2d oOLx6566 y-0OIx//T \u00a2 01x PZT 1oADeALHeG %8 %S'E %8 t tasw TIN uAS NN~I SISPOIN \"slepou oueue6 oui UO s}insei uolsSs91l69Mi (e)\n\nvt Old\n\npu\n\npuC\n\nU.S. Patent\n\nNov. 21, 2023\n\nSheet 18 of 18\n\n0 LSS\n\nOZSS\n\n4UBAS JUSWIBAOW 0} esuodsai ul jas eyep e azAjeue ol japouu puooas e pue ysuly e Suis SSION 1uaA9 BAHlsod 4 quand BAlleSau 1UaAa aAllisod e jo 1UaAa BANETSU e se jdUaAa 1uatuaAouu BUY 3UIUJa3ap 03 1UaAa _/| \u2018ul Jes Elep ayy SulzAjeue Aq japow TIA uogeoyissej2 adl YBNOIY) palsisseo sl | \u5341 suogiod apojloala BulAfade ay} \u2018Aq poJnseauu SI JUaAd 1UBuuaAou ou ol\n\nasUOdsal UI Jas eiep yy\n\nJUaWBAOW ay} 01 SUOdSal 3UaAa jUatuaAou ou\n\n4\n\nUS 11,822,732 B1\n\nST \u201cDId\n\nUS 11,822,732 B1\n\n1\n\n2\n\nINTERACTIVE WEARABLE DEVICE AND METHOD OF MACHINE LEARNING BASED TRAINING THEREOF\n\ndevice is configured to analyze the movement event at least according to a variation of a data set in response to the movement event, measured by receiving electrode portions.\n\nField of the Invention:\n\nThe present invention generally relates to an interactive wearable device and machine learning (ML) based training methods to train the interactive wearable device. More specifically the present invention relates to the interactive wearable device using electric field sensing technology and the ML based training methods to train the interactive wearable device for recognizing and continuous tracking of known microgestures and thumb\u2019s position of the wearer.\n\nBACKGROUND OF THE INVENTION:\n\nThumb-to-index-finger (T2I) microgesture interaction, such as tapping, rubbing, and circling with the thumb on the index finger, has shown a wide range of applications and benefits in natural, efficient, and privacy-preserving wearer input interfaces. It can be applied to text input, mobile interaction, drone controlling, Internet of Things (IoT), and AR/VR interaction. Thumb gesturing on an index-finger pad is akin to operating a touchable surface (e.g., touch screen) with an index finger. Compared with performing gestures in suspension or on other large surfaces, T2] microgesture interaction could potentially support more simple and pri- vacy-preserving wearer-machine interactions with less physical demand.\n\nAttracting increasing research interest in recent years, detecting T2I microgestures is considered to be a challeng- ing problem due to the small range and the occlusion problem associated with thumb motions. Recent researches have proposed to use different sensors and recognition methods to support such type of wearer inputs, including millimeter-wave radar, front-facing passive infrared (PIR) sensors, RGB Camera, on-thumb motion sensors, magnetic sensors, and touch-sensitive foils.\n\nHowever, some of these existing detectors for detecting T2I microgestures are not suitable for wearers due to their large size and/or insufficient accuracy. Therefore, there is a need to develop a new detector to address the aforesaid issues.\n\nSUMMARY OF THE INVENTION:\n\nIt is an objective of the present invention to provide an interactive wearable device for recognizing and analyzing a movement event caused by two or more fingers of a wearer of the interactive wearable device, and machine learning (ML) based methods to train the interactive wearable device.\n\nIn accordance with one embodiment of a first aspect of the present invention, the interactive wearable device includes a ring body and a detector. The ring body includes a top insulating layer, a bottom insulating layer, and an interme- diate insulating layer disposed in between the top and the bottom insulating layers. The detector includes a receiving electrode layer disposed in between the top and the inter- mediate insulating layers, a transmitting electrode layer disposed in between the intermediate and the bottom insu- lating layers, and a ground electrode layer embedding in the bottom insulating layer and electrically coupled to an elec- trical ground. The receiving electrode layer has a plurality of receiving electrode portions separated from each other, and the receiving electrode portions thereof are arranged in a matrix and along a curve path. The interactive wearable\n\nIn accordance with another embodiment, the ring body is configured to wear on the first finger, such that the receiving electrode portions of the receiving electrode layer are arranged around the first finger along the curved path during the movement event.\n\nIn accordance with a second aspect of the present inven- tion, a machine learning (ML) based training method to train a wearable device for recognizing one or more known microgestures by two different fingers includes the follow- ing steps.\n\n15\n\nStep 1: One or more microgesture training data sets in response to the known one or more microgestures is mea- sured by a plurality of separated receiving electrode portions of the wearable device arranged in a matrix and along a curve path.\n\nStep 2: a channel-wise gradient operation is performed, a processor of the wearable device, on each of microgesture training data sets, so as to extract a channel- wise gradient feature set in response to the corresponding known microgesture.\n\nby\n\n25\n\nStep 3: a temporal gradient operation is performed, by processor of the wearable device, on each of the microges- training data sets, so as to extract a temporal gradient feature set in response to the corresponding known micro- gesture.\n\nture\n\n30\n\nStep 4: a final feature set in response to the corresponding known microgesture is determined according to at least the corresponding channel-wise gradient feature set corresponding temporal gradient feature set.\n\nof\n\nthe\n\nStep 5: a ML model is trained, by the processor, with final feature sets, such that the wearable device is enable recognize the known microgestures in response to the final feature sets through the ML model.\n\n40\n\nIn accordance with one embodiment of the present inven- tion, each of the microgesture training data sets comprises a plurality of gesture samples, each of the gesture samples comprises N measurement frames, and each of the measure- ment frames comprises a plurality of electrical output sig- nals measured by the receiving electrode portions, respec- tively.\n\nIn accordance with a second aspect of the present inven- tion, a machine learning (ML) based training method to train a wearable device for recognizing one or more known microgestures by two different fingers includes the follow- ing steps.\n\n45\n\nStep 6: during a period of performing the 1D continuous movement by the different wearers, a plurality of 1D-con- tinuous-movement training data sets in response to the different wearers, respectively, are measured, by a plurality of separated receiving electrode portions of the wearable device arranged along a curve path.\n\n60\n\nStep 7: a sliding window operation on each of the 1D-con- tinuous-movement training data sets with a sliding window frame is performed, by a processor of the wearable device, in which the sliding window frame is moved across each of the 1D-continuous-movement training data sets during the sliding window operation.\n\n65\n\nStep 8: a channel-wise gradient operation is performed, the processor of the wearable device, on the sliding window frame to extract a feature vector at different times during the sliding window operation, such that a feature set including the feature vectors is obtained.\n\nby\n\nStep 9: a ML regression model is trained, by the proces- with the feature sets, such that the wearable device\n\nsor,\n\nthe\n\nthe\n\none\n\nand\n\nthe\n\nto\n\nis\n\nUS 11,822,732 B1\n\n3\n\n4\n\nenable to provide a regression result of estimating about position of the thumb finger on the index finger during period of performing the 1D continuous movement.\n\na\n\na\n\ndisclosure is written to enable one skilled in the art practice the teachings herein without undue experimenta-\n\ntion.\n\nto\n\nBRIEF DESCRIPTION OF THE DRAWINGS:\n\nDetailed Configuration about the interactive wearable device 100\n\nEmbodiments of the invention are described in more details hereinafter with reference to the drawings, in which:\n\nFIG. 1A depicts a schematic diagram of an exterior of an interactive microgesture recognition device according to an embodiment of the present invention;\n\nFIG. 1B depicts a circuit block diagram of the interactive microgesture recognition device in the FIG. 1A;\n\n10\n\nFIG. 1A depicts a schematic diagram of an exterior of an interactive wearable device 100 according to an embodiment of the present invention. FIG. 1B depicts a block diagram of the interactive wearable device 100 according to an embodi- ment of the present invention. FIG. 2 depicts an exploded. view of the interactive wearable device 100 in the FIG. 1A under different visual angles. FIG. 3 depicts a vertical cross-sectional view of the detector DD.\n\nFIG. 2 depicts schematic diagrams of the interactive microgesture recognition device in the FIG. 1A under dif- ferent visual angles;\n\nFIG. 3 depicts a vertical cross-sectional view of a detector the interactive microgesture recognition device in the FIG. 1A;\n\nof\n\nFIG. 4 depicts a circuit model o FIG. 1A; the detection device\n\nthe\n\nFIG. 5A depicts a scheme diagram about a wearer wearing with the detector in the FIG. 1A under different wearing angles;\n\nFIG. 5B depicts a step flow chart of determining the optimal wearing angle according to an embodiment of the invention;\n\npresent\n\nFIG. 6 depicts different microgestures and preferences of different wearers for these microgestures;\n\nFIG. 7 depicts an auto-encoder-decoder model according an embodiment of the present invention;\n\nFIG. 8 depicts a step flow chart of a training method for training the first ML model according to an embodiment of the present invention;\n\nFIG. 9 depicts an average signal-to-noise ratio (SNR) on data set after applying the low-pass filter with different cut-off frequencies ranging from 50 Hz to 94 Hz;\n\nFIG. 10 shows a testing accuracy of each of the known microgestures with different models and under different evaluation;\n\nFIG. 11 depicts a step flow chart of a training method for training the classification ML model according to an embodiment of the present invention;\n\nFIG. 12A depicts a step flow chart of a training method training the second ML model according to an embodiment the present invention;\n\nof\n\nto\n\nFIG. 12B depicts a step flow chart of a step $410 of the training method for training the second ML model according an embodiment of the present invention;\n\nFIG. 13 depicts a pipeline diagram in the FIG. 12A;\n\nFIG. 14 shows a result of mean squared error (MSE) regression model; and\n\neach\n\nthe\n\nFIG. 15 depicts a step flow chart of an analysis process interactive wearable device.\n\nin\n\nfor\n\nof\n\nof\n\n15\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\n50\n\nReferring to the FIG. 1A, in the embodiment, an interac- tive wearable device 100 includes a detector DD, a circuit board CB, and a plurality of conductive wires CW. Referring to FIGS. 1A and 1B, the detector DD is electrically coupled. to the circuit board CB through the conductive wires CW. The circuit board CB includes a processor P, a memory M coupled to the processor P and a signal generator SG. Referring to FIGS. 2 and 3, the detector DD includes a ring body 112, a receiving electrode layer 114, a transmitting electrode layer 116, a ground electrode layer 118, a strap S, a plurality of separated conductive electrode portions CP1, a conductive electrode portion CP2, a plurality of enameled wires EW, and a plurality of magnetic elements MM1, MM2. The aforesaid elements and the arrangement therebe- tween will be described follows.\n\nfully\n\nas\n\nReferring to FIGS. 2 and 3, the ring body 112 includes a top insulating layer 112a, an intermediate insulating layer 112, and a bottom insulating layer 112c. The intermediate insulating layer 1126 is disposed in between the top and the bottom insulating layers 112a@, 1126. The intermediate insu- lating layer 112c is conformally stacked on the bottom insulating layer 112b. The top insulating layer 112a is conformally stacked on the intermediate insulating layer 112c.\n\nTo be more specifically, each of the insulating layers 112a, 112+, 112c has a middle portion and a pair of side portions connecting to the middle portion thereof. The middle portion MP1 of the top insulating layer 112c, the middle portion MP2 of the intermediate insulating layer 1125, and the middle portion MP3 of the bottom insulating layer 112c protrude upwardly to collectively form an annular portion of the ring body 112. The middle portions MP1, MP2, MP3 are concentrically disposed with each other and share the same center C of circle.\n\nThe two side portions SP1 of the top insulating layer 112a extends toward two opposite sides of the ring body 112, respectively. The two side portions SP1 of the top insulating layer 112a make contact with the two side portions SP2 of the insulating layer 1126, respectively. The two side portions SP2 of the intermediate insulating layer 1124 extends toward two opposite sides of the ring body 112, respectively. The two side portions SP2 of the intermediate insulating layer 1124 make contact with the two side portions SP3 of the bottom insulating layer 112c, respectively.\n\nDETAILED DESCRIPTION:\n\nIn the following description, an interactive wearable device, and machine learning based training methods of training the interactive wearable device and the likes are set forth as preferred examples. It will be apparent to those skilled in the art that modifications, including additions and/or substitutions may be made without departing from the scope and spirit of the invention. Specific details may be omitted so as not to obscure the invention; however, the\n\n60\n\n65\n\nThe exemplary materials of the insulating layers 112a, 1124, 112c may include insulating materials, such as poly- vinyl chloride (PVC), and the present invention is not limited thereto. In some embodiments, the insulating layers 112a, 1124, 112c are fabricated using a stereolithography (SLA) 3D printer, and the present invention is not limited thereto.\n\nIn addition, the bottom insulating layer 112c further includes a pair of hangers H extending downward from portions SP3 of the intermediate insulating layer\n\nside\n\nthe\n\n112c.\n\nUS 11,822,732 B1\n\n5\n\n6\n\nThe strap S passes through the two hangers H to collectively define a wearable space WS with the ring body 112, such that the wearer can wear the detector DD on his/her finger through the wearable space WS. In some embodiments, the strap S includes a Velcro tape. The wearer can adjust the position/shape of the strap S to optimize the wearable space WS according to his/her requirement. In some embodiments, the ring body 112 is shaped so as to equip on the index finger of the wearer.\n\nThe receiving electrode layer 114 (i.e., antenna structure) may be disposed (or sandwiched) in between the top and the intermediate insulating layers 112a, 112h. The receiving electrode layer 114 may be disposed on a top surface of the intermediate insulating layer 1126. The receiving electrode layer 114 includes a plurality of receiving electrode portions RP separated from each other, and each of the receiving electrode portions RP may serve as a channel of the antenna structure. In some embodiments, the number of the receiving electrode portions RP is, for example without limitation, 5. The receiving electrode portions RP are arranged in a matrix and along a curve path on the top surface of the middle portion MP2 of the intermediate insulating layer 112. The receiving electrode portions RP of the receiving electrode layer 114 are equally spaced along a circumferential direc- tion.\n\nReferring to FIG. 3, in the embodiment, a covering angle 6 of each of the receiving electrode portions RP is defined as an angle included by two extending lines extending from two opposite edges thereof to the center C of circle, respec- tively. A total covering angle 6 of the receiving electrode layer 114, which means a maximum sensing angle range of the detector DD, is defined an angle included by two extending lines extending from two endmost opposite edges of the receiving electrode layer 114 to the center C of circle, respectively. In some embodiments, the covering angle 0 is 20 degrees(\u00b0), and the total covering angle \u00a9 is 120 degrees (*).\n\nThe transmitting electrode layer 116 may be disposed (or sandwiched) in between the intermediate and the bottom insulating layers 1126, 112c. The length of the transmitting electrode layer 116 in the circumferential direction may be longer than that of the receiving electrode layer 114.\n\nThe ground electrode layer 118 may be embedded in the bottom insulating layer 112c. In detail, the bottom insulating layer 112c includes two separated portions, and the ground electrode layer 118 is disposed (or sandwiched) therebe- tween. The ground electrode layer 118 is electrically coupled to an electrical ground. The receiving electrode layer 114, the transmitting electrode layer 116, and the ground elec- trode layer 118 are concentrically disposed with each other and share the same center C of circle.\n\nThe exemplary materials of\n\nthe receiving electrode layer\n\n114, the transmitting electrode layer 116, and the ground electrode layer 118 may be conductive and elastic materials, for example, thin-film conductive fabric or metal. In some embodiments, the aforesaid electrode layers 114, 116, 118 are produced by a laser cutter.\n\n25\n\n40\n\n45\n\nfor example, a central processing unit (CPU) with one core or multiple cores, a microprocessor, or other programmable processing unit, Digital Signal Processor (DSP), Program- mable Processor, Application Specific Integrated Circuits (ASIC), Programmable Logic Device (PLD) or other similar devices.\n\nIn some embodiments, the memory M is stored with trained classification machine learning (ML) model CM, trained first ML model ML1, and a trained second ML model ML2. The detailed training process is fully described in the following paragraphs.\n\nIn some embodiments, the signal generator SG is an oscillator to provide a square wave AC electrical signal.\n\nReferring to FIG. 2 again, each of the conductive elec-\n\ntrode portions CP1 wraps the side portion SP1 of the top insulating layer 112a. Specifically, each of the conductive electrode portions CP1 extends from a top surface of the side portion SP1 to a bottom surface of the side portion SP1 through a side surface of the side portion SP1. Each of the enameled wires EW extends from the corresponding con- ductive electrode portion CP1 to a position on the corre- sponding receiving electrode portion RP, such that the corresponding conductive electrode portion CPl may be coupled to the corresponding receiving electrode portion RP. Referring to FIGS. 1 and 2, the circuit board CB may be coupled to the receiving electrode layer 114 through the conductive wires CW, the conductive electrode portions CP1, and the enameled wires EW. On the other hand, the circuit board CB may be coupled to the transmitting elec- trode layer 116 through the conductive electrode portion CP2. Thus, the elements of the circuit board CB may be coupled to the receiving electrode layer 114 and the trans- mitting electrode layer 116 through the aforesaid configu-\n\nration. Furthermore,\n\nin\n\norder\n\nto\n\ncontact\n\na\n\nproper\n\nensure\n\nbetween the enameled wire EW and the corresponding conductive electrode portion CP1, the magnetic elements MM1, MM2 may be used to solve the aforesaid issue. The magnetic elements MM1 may be disposed on a bottom surface of the top insulating layer 112a. The magnetic elements MM1 may be disposed on the receiving electrode portions RP, respectively. The magnetic elements MM1 may be disposed on ends of the enameled wires EW, respectively. Each of the magnetic elements MM1 may be disposed on an overlap area between the corresponding receiving electrode portion RP and the corresponding enameled wire EW. On the other hand, the magnetic elements MM2 may be dis- posed on the top surface of the intermediate insulating layer 1124. The magnetic elements MM2 may be disposed under the receiving electrode portions, respectively. The magnetic elements MM2 may be disposed under the ends of the enameled wires EW. By such a configuration, the relative position between the receiving electrode portion RP and the enameled wire EW is more stable, thereby achieving a better connection through the magnetic force generated by the magnetic elements MM1, MM2. In some embodiments, the magnetic elements MM1, MM2 are, for example without\n\nThe circuit board CB may be a print circuit board (PCB), a flexible circuit board, or a rigid circuit board. In some embodiments, the circuit board CB is a PCB board based on Microchip MGC3130, for example, and the present inven- tion is not limited thereto. The circuit board CB includes a processor P, a memory M coupled to the processor P, and a signal generator SG.\n\nIn some embodiments, the processor P is a hardware with computing capability, and is used to manage the overall operation of the device. In the embodiment, the processor\n\nis,\n\n60\n\n65\n\nlimitation, magnets.\n\n100\n\nBrief Sensing Principle of the interactive wearable device\n\nas\n\nThe sensing principle of the present invention is described follows.\n\nIn the present invention, the interactive wearable device 100 adopts electric field sensing technology for analyzing movement event caused by two different fingers of an wearer TG. FIG. 4 depicts a circuit model of the detection device DD. Referring to FIG. 4, in the embodiment, the signal\n\na\n\na\n\nin\n\na\n\nUS 11,822,732 B1\n\n7\n\n8\n\ngenerator SG of the circuit board CB provides at least one input electrical signal Vi (e.g., input voltage signal) to the transmitting electrode layer 116. In some embodiments, the input electrical signal V, is a low-voltage alternating voltage signal, and its switching frequency is, for example without limitation, 115 kHz.\n\nWhere A represents the overlapping area between the two electrodes, d is the relative distance between the electrodes, \u20ac, represents relative permittivity of a dielectric between the two electrodes with respective to vacuum permittivity, and represents vacuum permittivity.\n\n\u20ac\n\nBefore analysis process\n\nof the present invention, the\n\nwearer TG can wear on the detector DD by putting his/her finger into the wearable space WS of the detector DD, such that the receiving electrode portions RP is fitted around wearer's TG finger along a curve path during a movement event caused by the wearer\u2019s fingers. In some embodiments, the detector DD detects the thumb-to-index finger move- ment by generating an electric field around the ring-worn position around the index finger. The receiving electrode layer 114 receives a transmitted alternating current (AC) signal through a capacitive path between the transmitting electrode layer 116 and the receiving electrode layer 114 in response to the input electrical signal V,. When the wearer\u2019s TG hand or finger intrudes an electrical field generated by the AC signal, the low-frequency energy is capacitively coupled into the human body. Since the human body is much larger than the size of the electrode, the portion of the human body that is out of the electrical field serves as another charge reservoir. In other words, the coupling of the human body causes the change of the capacitance of the detector DD. Such changes can be processed to track the body movements and classify the movement patterns. When the relative movements between the thumb finger and the index finger occur, the capacitance changes, and the output elec- trical signal V (ie., output voltage signal) from the receiv- ing electrode portions RP of the receiving electrode layer\n\n114 also changes correspondingly.\n\nBased on the aforementioned sensing principle, it is implemented a multiple-receiver electric-field sensing sys- tem for the ring-formed detector DD of the interactive wearable device 100 to enrich the sensing resolution for detecting the subtle finger movement. Furthermore, since the receiving portions RP of the receiving electrode layer 114 are arranged along a curved path, which can conform/fit to the shape of the wearer\u2019s TG finger, the signal measured/ sensed/detected by the detector DD of the present invention can more accurately respond to a movement event caused by fingers of the wearer TG.\n\nGiven the input electrical electrode layer 116, the out K receiving electrode following equation (1). signal as V, to the transmitting ut electrical signal as V,* from portion RP is formulated by the\n\nthe\n\n\u4eba 0 \u8aaa = \u5404 + \u4e00 \u4e00 \u4e00 \u4e00 Ch + Chg Cr 50\n\nWhere C,,* represents a capacitance between the ke receiv- ing electrode portion RP of the receiving electrode layer 114 and the transmitting electrode layer 116, C,\u00a2* represents a capacitance between the k* receiving electrode portion RP of the receiving electrode layer 114 and the ground electrode layer 116, and C,,;\u201c is a capacitance between the k\" receiving electrode portion RP of the receiving electrode layer 114 and wearer\u2019s TG finger.\n\nThe capacitance C between two electrodes is given by equation (2).\n\nfollowing\n\nthe\n\nA \u4eba)\n\n10\n\n20\n\n25\n\n30\n\n35\n\n40\n\n45\n\n60\n\n65\n\nCombining the equations (1) and (2), it can be inferred that the electric field is affected by parameters A and d mainly, which means that placement of the transmitting electrode layer 116 and the receiving electrode layer 114 affects the sensing range of the detector DD. In the actual hardware implementation, the relative positions between the receiving electrode layer 114, the transmitting electrode layer 116, and the ground electrode layer 118 usually do not change. Thus, given the equations (1) and (2), it can be inferred the following equation (3).\n\nGB)\n\nWhere d,,;\u201c represents a distance between the Kk receiving electrode portion RP of the receiving electrode layer 114 and wearer\u2019s finger, and A,,,\u201c represents an overlapping area between the K receiving electrode portion RP of the receiv- ing electrode layer 114 and wearer\u2019s finger. The equation (3) indicates the output electrical signal V,* from the KE receiv- ing electrode portion RP of the receiving electrode layer 114 changes proportionally to the distance between the finger and the k\u2019 receiving electrode portion RP of the receiving electrode layer 114, and changes inversely proportional to the overlapping area between them. In short, it is reasonable to hypothesize that the change of the output electrical signal V.* reflects the movement pattern of the thumb finger which may indicate the type of the T2I gestures.\n\nOptimizing device wearing angle\n\nIn addition, the present invention provides a method to optimize the device wearing angle. FIG. 5A depicts a scheme diagram about a wearer TG wearing on the detector DD under different wearing angles. Assuming the wearer TG and the detector DD are located in a space defined by X axis, Y axis, and Z axis perpendicular to each other. The left part, middle part, and the right part in the FIG. 5A depict the schematic diagrams of the wearer TG wearing on the detec- tor DD in the FIG. 1A under 0 degree, 45 degrees, and 90 degrees, respectively. A wearing angle is defined as an included angle between a facing direction of the receiving electrode layer (e.g., as marked as the dark grey shade area) and a YZ plane defined by the Y axis and Z axis. FIG. 5B depicts a step flow chart of determining the optimal wearing angle according to an embodiment of the present invention. FIG. 6 depicts different microgestures (a)~(j) and corre- sponding wearer\u2019s rating. FIG. 7 depicts an auto-encoder- decoder model according an embodiment of the present invention. The detailed steps S100~S150 for optimizing device wearing angle/orientation are fully described in com- bination with FIGS. 5A, 5B, 6 and 7 as follows.\n\nReferring to FIGS. 5B and 6. In the step S100: A microgesture set including a plurality of discrete microges- tures (a)~(j) different from each other is determined, as shown in the FIG. 6. A discrete microgesture is defined as a gesture performed by two different fingers that is completed with a sequence of motions (e.g., drawing a circle) and is mapped to a specific command/event. In some embodi- ments, the microgestures (a)~(j) include, for example, 10 T2I microgestures, such as (a) Tap microgesture, (b) Double Tap microgesture, (c) Swipe Up microgesture, (d) Swipe\n\nUS 11,822,732 B1\n\n9\n\n10\n\n(j)\n\nDown microgesture, (e) Swipe Left microgesture, (f) Swipe Right microgesture, (g) Triangle microgesture, (h) Check microgesture, (i) Counterclockwise Circle microgesture, and Clockwise Circle microgesture.\n\nIn the step S110: Wearing-angle training data sets are measured, by the receiving electrode portions RP of the receiving electrode layer 114, during a period of performing each of microgestures by two fingers of each of wearers under different wearing angles. In some embodiments, the wearing angle is, for example without limitation, 0 degree, 45 degrees, or 90 degrees.\n\n10\n\nIn the embodiments of the present invention, the interac- tive wearable device 100 is configured to recognize and analyze a movement event caused by an index finger and a thumb finger of the wearer. Specifically, the interactive wearable device 100 recognizes one or more known micro- gestures and provides a regression result of estimating a position of the thumb finger on the index finger of 1D continuous movement performed by two thumb and index fingers. In order to achieve the above objective, three ML models ML1, CM, ML2, are trained. Each of the training methods of the ML models ML1, CM, ML2 is fully described as follows.\n\nIn the step $120: A Convolutional Neural Network (CNN) based encoder-decoder ML model is trained with the wear- ing-angle training data sets in response to microgestures, thereby regenerating/reconstructing output electrical signals of each of the wearing-angle training data set. Referring to FIG. 7, in some embodiments, the CNN based encoder- decoder model includes six 2D convolutional layers for the encoder and six 2D transposed convolutional layers for the decoder.\n\nIn the step S130: A latent feature vector set is extracted, by the processor P, from each of the regenerated wearing- angle training data sets, in which the latent feature vector set used as a low-dimensional representation for Calinski- Harabasz (CH) Index computation in the following step.\n\nis\n\nIn the\n\n$140: Values of CH Index of the\n\nstep regenerated wearing-angle training data sets are computed, by the pro- cessor P, using the latent feature vector sets. In one aspect, the output electrical signals of each microgesture are regarded as one cluster. Specifically, for a specific wearing angle, a better gesture distinguishability leads to a larger between-clusters dispersion in the latent space, while a good feature stability across different wearers precipitates a small inter-cluster dispersion in a latent space. It should be noted that a CH Index indicates a between-clusters dispersion and inter-cluster dispersion of the output electrical signal clus- ters. The CH Index CH is given as the following equation\n\n20\n\n25\n\n30\n\n35\n\nFirst of all, a ML model ML1 is trained with training in response to one or more known microgestures. process steps of the method to train the ML model ML1 described as follows.\n\nsets\n\nfully\n\nFIG. 8 depicts a step flow chart of a training method for training the first ML model according to an embodiment of the present invention. FIG. 9 depicts an average signal-to- noise ratio (SNR) on data set after applying the low-pass filter with different cut-off frequencies ranging from 50 Hz to 94 Hz.\n\nReferring to FIG. 8, in the step S210: A microgesture set including a plurality of known (or selected) microgestures is determined according to the preferences of a plurality of wearers. Referring again to FIG. 6, each of the wearers is asked to wear on the detector DD through their index finger, and then is asked to perform the known microgestures (a) to (j). After that, each of the wearers is asked to rate each of the known microgestures (a) to (j) according to three criteria along a 7-point Likert-scale (1: strongly disagree to 7: strongly agree).\n\nThe criteria 1, \u201cEase to perform\u201d is abbreviated as \u201cEase\u201d the FIG. 6, means that it is easy to perform this gesture precisely for the wearer.\n\nin\n\nThe criteria 2, \u201cSocial acceptance\u201d is abbreviated \u201cSocialAcc\u201d in the FIG. 6, means that the microgesture performed without social concern.\n\nbe\n\n(4).\n\n40\n\nThe criteria 3, \u201cFatigue\u201d in the FIG. 6, means that the microgesture makes the wearer tired.\n\n(4)\n\nWhere SSac represents a between-clusters sum of square, SS we tepresents an inter-cluster sum of square, K is the total number of clusters, and N is the total number of samples. With the ground truth cluster label for each gesture sample in the latent space feature point, SS,,, and SSwe can be calculated as the following equations (5), (6):\n\n\u592a Scale =a? \u00ae\n\n45\n\nThus, based on the result of preferences of the wearers shown in the FIG. 6, a microgesture set including micro- gestures (a)~(i) (e.g., as marked as a light gray shade region in the FIG. 6) is determined (or selected), and the micro- gesture (j) (e.g., as marked as a dark gray shade region in the FIG. 6) is not to be selected due to low wearer preferences.\n\nIn the step $220: One or more training data set in response to the known one or more microgestures (a)~(i) is measured, by the receiving electrode portions RP of the interactive wearable device 100 arranged in a matrix and along a curve path. In detail, each of the wearers is asked to wear on the detector DD on his/her index finger, and performs each of the microgestures (a)~(i) several times.\n\n50\n\n=-y* cn (6) S806 = Die ol Cll 55\n\nwhere k represents the Ke cluster, C, is the set of points in is the cluster center of k, and n, is the number of points k. c is the center of all sample in the space.\n\nc, in\n\nIn the step $150: An optimal wearing angle is determined according to the values of CH Index of the regenerated wearing-angle training data sets. The results shows that CH value (55.04) of wearing angle at 90 degrees outperforms that (36.13) of wearing angle at 0 degree and that (24.12) of wearing angle at 45 degrees. Thus, the optimal wearing angle is, for example, 90 degrees.\n\nk,\n\n60\n\n65\n\nFor example, in this particular experiment conducted, the\n\ndata set measurement process included two parts. With respect to the first part, each of the wearers was asked to repeat each of the known microgestures (a)~(i) 40 times until he/she finished all known gestures in random order. In another experiment conducted, each of the wearers was asked to take off the detector DD and put it on again after every 10 times of recording so to increase the data variance. The data sets measured from the first part served as micro- gesture training data sets. With respect to the second part, each of the wearers was asked to repeat each known micro- gesture 10 times, and the known microgestures were shuffled randomly. The data sets measured from the second part\n\nserved as testing data sets.\n\ndata The\n\nare\n\nas\n\ncan\n\nUS 11,822,732 B1\n\n11\n\n12\n\nEach of microgesture training data sets includes a plural- ity of gesture samples X (e.g., the number of the gesture Samples X is 40), each of the gesture samples X includes N measurement frames X and each of the measurement frames includes a plurality of electrical output signals measured by the receiving electrode portions RP at a timestamp t, respec- tively.\n\nWhere each of the gesture samples X is represented as X={x,}t e{0, . . . N-1}\u20acR(SXN); and the measurement frames x, is represented as x,=[V,', V7,...,.V/\"...V/\")\" V represents an output electrical signal measured by n\u2122\u201d channel (i.e., n\u201d receiving electrode portion RP) of the receiving electrode layer 114, and m represents the numbers of the receiving electrode portion RP (e.g., m=5). In some embodiments, N is a positive integer and is set to be 190.\n\nIn the step $230: each of the gesture sample X is denoised, by a filter (not shown in the figures), such that a nail portion and a tail portion of the N measurement frames of the corresponding gesture sample X are removed. In some embodiments, the number of the removed frames is, for example without limitation, 19 measurement frames. Thus, the noisy measurement occurred while activating and ter- minating the recording program can be avoided.\n\nIn the step $240: a Fourier re-sampling operation is performed, by the processor P, on each of the denoised gesture samples X, such that the sample length on every channel to be N. N is, for example, 190. Then, a low-pass filter with cut-off frequency of 80 Hz is applied to filter each of the gesture samples X again after the Fourier re-sampling operation, such that the potential high-frequency random noise signal generated by the circuit is filtered. The value of 80 Hz for the cut-off frequency of the low-pass filter is determined as it achieved the highest signal-to-noise ratio (SNR) as shown in the FIG. 9.\n\n10\n\n20\n\n30\n\nprocessor P, on each of the electrical output signals in each the measurement frames of each of the gesture samples X, as to extract the temporal gradient feature set dT(X) response to the corresponding known microgesture, which dT(X)eRO\u2122.\n\nof\n\nso\n\nIn the step $270: a final feature set F(X) in response to the corresponding known microgesture is determined, by the processor P, with according to at least one of the correspond- ing channel-wise gradient feature set dC(X) and the corre- sponding temporal gradient feature set dT(X). In some embodiments, only the channel-wise gradient feature set dC(X) serves as the final feature set F(X). In some embodi- ments, only the temporal gradient feature set dT(X) serves as the final feature set F(X). In some embodiments, a combination of the channel-wise gradient feature set dC(X) and the temporal gradient feature set dT(X), in which F(X)=dC(X)UdT(X),eRo*\u2122, serves as the final feature set F(X).\n\nIn the step $280: the ML model ML1 is selected, by processor P, according to a signal type of the feature\n\nF(X).\n\nFor example, in some embodiments, if the signal type of the final feature set F(X) is time-domain, the ML model ML1 is selected, by the processor \u4e86 from a support vector machine (SVM) model, a multilayer perceptron (MLP) model, or a convolutional neural network (CNN) model.\n\nIn some embodiments, a Short-time Fourier transform (STFT) operation is performed on each feature channel of the final feature set F(X), such that signal type of the feature set F(X) is transformed into frequency-domain. Under such acondition, the ML model ML1 is selected, by the processor P, from a visual geometry group (VGG) model, a visual geometry group (VGG) model, a residual networks (ResNet) model, a densely connected convolutional network (DenseNet) model, or a vision transformer (ViT) model.\n\nIn the step $250: a channel-wise gradient operation is performed, by the processor P, on each of the microgesture training data sets, so as to extract a channel-wise gradient feature set dC(X) in response to the corresponding known microgesture. In detail, the channel-wise gradient operation is performed on any two of the electrical output signals of each of the measurement frames of the each of the gesture samples X, so as to extract the channel-wise gradient feature set in response to the corresponding known microgesture.\n\nWhere the channel-wise gradient feature set dC(X) represented as: dC(X)=AV,,)' lt \u20ac{0, ... , N-1), G, j) \u20acP,}eRO\u00b0\u2122 and (i, j) represents the channel pairs in the gesture sample X. AV,\u00bb is given by the following equation\n\n(7).\n\nis\n\n35\n\n45\n\nIn the step $290: the selected ML model ML1 is trained with the final feature sets F(X), by the processor P, such that the interactive wearable device 100 is enable to recognize the known microgestures (a)~(i) in response to the feature sets F(X) through the selected ML model ML1.\n\nIn some embodiments, in the step $280, a data-augmen- tation scheme is performed by rolling each of the gesture samples X by a random offset, in which the data-augmen- tation probability is set to be, for example, 0.5 for every gesture sample X. Also, in some embodiments, a label- smoothing technique with the smoothing parameter (e.g., \u20ac=0.1) is performed during the training process. Thus, the generalizability of the ML model MLI is improved, and the oyer-fitting issue is avoided.\n\n1 7 t \u4e86 ka ; O) AM \u4e09\n\nWhere V; represents the output electrical signal measured by i\u201d channel (i.e., i\u201d receiving electrode portion RP); and VF represents the output electrical signal measured by \u8fd9 channel (i.e., j receiving electrode portion RP). That is to say, the AV;,,\u2019 in the channel-wise gradient feature set represents a spatial correlation between each pair of the i\u201d and j\u201d receiving electrode portions RP.\n\nIn the step $260: a temporal gradient operation is per- formed, by the processor P, on each of the microgesture training data sets, so as to extract a temporal gradient feature set dT(X) in response to the corresponding known micro- gesture. In detail, for each of the microgesture training data sets, the temporal gradient operation is performed, by the\n\n50\n\n55\n\n60\n\n65\n\nFIG. 10 shows a testing accuracy of each of the known microgestures with different models and under different evaluation. Referring to FIG. 10, \u201cW\u201d represents within- wearer evaluation, \u201cL\u201d represents a leave-3-wearer-out evaluation, \u201cT\u201d represents a temporal gradient feature set, \u201cC\u201d represents a channel-wise gradient feature set, and \u201cT+C\u201d represents a mix gradient. The upper part in the FIG.\n\n10 shows the models trained with feature sets in the time domain, and the lower part in the FIG. 10 focuses on the evaluation in the frequency domain.\n\nIn some embodiments, during the evaluation of the gen- eralizability of the gesture classification using the trained ML model ML1, two data-splitting schemes 1, 2 are applied. 14 wearers are asked to participate the evaluation. Data sets measured from all 14 wearers are used as within-wearer evaluation, with the split of 3:1:1 for the training, the validation, and the testing data respectively. Then, the selected ML model is trained with the microgesture data sets\n\nin\n\nin\n\nthe\n\nset\n\nUS 11,822,732 B1\n\n13\n\n14\n\nmeasured from 11 wearers. The selected ML model is tested on 3 left-out wearers as a leave-three-wearer-out evaluation. The 3 left-out wearers are selected based on their signal quality. Specifically, it is calculated the average SNR for data set of each of wearers, and 3 wearers are picked whose SNRs are three lowest as three leave-three-out testing data- set. During the leave-three-out evaluation, it is used a random 30% subset of the microgesture training data sets as the validation data sets.\n\nAccording to a result in the FIG. 10, overall, transferring the time-series signal to the frequency domain can achieve a higher performance compared to directly using the signals in the time domain. This is because the 2D data represen- tation of the signal in the frequency domain allows the usage of deep neural networks for hidden feature extraction. The results also show that in general, using the T+C data leads to a better performance than using T or C only. This suggests the importance of both the channel-wise gradient feature set and the temporal gradient feature sets for known microges- ture classification on the interactive wearable device 100.\n\nSecondly, a classification ML model CM is trained with at least one negative training data sets in response to the negative event, and is trained with at least one of positive training data sets in response to the positive event. The steps of training method to train the ML model ML2 are fully described as follows.\n\nFIG. 11 depicts a step flow chart of a training method for training the classification ML model CM according to an embodiment of the present invention.\n\n10\n\n15\n\n20\n\n25\n\nincluding the positive feature vectors is obtained. In some embodiments, while each positive feature set includes vector with the dimension of R\u00b0*?,\n\nIn the\n\nA classification ML model\n\n$370:\n\nCM\n\nstep trained, by the processor P, with the negative feature set and the positive feature set, such that the interactive wearable device 100 is enable to classify a movement event as one of the negative event and the positive event. In some embodi- ments, the classification ML model CM is a binary SVM classifier. In addition, in some embodiments, in the step S20, a grid-searching mechanism is performed, by the processor P, to find the optimal set of hyper-parameters (RBF kernal, C =1000, =0.1) for the SVM classifier.\n\ng\n\nThirdly, a ML model ML3 is trained with a plurality of 1D-continuous-movement training data sets in response to different wearers, in which each of the 1D-continuous- movement training data sets is measured during a period of performing1D continuous movement by a thumb finger and an index finger of the corresponding wearer. The steps of method to train the ML model ML3 are fully described as follows.\n\nFIG. 12A depicts a step flow chart of a training method for training the second ML model ML2 according to an embodi- ment of the present invention. FIG. 12B depicts a step flow chart of a step $410 of the training method for training the second ML model ML2 according to an embodiment of the present invention. FIG. 13 depicts a pipeline diagram in the FIG, 12A.\n\nReferring to FIG. 11, in the step S310: A negative event and a positive event are defined. In detail, the negative event may include, for example, two different situations, such as a separate event of the thumb finger and the index finger and a staying-still event of the thumb finger and the index finger. The separate event of the thumb finger and the index finger represents that the thumb finger is not in contact with the index finger. The staying-still event of the thumb finger and the index finger represents that thumb finger is in contact with the index finger but staying still. On the other hand, the positive event may include, for example, one or more known microgestures (a) to (j) as shown in the FIG. 6.\n\nIn the step $320: One or more negative training data sets is measured by, the receiving electrode portions RP, in response to the negative event during a negative event caused by the two fingers.\n\nIn the step S330: One or more positive training data sets is measured by, the receiving electrode portions RP, in response to the positive event during a positive event caused by the two fingers.\n\nIn the step $340: A sliding window operation is per- formed, by the processor P, on each of the negative training data sets and on each of the positive training data sets with a sliding window, in which the sliding window is moved across each of the negative training data sets and each of the positive training data sets. In detail, during the sliding window operation, the processor P applies a sliding window frame F with the step size of 5 ms (1 frame) across the training data set (16 ms, 3-frames).\n\n30\n\n35\n\n40\n\n45\n\n50\n\n35\n\nReferring to FIGS. 12A and 13, in the step S410: during a period of performing the 1D continuous movement by the different wearers, a plurality of 1D-continuous-movement training data sets in response to the different wearers are measured, by the receiving electrode portions RP, respec- tively. To be more specific, the step $410 further includes two steps $412, S414.\n\nReferring to the FIG. 12B, in the step $412: A reference slider is moved at a constant speed on a screen, in which the reference slider is moved from two positions on the screen. The position information of the two positions on the screen in the step $412 is also recorded. In detail, an experiment facilitator firstly shows a 600-pixel-long reference slider with its handle gradually moving in a constant speed (400 pixels/second) on the screen.\n\nIn the step $414: during a period of performing the 1D continuous movement by the corresponding wearer, each of the 1D-continuous-movement training data sets is measured, by the processor P, while the thumb finger of the corre- sponding wearer moves between two positions on the index finger of the corresponding wearer following the moving of the reference slider. In some embodiments, the two positions on the index finger are, for example, the distal of the index finger and a middle phalanx of the index finger, and the tip of the distal phalanx serves as a starting point during a period of performing1D continuous movement.\n\nto\n\nThen, in some embodiments, each of the 1D-continuous- movement training data sets is filtered, by a low-pass filter, remove circuit noise after the step S410.\n\nIn the step $350: A channel-wise gradient operation is performed, by the processor P, on each of the sliding windows of each of negative training data sets to extract a negative feature vector, such that a negative feature set including the negative feature vectors is obtained.\n\nIn the step S360: A channel-wise gradient operation is performed, by the processor P, on each of the sliding windows of each of positive training data sets to extract a positive feature vector, such that a positive feature set\n\n60\n\n65\n\nIn the step $420: a sliding window operation is performed on each of the 1D-continuous-movement training data sets with a sliding window frame F (see FIG. 13), in which the sliding window frame F is moved across each of the 1D-con- tinuous-movement training data sets during the sliding win- dow operation. In detail, during the sliding window opera- tion, the processor P applies a sliding window frame F with the step size of 5 ms (1 frame) across the training data set (16 ms, 3-frames).\n\na\n\nis\n\nUS 11,822,732 B1\n\n15\n\n16\n\nIn the step $430: a channel-wise gradient operation performed, by the processor P, on the sliding window frame to extract a feature vector at different times during the sliding window operation, such that a feature set including the feature vectors is obtained.\n\nF\n\nIn the step S440: a ML regression model is trained, by the processor P, with the feature sets, such that the interactive wearable device 100 is enable to provide a regression result of estimating about a position of a thumb finger on an index finger during a period of performing the 1D continuous movement. In some embodiments, the ML regression model is a k-nearest neighbors (KNN) model (k=5), an Epsilon- Support Vector Regression (e-SVR, RBF kernal, C=0.1, g=1) model, or a small multilayer perceptron (MLP) model.\n\nis\n\nthe\n\nnegative movement event through the classification model CM to determine if the movement event is a negative or a positive event.\n\nevent\n\nIf the movement event is classified as the negative event, then the movement event is analyzed to be noise (as shown the FIG. 13), and if the movement event is classified as positive event, then the movement event is analyzed accord- to the first and second ML models ML1, ML2.\n\nin\n\ning\n\nIn some embodiments, the data set in response to the movement event is analyzed/processed by the processor P by performing the steps similar to the steps $210 to $280, and the interactive wearable device 100 determines a microges- ture of the movement event to be classified as one of the microgestures (a)~(i) in FIG. 6 through the first ML model ML1.\n\nThen, in some embodiments, a customized Kalman Filter is employed to smooth out the regression result.\n\nIn addition, various hand sizes and finger lengths of different wearers could affect the ranges of the signal values while wearers moving the thumb along the index finger. For wearers who have a higher ratio of a wearer\u2019s thumb width to his/her index finger length (Le., large thumb over short index finger), the moving range is smaller, and the sensor signal range is also smaller, and vice versa. To reduce the effect of this problem, in some embodiments, a personalized model is a normalized/calibrated generic model with the aforesaid parameters taken into consideration.\n\nFIG. 14 shows a result of mean squared error (MSE) of each regression model. To examine the smoothness of the simulated real-time tracking, the 2nd derivative of the regression result after applying our customized Kalman Filter. A smaller 2nd derivative value reveals a smoother regression result. Tables (a), (b) in the FIG. 14 show result of our experiments on the generic models and the person- alized models respectively. For the generic models, e-SVR. model shows the smallest MSE, while MLP model has the smoothest prediction. Although MLP model shows the best smoothness, it relies on large data for a better convergence. In practice, it is reasonable to collect a small amount of data from a new wearer for calibration. Such calibration data may also be used for training a personalized model for a better\n\nperformance.\n\nBy the assistance of the classification ML model, a first and a second ML models, the interactive wearable device analyzes a movement event caused by two fingers of an wearer. The analysis process is fully described as follows.\n\nFIG. 15 depicts a step flow chart of an analysis process the interactive wearable device 100.\n\nReferring to the FIG. 15, as an wearer wears on the detector DD through his/her index finger, a movement event caused by the index finger and the thumb finger is detected/ measured by the detector DD. Referring to FIG. 15, spe- cifically, in the step $510: A data set in response to the movement event is measured by, the receiving electrode portions RP.\n\nIn the step $520, the movement event is classified through the classification ML model by analyzing the data set in response to the movement event, so as to determine if the movement event is a negative event or a positive event. A sliding window operation and a channel-wise gradient operation, which are similar to the steps $340, $350, are performed on the data set, and the descriptions are not repeated herein. Thus, a feature set including feature vectors in response to the movement event is determined. The processor P compares the feature set in response to the movement event and the negative feature set in to\n\nof\n\n25\n\n40\n\n45\n\n60\n\n65\n\nIn some embodiments, the data set in response to the movement event is analyzed/processed by the processor P by performing the steps similar to the steps S420, S430 to obtain an estimate result of continuous 1D tracking of the thumb\u2019s position on the index through the second ML regression model ML2.\n\nBased\n\nabove,\n\nin\n\nthe\n\nof the\n\nembodiments\n\non present invention, an interactive wearable ring-formed device is provided. The interactive wearable device adopts electric field sensing technology due to its potentially high sensitiv- ity to detect a movement event caused by two fingers of an wearer. The detector of the interactive wearable device includes three electrode layers, which are, a receiving elec- trode layer, a transmitting electrode layer, and a ground electrode layer. Each of the electrode layers is arranged along a corresponding curved path. Thus, when a wearer wears the detector through his/her finger, the shape of the electrode layer can better fit the shape of the finger, which is advantageous to improve signal accuracy in response to\n\nthe movement event. Also, the interactive wearable device is trained to by at\n\nleast two ML based methods for recognizing known one or more microgestures and continuous 1D tracking of the thumb\u2019s position on the index finger, respectively. In some embodiments, steps of performing channel-wise gradient operation and temporal gradient operation are applied to a training method of a first ML model related to recognize known one or more microgestures, so as to obtain fina\u2019 feature sets for training the first ML model. In some embodi- ments, steps of performing sliding window operation an channel-wise gradient operation are applied to a training method of a second ML model related to continuous 1D tracking of the thumb\u2019s position on the index finger, so as to obtain final feature sets for training the second ML model. Thus, the interactive wearable device adopting the first and second ML models shows an excellent accuracy in recog- nizing known microgestures or tracking 1D tracking of the\n\nthumb\u2019s position.\n\nThe functional units and modules of the devices and methods in accordance with the embodiments disclosed herein may be implemented using computing devices, com- puter processors, or electronic circuitries including but not limited to application specific integrated circuits (ASIC), field programmable gate arrays (FPGA), microcontrollers, and other programmable logic devices configured or pro- grammed according to the teachings of the present inven- tion. Computer instructions or software codes running in the computing devices, computer processors, or programmable logic devices can readily be prepared by practitioners skilled in the software or electronic art based on the teachings of the present invention.\n\nresponse\n\nML\n\nthe\n\nUS 11,822,732 B1\n\n17\n\n18\n\nAll or portions of the methods in accordance to the embodiments may be executed in one or more computing devices including server computers, personal computers, laptop computers, mobile computing devices such as smart- phones and tablet computers.\n\nThe embodiments\n\ninclude\n\nmay computer storage media, transient and non-transient memory devices having com- puter instructions or software codes stored therein, which can be used to program or configure the computing devices, computer processors, or electronic circuitries to perform any of the processes of the present invention. The storage media, transient and non-transient memory devices can include, but are not limited to, floppy disks, optical discs, Blu-ray Disc, DVD, CD-ROMs, and magneto-optical disks, ROMs, RAMs, flash memory devices, or any type of media or devices suitable for storing instructions, codes, and/or data.\n\n15\n\n3. The interactive wearable device of claim 1, further comprising:\n\na memory module having a classification machine learn- ing (ML) model configured for classifying the move- ment event, a first ML model configured for recogniz- ing one or more known microgestures performed by the first and second fingers, and a second ML model configured for providing a regression result of estimat- ing a position of the second finger on the first finger in an one-dimensional (1D) continuous movement per- formed by the first and second fingers; and\n\nprocessor coupled to the detector and the memory module, wherein the processor is configured with pro- cessor-executable instructions to perform operations comprising:\n\na\n\nclassifying the movement event through the classifica- tion ML model to determine if the movement event is a negative event or a positive event; and\n\nEach of the functional units and modules in accordance with various embodiments also may be implemented in distributed computing environments and/or Cloud comput- ing environments, wherein the whole or portions of machine instructions are executed in distributed fashion by one or more processing devices interconnected by a communica- tion network, such as an intranet, Wide Area Network (WAN), Local Area Network (LAN), the Internet, and other forms of data transmission medium.\n\nThe foregoing description of the present invention has been provided for the purposes of illustration and descrip- tion. It is not intended to be exhaustive or to limit the invention to the precise forms disclosed. Many modifica- tions and variations will be apparent to the practitioner skilled in the art.\n\n20\n\n25\n\n30\n\nif\n\nthe\n\nis classified\n\nthe negative be\n\nmovement event as event, then the movement event is analyzed to noise;\n\nif\n\nthe movement event is classified as the positive event, then the data set in response to the movement event is analyzed according to at least one of the and second ML models.\n\nfirst\n\n4. The interactive wearable device of claim 3, wherein the classification ML model is trained with at least one of negative training data sets in response to the negative event, and is trained with at least one of positive training data sets in response to the positive event.\n\nor\n\n\u00a7. The interactive wearable device of claim 3, wherein negative event comprises a separate event of the two fingers a staying-still event of the two fingers.\n\nthe\n\nThe embodiments were chosen and described in order to best explain the principles of the invention and its practical application, thereby enabling others skilled in the art to understand the invention for various embodiments and with various modifications that are suited to the particular use contemplated.\n\n35\n\n40\n\n6. The interactive wearable device of claim 3, wherein the first ML model is trained with a plurality of microgesture training data sets in response to one or more discrete known microgestures, and the second ML model is trained with a plurality of continuous movement training data sets in response to different 1D continuous movements.\n\n7. The interactive wearable device of claim 1, wherein the ring body is shaped so as to equip on the index finger.\n\nWhat is claimed is:\n\n1. An interactive wearable device for analyzing a move- ment event caused by a first and a second fingers of a wearer, the interactive wearable device comprising:\n\na\n\nring body comprising a top insulating layer, a bottom insulating layer, and an intermediate insulating layer disposed in between the top and the bottom insulating layers; and\n\na detector comprising a receiving electrode layer disposed in between the top and the intermediate insulating layers, a transmitting electrode layer disposed in between the intermediate and the bottom insulating layers, and a ground electrode layer embedding in the bottom insulating layer and electrically coupled to an electrical ground,\n\nwherein\n\nthe receiving electrode layer has a plurality receiving electrode portions separated from each and arranged in a matrix and along a curve path,\n\nof\n\nother\n\nwherein the interactive wearable device is configured to analyze the movement event at least according to a variation of a data set in response to the movement event, measured by receiving electrode portions.\n\n45\n\n50\n\n35\n\n60\n\n8. The interactive wearable device of claim 1, wherein receiving electrode portions of the receiving electrode layer concentrically disposed with each other.\n\nare\n\na\n\n9. Amachine learning (ML) based training method to train wearable device for recognizing one or more known microgestures performed by two different fingers, compris- ing:\n\nmeasuring, by a plurality of separated receiving electrode portions of the wearable device arranged in a matrix and along a curve path, one or more microgesture training data sets in response to the known one or more microgestures, respectively;\n\nperforming, by a processor of the wearable device, channel-wise gradient operation on each of the micro- gesture training data sets, so as to extract a channel- wise gradient feature set in response to the correspond- ing known microgesture;\n\nperforming, by the processor of the wearable device, temporal gradient operation on each of the microges- ture training data sets, so as to extract a temporal gradient feature set in response to the corresponding known microgesture;\n\nthe\n\na\n\na\n\n2. The interactive wearable device of claim 1, wherein the ring body is detachably worn on the first finger, such that the receiving electrode portions of the receiving electrode layer are arranged around the first finger along the curved path during the movement event.\n\n65\n\ndetermining, by the processor of the wearable device, a final feature set in response to the corresponding known microgesture, according to at least one of the corre- sponding channel-wise gradient feature set and the corresponding temporal gradient feature set; and\n\nUS 11,822,732 B1\n\n19\n\n20\n\ntraining, by the processor, a ML model with the feature sets, such that the wearable device is enable recognize the known microgestures in response to final feature sets through the ML model.\n\nfinal\n\nto\n\nthe\n\n10. The method of claim 9, wherein each of the micro- gesture training data sets comprises a plurality of gesture samples, each of the gesture samples comprises N measure- ment frames, and each of the measurement frames comprises a plurality of electrical output signals measured by the receiving electrode portions, respectively.\n\n11. The method of claim 10, wherein in the step performing the channel-wise gradient operation,\n\nof\n\n10\n\nduring a period of performing the 1D continuous move- ment by the different wearers, measuring, by a plurality of separated receiving electrode portions of the wear- able device arranged along a curve path, a plurality D-continuous-movement training data sets response to the different wearers, respectively;\n\nperforming, by a processor of the wearable device, a sliding window operation on each of the 1D-continu- ous-movement training data sets with a sliding window frame, wherein the sliding window frame is moved across each of the 1D-continuous-movement training data sets during the sliding window operation;\n\nthe\n\nchannel-wise gradient operation is performed, by processor, on any two of the electrical output signals each of the measurement frames of the each of gesture samples, so as to extract the channel-wise gradient feature set in response to the corresponding known microgesture.\n\nthe of the\n\n15\n\nperforming, by the processor of the wearable device, channel-wise gradient operation on the sliding window frame to extract a feature vector at different times during the sliding window operation, such that a feature set including the feature vectors is obtained; and\n\n12. The method of claim 10, wherein in the step of performing the temporal gradient operation,\n\nthe temporal gradient operation is performed, by processor, on each of the electrical output signals each of the measurement frames of each of the gesture samples, so as to extract the temporal gradient feature set in response to the corresponding known microges- ture.\n\n13. The method of claim 10, wherein each of the mea- surement frames is measured at a timestamp by the receiving electrode portions, and N is a positive integer.\n\n14. The method of claim 10, wherein before the steps of performing the channel-wise gradient operation and the\n\nthe in\n\n20\n\n25\n\n30\n\ntraining a ML regression model, by the processor of wearable device, with the feature sets, such that wearable device is enable to provide a regression result of estimating about a position of the thumb finger the index finger during a period of performing the continuous movement.\n\n17. The method of claim 16, wherein the step of measur- the 1D-continuous-movement training data sets, method further comprising:\n\ning\n\nmoving a reference slider at a constant speed on a screen, wherein the reference slider is moved from a first position to a second position on the screen; and\n\ntemporal gradient operation, the method further comprising: for each of the microgesture training data sets,\n\ndenoising each of the gesture samples; and\n\nperforming a Fourier re-sampling operation on each of the denoised gesture samples.\n\n15. The method of claim 9, wherein before the step of training the ML model, further comprising:\n\nselecting the ML model, by the processor, according to signal type of the final feature set.\n\n16. A machine learning (ML) based training method to train a wearable device to track a position of a thumb finger on an index finger during a period of performing a one- dimensional (1D) continuous movement by the thumb finger and the index finger, the method comprising:\n\na\n\n40\n\nmeasuring each of the 1D-continuous-movement training data sets, by the processor, while the thumb finger the corresponding wearer moves between two positions on the index finger of the corresponding wearer lowing the moving of the reference slider.\n\n18. The method of claim 17, wherein the ML regression model is trained with the feature sets and a ground-truth information about position information of the first second positions on the screen.\n\n19. The method of claim 17, wherein the ML regression model comprises a k-nearest neighbors (KNN) model, Epsilon-Support Vector Regression (\u20ac-SVR) model, or small multilayer perceptron (MLP) model.\n\n\u515a \u4e8b \u7576 \u515a\n\nof\n\nin\n\na\n\nthe\n\nthe\n\non\n\n1D\n\nthe\n\nof\n\nfol-\n\nand.\n\nan\n\na", "type": "Document"}}