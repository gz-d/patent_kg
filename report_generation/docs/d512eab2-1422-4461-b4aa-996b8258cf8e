{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "document", "Document"], "kwargs": {"metadata": {"source": "/home/guanzhideng145/research/ip_portal/patent_kg/patents/723-US20210044791A1(Pending) re (Done on website already).pdf"}, "page_content": "IN\n\nUS 20210044791A1\n\n( 19 ) United States\n\n( 12 ) Patent Application Publication ( 10 ) Pub . No .: US 2021/0044791 A1\n\nZhang et al .\n\n( 43 ) Pub . Date :\n\nFeb. 11 , 2021\n\n( 54 ) VIDEO QUALITY DETERMINATION\n\n( 2006.01 )\n\nG06T 7/13\n\n( 2006.01 )\n\nSYSTEM AND METHOD\n\nG06T 7/00\n\n( 52 ) U.S. CI .\n\n( 71 ) Applicant : City University of Hong Kong ,\n\nCPC\n\nH04N 13/144 ( 2018.05 ) ; H04N 17/004\n\nKowloon ( HK )\n\n( 2013.01 ) ; G06T 7/50 ( 2017.01 ) ; G06T\n\n2207/30168 ( 2013.01 ) ; G06T 7/97 ( 2017.01 ) ;\n\n( 72 ) Inventors : Yun Zhang , Shenzhen ( CN ) ; Tak Wu\n\nG06T 7/0002 ( 2013.01 ) ; G06T 2207/10016\n\nSam Kwong , Quarry Bay ( HK )\n\n( 2013.01 ) ; G06T 7/13 ( 2017.01 )\n\n( 21 ) Appl . No .: 16 / 531,281\n\n( 57 )\n\nABSTRACT\n\nA computer - implemented method and related system for\n\nAug. 5 , 2019\n\n( 22 ) Filed :\n\ndetermining a quality of a synthesized video file . The\n\nmethod includes processing a reference video file and a\n\nPublication Classification\n\nsynthesized video file associated with the reference video\n\nfile to compare the original video file and the synthesized\n\n( 51 ) Int . Cl .\n\nvideo file . The method also includes determining an extent\n\nH04N 13/144\n\n( 2006.01 )\n\nof flicker distortion of the synthesized video file based on the\n\n( 2006.01 )\n\nH04N 17/00\n\nprocessing .\n\n( 2006.01 )\n\nG06T 7/50\n\n100\n\nProcessing a reference video file and the associated synthesized video\n\n102\n\nfile\n\nDetermining an extent of flicker\n\nDetermining an extent of spatial\n\n104B\n\n104A\n\ntemporal activity distortion of\n\ndistortion of the synthesized\n\nthe synthesized video file\n\nvideo file\n\n106\n\nDetermining a quality of the synthesized video file\n\nUS 20210044791A1\n\nas) United States a2) Patent Application Publication\n\ndo Pub. No.: US 2021/0044791 Al\n\n(43) Pub. Date: Feb. 11, 2021\n\nZhang et al.\n\n(54) VIDEO QUALITY DETERMINATION SYSTEM AND METHOD\n\nGO067 7/13 (2006.01) G06T 7/00 (2006.01)\n\n(52) U.S. CL CPC veces HO4N 13/144 (2018.05); HO4N 17/004 (2013.01); GO6T 7/50 (2017.01); GO6T 2207/30168 (2013.01); GO6T 7/97 (2017.01): GO6T 7/0002 (2013.01); GOT 2207/10016 (2013.01); GO6T 7/13 (2017.01)\n\noo (71) Applicant: City University of Hong Kong, Kowloon (HK)\n\nInventors: Yun Zhang, Shenzhen (CN); Tak Sam Kwong, Quarry Bay (HK)\n\n(21) Appl. No.: 16/531,281\n\n(57) ABSTRACT\n\nA computer-implemented method and related system for determining a quality of a synthesized video file. The method includes processing a reference video file and a synthesized video file associated with the reference video file to compare the original video file and the synthesized. video file. The method also includes determining an extent of flicker distortion of the synthesized video file based on the processing.\n\n(22) Filed: Aug. 5, 2019 \u00b0\n\nPublication Classificati \u4eba\n\n(51) Int. Cl.\n\nHOAN 13/144 (2006.01) HO4N 17/00 (2006.01) G06T 7/50 (2006.01)\n\n160 LL\n\nProcessing a reference video file and the associated synthesized video | 102 file A Determining an extent of flicker Determining an extent of spatial- distortion of the synthesized video file Determining a quality of the synthesized video file temporal activity distortion of the synthesized video file 106\n\n;\n\n(72)\n\nWu\n\n104.\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 1 of 18\n\nUS 2021/0044791 A1\n\n104B\n\n106\n\n102\n\nr 100\n\nDetermining an extent of spatial temporal\n\ndistortion of the\n\nactivity\n\nfile\n\nvideo\n\nsynthesized\n\nvideo\n\nfile\n\nsynthesized\n\nDetermining a quality of the\n\nFigure 1\n\nsynthesized\n\nassociated\n\nvideo file\n\nthe\n\nvideo\n\nfile and\n\nProcessing a reference\n\nDetermining an extent of flicker\n\nvideo\n\nfile\n\n104A\n\ndistortion of the\n\nsynthesized\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021 Sheet 1 of 18\n\nPatent Application Publication\n\n9I OaplA pezisoyyuds oy} Jo Ayyenb e Sururaieled afJ ospIA paztfseUlduAs oy} ol o9DA Jo orrIolsIp AiIAHoe IeIoduol pezisoy As aq} JO orlolsrp -jetyeds jo ]uolxo UP BUTUTULIO}0q 4IoxoID JO laelxe Ue SurIuIUUIolotl ay ZOL | osepIA pozisoyyUAs peleIoosse oY} pue 9I OAplA ooUoIoJoIB SUISSIDOIg\n\noot \u4e00\n\nT oINnSLy\n\nVror\n\nPatent Application Publication\n\nUS 2021/0044791 A1\n\nFeb. 11 , 2021 Sheet 2 of 18\n\n88\n\n82\n\nIntensity\n\nThe\n\nVariation of Point\n\nAsus Frame 70 to Franie 90\n\nVariation of Point A Frame 70 to Fraine 90\n\nIntensity\n\nThe\n\n??\n\nFigure 2A\n\nFigure 2B\n\nPoint As on the\n\nEdge\n\nPoint A on the\n\nEdge\n\nSO\n\n( x , y ) = ( 1428,208 )\n\n( x , y ) ( 1428,208 )\n\nUndodancer . View 5 Synthesized\n\nView ( Depth QP 40 )\n\nUndodancer , Views Original\n\nView\n\nFeb. 11, 2021 Sheet 2 of 18\n\nPatent Application Publication\n\nUS 2021/0044791 Al\n\ngz ainsty tan cone top = dQ ipdacy) MALA pozrsaq AS \u00a7 MOLA \u201ciaoUEpopUr) \u201c96 atuesy OF HL addd\n\nVo oINnBly (BO7'8tP 1) = (48) SEA BBS \u4eba 3 aapa OU doy aod \u00a7 MOLA SnREEOPHCY Aa 06 2aw2g OF GL DALY\n\n\u201cSr preg JO NO0EEEA HUB 39\n\n\u2018y POY JO WoBSEEA ANSUsIUT ou,\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 3 of 18\n\nUS 2021/0044791 A1\n\nTessyr3\n\nlaret\n\nVistosta Gradient nu\n\nDeco\n\nDONA\n\nlick\n\nGtxitut\n\nExxxtion\n\nPertwe\n\nNe nesses\n\nx ???? ;\n\nDistorted besized V\n\nFlicker\n\nScore :\n\nvideo\n\nTeWXXayat Cowasion\n\n{ } }\n\nMeasurement\n\nDistortion\n\nFlicker\n\nRepresentation\n\nbased\n\nSparse\n\nDaleation\n\nEdge\n\n4330\n\nSES\n\nFigure 3\n\nXXXXBL89 :\n\nExtraction\n\nFaalista\n\nGradieu\n\nRelucu\n\ndicucnan y\n\nFestpora\n\nDraualdea V. ,\n\nOrigues\n\nGradient nun\n\nlayer\n\nxoszt\n\nTem\n\nTraining\n\nStage $ 53133338\n\nCowcision\n\nIcupom\n\nzver\n\nduta\n\nFeb. 11, 2021 Sheet 3 of 18\n\nUS 2021/0044791 Al\n\nPatent Application Publication\n\nUG WAVIg APMP te nae teat fe Sea 3 x: Do TO \u548c DSE 9593 \u4eba \u8aaa < de A 09BrAj9mGb FRRORIG 3 8 \u548c 8 sees Sutures yj, juomasmse: ay HOHAOISI Py paseq batas ds asands\n\n\u20ac arn3r\n\nsoy\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 4 of 18\n\nUS 2021/0044791 A1\n\nVideo Da\n\nEdge\n\nDepth\n\ntemporal en onverkon\n\nEdge delection\n\nand dilationi\n\nVideo\n\nwith\n\nTemporal\n\nEdge\n\nLayers { M\n\nVideo D )\n\nDepth\n\nFigure 4\n\n?? .\n\nGradient\n\nTemporal\n\n***\n\nmonster\n\nVickers\n\nTemporal\n\nLayers { U } }\n\n.. Synthesized\n\nVideo V\n\n$$ 322338\n\ntine T\n\nVida\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021 Sheet 4 of 18\n\nPatent Application Publication\n\nGQ Oapta 3 \u4e86 \u548c 9 Gem ff IBPTAN crip ; \u5168 094A Ho \u4eba A OSDLA pazisorpess\n\nPatent Application Publication\n\nUS 2021/0044791 Al\n\nFeb.11 , 2021 Sheet 5 of 18\n\n.\n\nFigure 5C\n\nFigure 5B\n\nFigure 5 ?\n\nFigure 5 ?\n\nVideo\n\nwidth W\n\nFigure 5A\n\nFeb. 11, 2021 Sheet 5 of 18\n\nUS 2021/0044791 Al\n\nPatent Application Publication\n\nAA WIPES OAP LAY dS ainsi q$ ainsi\n\naS oins1J\n\nVS angsty\n\nPatent Application Publication\n\nUS 2021/0044791 A1\n\nFeb. 11 , 2021 Sheet 6 of 18\n\nFigure 6C\n\nFigure 6F\n\nFigure 6E\n\nFigure 6B\n\nVideo\n\nwidth\n\n7 ,\n\nFigure 6D\n\nFigure 6A\n\n\u201dFeb. 11, 2021 Sheet 6 of 18\n\nUS 2021/0044791 Al\n\nPatent Application Publication\n\nFigure 6E\n\nFigure 6C\n\nFigure 6A\n\nFigure 6D\n\nPatent Application Publication\n\nUS 2021/0044791 A1\n\nFeb. 11 , 2021 Sheet 7 of 18\n\nTemporal\n\ndictionary\n\nTIL Figure 7B\n\nNE\n\ndictionary\n\nSpatial\n\nFigure 7A\n\nLE\n\n27\n\nFeb. 11, 2021 Sheet 7 of 18\n\nUS 2021/0044791 Al\n\nPatent Application Publication\n\nd/ armn3r euoholp jelodwa\n\nv4 9InSrH\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 8 of 18\n\nUS 2021/0044791 A1\n\n1 1 I I I\n\nI I 1 I\n\nSu\n\nAaaaa\n\nSAY\n\nDistortion\n\nMeasurement\n\nSpatio - temporal\n\nActivity\n\nPooling\n\nSy\n\nFigure 8\n\nvideos\n\nOriginal\n\nsynthesized\n\nDistorted\n\nvideos\n\nMeasurement SFY\n\nRepresentation\n\nSparse\n\nDistortion\n\nFlicker\n\nbased\n\nvideos\n\nDepth\n\n10\n\nKA\n\ni\n\ni \u65e5\n\nPatent Application Publication\n\nUS 2021/0044791 A1\n\nFeb. 11 , 2021 Sheet 9 of 18\n\nGOP 2N + 1 Frames\n\nAssessment\n\nQuality\n\nFigure 9\n\nHI\n\nTube\n\nSpatio - Temporal\n\nFeb. 11, 2021 Sheet 9 of 18\n\nUS 2021/0044791 Al\n\nPatent Application Publication\n\nSOUIEL A T+NZ dQ*) Wourssassy Anyend) aqny jaodway-onnds\n\nPatent Application Publication\n\nUS 2021/0044791 A1\n\nFeb. 11 , 2021 Sheet 10 of 18\n\n10F\n\nFigure\n\n100\n\nFigure\n\n101\n\nFigure\n\nVID\n\n10E\n\nFigure\n\n10B\n\nFigure\n\n10H\n\nFigure\n\n10A\n\nFigure\n\nFigure\n\n10G\n\n10D\n\nFigure\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021 Sheet 10 of 18\n\nPatent Application Publication\n\n[OT O1nBL HOT ainsi DOI9In3IH\n\n\u53ef OIoInSUI\n\nre Dof amnsiq\n\nWOT oIn31y\n\n\u540c doOT9InIH\n\nCOT In\n\nVOT 3InS a\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 11 of 18\n\nUS 2021/0044791 A1\n\nM\n\n// W\n\nW\n\n11C\n\nFigure\n\n11F\n\nFigure\n\n111\n\nFigure\n\n11H\n\nFigure\n\n11E\n\nFigure\n\n11B\n\nFigure\n\nho\n\nM\n\nUI /\n\n11A\n\nFigure\n\n116\n\nFigure\n\n11D\n\nFigure\n\nFeb. 11, 2021 Sheet 11 of 18\n\nUS 2021/0044791 Al\n\nPatent Application Publication\n\nJET arnSr AIT wnsLy HIE ainsi ODTII9InSIUH JIT 9InSTH OI ainsi dII 9InSIH CII ainsi VII arn3t \u53ef\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 12 of 18\n\nUS 2021/0044791 A1\n\n12F\n\nFigure\n\n12C\n\nFigure\n\n121\n\nFigure\n\nV\n\nV\n\nIN\n\nFigure\n\n12B\n\n12H\n\nFigure\n\n12E\n\nFigure\n\nV\n\nIN\n\nV\n\n:: .......\n\n12G\n\nFigure\n\n12A\n\nFigure\n\n12D\n\nFigure\n\nA\n\nV\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021 Sheet 12 of 18\n\nPatent Application Publication\n\nSl 9aInS3STIT\n\nasIn8U\n\nat\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 13 of 18 US 2021/0044791 A1\n\n210.432,648,864\n\n144,288,432,576\n\nFrame\n\nLength\n\nFigure 13\n\nResolution\n\nFourPeople\n\nTraining\n\nSequences\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021 Sheet 13 of 18\n\nPatent Application Publication\n\nOLE TEP BRE PFI ZHU PEL OO RP OLE TEP ROT FE GLE TEP BRT FFI OCLXOSEL Aueqpoy PSESRT TOT 9G OBP* CUR AAA \u4eba BRT FFI OCLAOSTL adoagmag FOS EPO TEP SIT OSO1 \u201cOZ61 SALICTHEqayseY xopuy made] pamenxg | \u4e2d Br ad | nonnposay | seonanheg Sunmay,\n\nCIoIngIH\n\n|\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 14 of 18 US 2021/0044791 A1\n\n0.125\n\nFigure 14\n\n0.327\n\nPSIINR\n\nSR - 3DVQA\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021 Sheet 14 of 18\n\nPatent Application Publication\n\nPee HER BESS PL 9InSUH rere VOACE-AS AES S65\u00b0G id ee S98\" EGET EAD REY HN | HSE usa | Fir \u672c sam | osu sper | wre een | cere | eoro | sore 9 \u661f\u4eba | Ben WISE | eave apo | gia | isro | asro asa | ssn FANS | sase geso | fea wise | aes BNR | ceca | sara | PANE \u533a NS | caen | one | BD Leo eG SHOR HLAGHY A Fitts ANS DLLD LG FUERSSIN ges Bao EH \u672c WIESE | Lee | sere | see | cere BHD | se | ween | fae UNG | 20088 | 3d | BSH | OCs aSWa | 20088 | cord | ASH \u548c de \u6709 RRC TEN \u5982 \u4eba 2 \u533a\n\nEse OED POD\n\nESA\n\neee sie cove cove\n\nAN AUS\n\ncore\n\n\u533a\n\ncers\n\nSAE\n\ngene HSWe\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 15 of 18 US 2021/0044791 A1\n\nCTCD\n\nFigure 15\n\nMSSSIM\n\nUNLASH\n\nWISS\n\nINSI MW\n\nWISS A\n\nPatent Application Publication\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021 Sheet 15 of 18\n\nPSELP I OOO T \u4eba 69 | pjoysenp GB RE LESS \u533a \u516d \u5168 LOR LY PCGLES LLCS LARRY SC VOLES | 1 \u7cfb \u4eba NScdrd \u5168 \u591a \u5982 \u5165 0 | LORE Pr ANS 1OOOE 3 SOR} POLee Te | TOO C | L/ecce\u2019s | T/O000'r HLAQIN LGV\u2019 LPCOUT \u8ba9 TASis se 1/0000\u00b0C TOLLee LST OBOE CT \u4eba \u591a \u591a \u5168 RO \u6709 ES LODO? tS & WISS ALL HINES DACP L | TEREST HoODre LLOEG\u2019 Pele | Lepert \u5168 \u5982 \u5168 [\u516d Bil ty THY LStOret | LALOL? ALLOS'C | \u4eba HOODE'S LQOOP \u6761 Bi | Sis Sey | Say ey | aptly | Gip \u201ceen\n\nGT arm3H\n\n|\n\n|\n\n|\n\ndN\n\ni\n\n\u7ad9\n\nA\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 16 of 18 US 2021/0044791 A1\n\nRMSE\n\nSROCC\n\n0.889\n\nFigure 16\n\n5\n\nSummation\n\nmethods\n\nPooling\n\nmethods\n\nPooling\n\nUS 2021/0044791 Al\n\nPatent Application Publication\n\nFeb. 11, 2021 Sheet 16 of 18\n\nLeg 8061 SOC CeO 5060 Wor RS \u5168 cleo FIGO MEG \u4eba 61 (Oe ipgog io Sophos any RE \u533a \u548c \u4e2a 6080 \u4eba \u756b \u4eba \u4e8c Loe so0G Lg00 L9G Ge8 0 \u4eba \u5bb6 \u5931\u8bed FORD Jannbs, 4680 \u8aaa 879 \u672c \u751f \u2018peip 9} augesTidhnyy \u7ad9 5800 O80 | ORs Oso, G80 Sig BONS GE8'O PREP RR BAIS Surg Ge CSD SB \u5168 were LAO ESE0 (ORO SSD $066 Pie SOSO) (here + Bes} (9 \u2018Tietp 9} OpMUREBS \u4eba Oh Fagqong\n\nOT OMB\n\nPatent Application Publication\n\nFeb. 11 , 2021 Sheet 17 of 18 US 2021/0044791 A1\n\nRMSE\n\nVideo SROCC\n\nDepth\n\nOriginal\n\nRMSE\n\nFigure 17\n\nVideo SROCC\n\nDepth\n\nSynthesized\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021 Sheet 17 of 18\n\nPatent Application Publication\n\n/IoInSIH ECE TIN S160 | 1Z60 | 160 Heo QC60 6060 FIG 8060 Spoo \u4e86 \u4eba \u5316 \u4eba 60 ET 68 \u4e2a \u548c\u5408 \u533a oopLa uidary prurdugy ACT pozisaumds SRST]\n\nPatent Application Publication\n\nUS 2021/0044791 A1\n\nFeb. 11 , 2021 Sheet 18 of 18\n\n200\n\n210\n\n212\n\n204\n\n202\n\nCommunication Device\n\nDisk\n\nDrive\n\nMemory\n\nProcessor\n\nFigure 18\n\nDevice\n\nOutput\n\nDevice\n\nInput\n\n208\n\n206\n\nFeb. 11, 2021 Sheet 18 of 18\n\nUS 2021/0044791 Al\n\nPatent Application Publication\n\nSI9InH\n\nUS 2021/0044791 A1\n\nFeb. 11 , 2021\n\n1\n\nincludes : determining respective extents of flicker distortion\n\nVIDEO QUALITY DETERMINATION\n\nfor each temporal frame of the synthesized video file ; and\n\nSYSTEM AND METHOD\n\ndetermining an overall extent of flicker distortion of the\n\nsynthesized video file based on the respective extents of\n\nTECHNICAL FIELD\n\nflicker distortion .\n\n[ 0001 ] The invention relates to a video quality determi\n\n[ 0009 ]\n\nPreferably , determining an extent of flicker distor\n\nnation method , and a related system implementing the\n\ntion of the synthesized video file based on the comparison\n\nmethod .\n\nfurther includes : weighting the respective extents of flicker\n\ndistortion for each temporal frame of the synthesized video\n\nBACKGROUND\n\nfile to determine the overall extent of flicker distortion .\n\n[ 0002 ] Three - Dimensional ( 3D ) an\n\nirtual reality ( VR )\n\n[ 0010 ]\n\nPreferably , processing the reference video file and\n\nvideo are becoming increasingly popular because it can\n\nthe synthesized video file includes : segmenting the reference\n\nprovide real depth perception , immersive vision , and novel\n\nvideo file into a plurality of temporal layers ; segmenting the\n\nvisual enjoyment for multimedia applications , such as in\n\nsynthesized video file into a plurality of temporal layers ; and\n\nomnidirectional videos , 3D Free - viewpoint Television , 3D\n\nprocessing the temporal layers of the reference video file and\n\nTelevision broadcasting , immersive teleconference , 3DoF\n\nthe temporal layers of the synthesized video file to identify\n\nand 6DOF VR .\n\nflicker distortion in the synthesized video file .\n\n[ 0003 ]\n\nObjective virtual viewpoint image ( VVI ) quality\n\n[ 0011 ]\n\nPreferably , processing the temporal layers of the\n\nmetrics for quantifying and determining ( e.g. , predicting )\n\nreference video file and the temporal layers of the synthe\n\nthe quality of these videos is highly desirable . This is\n\nsized video file includes : processing the temporal layers of\n\nbecause based on these metrics , visual processing tech\n\nthe reference video file to determine temporal gradient\n\nniques , e.g. , such as image / video compression , digital water\n\nlayers associated with the temporal layers of the reference\n\nmarking , and image / video reconstruction in 3D video sys\n\nvideo file ; and / or processing the temporal layers of the\n\ntem can then be optimized accordingly to improve the\n\nsynthesized video file to determine temporal gradient layers\n\nquality of experience of 3D or VR systems .\n\nassociated with the temporal layers of the synthesized video\n\n[ 0004 ]\n\nConventional 2D quality metrics , such as the well\n\nfile .\n\nknown Mean Squared Error ( MSE ) or Peak Signal - to - Noise\n\n[ 0012 ]\n\nPreferably , processing the temporal layers of the\n\nRatio ( PSNR ) , are relatively simple and are commonly used\n\nreference video file and the temporal layers of the synthe\n\nin measuring the quality of image and video applications .\n\nsized video file further includes : filtering the temporal\n\nHowever , these metrics are based on pixel - wise difference\n\ngradient layers of the reference video file to remove gradient\n\nbetween the distorted images and the source images , which\n\nfeatures with values below a threshold ; and filtering the\n\ncould not properly reflect the real perceptual quality of\n\ntemporal gradient layers of the synthesized video file to\n\nvisual signal ( video ) . In addition , these metrics are not based\n\nremove gradient features with values below a threshold .\n\non human perception of the visual signal ( video ) .\n\n[ 0013 ]\n\nPreferably , processing the reference video file and\n\nthe synthesized video file further includes : processing a\n\nSUMMARY OF THE INVENTION\n\nreference depth video file associated with the reference\n\nvideo file for facilitating comparison of the reference video\n\n[ 0005 ]\n\nIt is an object of the invention to address the above\n\nfile and the synthesized video file .\n\nneeds , to overcome or substantially ameliorate the above\n\n[ 0014 ]\n\nIn one example , in the 3D and VR applications ,\n\ndisadvantages or , more generally , to provide an improved\n\nthere are multiple views of colour videos and corresponding\n\nsystem and method for determining a quality of a synthe\n\ndepth videos to represent the 3D video . This is denoted as\n\nsized video file such as a synthesized view video file of 3D\n\n\u201c multi - view video plus depth ( MVD ) \" . In these cases , the\n\nor VR video system .\n\ndepth video file ( or reference depth video file ) is part of the\n\n[ 0006 ]\n\nIn accordance with a first aspect of the invention ,\n\nthere is provided a computer - implemented method for deter\n\n3D video file .\n\n[ 0015 ]\n\nPreferably , processing the reference depth video\n\nmining a quality of a synthesized video file . The method\n\nfile includes : processing the reference depth video file to\n\nincluding : processing a reference video file and a synthe\n\ndetect edges of the reference depth video file to generate a\n\nsized video file associated with the reference video file to\n\ndepth edge video file ; and segmenting the depth edge video\n\ncompare the reference video file and the synthesized video\n\nfile into a plurality of temporal depth layers .\n\nfile ; and determining an extent of flicker distortion of the\n\n[ 0016 ]\n\nPreferably , processing the reference depth video\n\nsynthesized video file based on the processing . The synthe\n\nfile further includes : processing the temporal depth layers to\n\nsized video file may be a 3D video file containing 3D video\n\nexpand the detected edge width in the temporal depth layers .\n\ndata , or it may be a virtual reality video file containing\n\n[ 0017 ]\n\nPreferably , processing the reference video file and\n\nvirtual reality video data .\n\nthe synthesized video file further includes : processing the\n\n[ 0007 ]\n\nIn one embodiment of the first aspect , the synthe\n\ntemporal gradient layers of the reference video file based on\n\nsized video at virtual viewpoint is rendered from the colour\n\nthe temporal depth layers to obtain weighted temporal\n\nvideos and depth videos of the neighboring real viewpoints ,\n\ne.g. , by using Depth Image based Rendering ( DIBR ) . For\n\ngradient layers associated with the reference video file ; and\n\nprocessing the temporal gradient layers of the synthesized\n\nexample , the synthesized videos can be rendered from\n\ndistorted colour / depth videos from neighboring real views\n\nvideo file based on the temporal depth layers to obtain\n\nweighted temporal gradient layers associated with the syn\n\n( left and right ) in the practical applications . Preferably , the\n\nsynthesized video file is created by processing the video file\n\nthesized video file .\n\n[ 0018 ]\n\nPreferably , processing the reference video file and\n\ncontaining the video at the real viewpoints .\n\n[ 0008 ]\n\nPreferably , determining an extent of flicker distor\n\nthe synthesized video file further includes : processing the\n\nweighted temporal gradient layers associated with the origi\n\ntion of the synthesized video file based on the comparison\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\nincludes: determining respective extents of flicker distortion for each temporal frame of the synthesized video file; and determining an overall extent of flicker distortion of the synthesized video file based on the respective extents of flicker distortion.\n\nVIDEO QUALITY DETERMINATION SYSTEM AND METHOD\n\nTECHNICAL FIELD\n\na quality nation method, and a related system implementing the method.\n\n[0009] Preferably, determining an extent of flicker distor- tion of the synthesized video file based on the comparison further includes: weighting the respective extents of flicker distortion for each temporal frame of the synthesized video file to determine the overall extent of flicker distortion.\n\nBACKGROUND\n\n[0002] Three-Dimensional (3D) and virtual reality (VR) video are becoming increasingly popular because it can provide real depth perception, immersive vision, and novel visual enjoyment for multimedia applications, such as in omnidirectional videos, 3D Free-viewpoint Television, 3D Television broadcasting, immersive teleconference, 3DoF and 6DoF VR.\n\n[0010] Preferably, processing the reference video file and the synthesized video file includes: segmenting the reference video file into a plurality of temporal layers; segmenting the synthesized video file into a plurality of temporal layers; and processing the temporal layers of the reference video file and. the temporal layers of the synthesized video file to identify flicker distortion in the synthesized video file.\n\n[0011] Preferably, processing the temporal layers of the reference video file and the temporal layers of the synthe- sized video file includes: processing the temporal layers of the reference video file to determine temporal gradient layers associated with the temporal layers of the reference video file; and/or processing the temporal layers of the synthesized video file to determine temporal gradient layers associated with the temporal layers of the synthesized video file.\n\n[0004] Conventional 2D quality metrics, such as the well- known Mean Squared Error (MSE) or Peak Signal-to-Noise Ratio (PSNR), are relatively simple and are commonly used in measuring the quality of image and video applications. However, these metrics are based on pixel-wise difference between the distorted images and the source images, which could not properly reflect the real perceptual quality of visual signal (video). In addition, these metrics are not based. on human perception of the visual signal (video).\n\n[0012] Preferably, processing the temporal layers of the reference video file and the temporal layers of the synthe- sized video file further includes: filtering the temporal gradient layers of the reference video file to remove gradient features with values below a threshold; and filtering the temporal gradient layers of the synthesized video file to remove gradient features with values below a threshold.\n\n[0013] Preferably, processing the reference video file synthesized video file further includes: processing reference depth video file associated with the reference video file for facilitating comparison of the reference video and the synthesized video file.\n\nSUMMARY OF THE INVENTION\n\n[0005] It is an object of the invention to address the above needs, to overcome or substantially ameliorate the above disadvantages or, more generally, to provide an improved system and method for determining a quality of a synthe- sized video file such as a synthesized view video file of 3D or VR video system.\n\n[0014] In one example, in the 3D and VR applications, there are multiple views of colour videos and corresponding depth videos to represent the 3D video. This is denoted as \u201cmulti-view video plus depth (MVD)\u201d. In these cases, the depth video file (or reference depth video file) is part of the 3D video file.\n\n[0006] In accordance with a first aspect of the invention, there is provided a computer-implemented method for deter- mining a quality of a synthesized video file. The method including: processing a reference video file and a synthe- sized video file associated with the reference video file to compare the reference video file and the synthesized video file; and determining an extent of flicker distortion of the synthesized video file based on the processing. The synthe- sized video file may be a 3D video file containing 3D video data, or it may be a virtual reality video file containing virtual reality video data.\n\n[0015] Preferably, processing the reference depth video includes: processing the reference depth video file detect edges of the reference depth video file to generate depth edge video file; and segmenting the depth edge video into a plurality of temporal depth layers.\n\n[0016] Preferably, processing the reference depth video further includes: processing the temporal depth layers expand the detected edge width in the temporal depth layers.\n\n[0017] Preferably, processing the reference video file and the synthesized video file further includes: processing the temporal gradient layers of the reference video file based on the temporal depth layers to obtain weighted temporal gradient layers associated with the reference video file; and processing the temporal gradient layers of the synthesized video file based on the temporal depth layers to obtain weighted temporal gradient layers associated with the syn- thesized video file.\n\n[0007] In one embodiment of the first aspect, the synthe- sized video at virtual viewpoint is rendered from the colour videos and depth videos of the neighboring real viewpoints, e.g., by using Depth Image based Rendering (DIBR). For example, the synthesized videos can be rendered from distorted colour/depth videos from neighboring real views (left and right) in the practical applications. Preferably, the synthesized video file is created by processing the video file containing the video at the real viewpoints.\n\n[0018] Preferably, processing the reference video file and the synthesized video file further includes: processing the weighted temporal gradient layers associated with the origi-\n\n[0008] Preferably, determining an extent of flicker distor- tion of the synthesized video file based on the comparison\n\n[0001]\n\nThe invention relates to\n\nvideo\n\ndetermi-\n\n[0003] Objective virtual viewpoint image (VVI) quality metrics for quantifying and determining (e.g., predicting) the quality of these videos is highly desirable. This is because based these metrics, visual processing tech- niques, e.g., such as image/video compression, digital water- marking, and image/video reconstruction in 3D video sys- tem can then be optimized accordingly improve the quality of experience of 3D or VR systems.\n\non\n\nto\n\nand\n\nthe\n\na\n\nfile\n\nfile\n\nto\n\na\n\nfile\n\nfile\n\nto\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n2\n\nnal video file and the weighted temporal gradient layers\n\n[ 0029 ]\n\nIn accordance with a third aspect of the invention ,\n\nassociated with the synthesized video file using sparse\n\nthere is provided a non - transitory computer readable\n\nrepresentation processing techniques to determine flicker\n\nmedium for storing computer instructions that , when\n\ndistortion in each of the weighted temporal gradient layers\n\nexecuted by one or more processors , causes the one or more\n\nof the synthesized video file .\n\nprocessors to perform a method of the first aspect .\n\n[ 0019 ]\n\nPreferably , processing the reference video file and\n\nthe synthesized video file further includes : processing the\n\nBRIEF DESCRIPTION OF THE DRAWINGS\n\nweighted temporal gradient layers associated with the origi\n\n[ 0030 ) Embodiments of the invention will now be\n\nnal video file and the weighted temporal gradient layers\n\ndescribed , by way of example , with reference to the accom\n\nassociated with the synthesized video file using sparse\n\npanying drawings in which :\n\nrepresentation processing techniques to determine phase\n\n[ 0031 ] FIG . 1 is a block diagram of a method for deter\n\ndistortion and amplitude distortion associated with flicker\n\nmining a quality of a synthesized video file in one embodi\n\ndistortion in each of the weighted temporal gradient layers\n\nment of the invention ;\n\nof the synthesized video file .\n\n[ 0032 ] FIG . 2A shows a video frame in an original video\n\n[ 0020 ]\n\nPreferably , processing the reference video file and\n\nentitled \u201c Undodancer \u201d , its zoom - in view , and an intensity\n\nthe synthesized video file further includes : weighting the\n\nplot along the line in the video frame ;\n\nphase distortion and the amplitude distortion respectively\n\n[ 0033 ] FIG . 2B shows a video frame in a synthesized\n\nassociated with flicker distortion in each of the weighted\n\nvideo corresponding to the original video of FIG . 2A , its\n\ntemporal gradient layers of the synthesized video file .\n\nzoom - in view , and an intensity plot along the line in the\n\n[ 0021 ]\n\nPreferably , the method further includes determin\n\nvideo frame ;\n\ning an extent of spatial - temporal activity distortion of the\n\n[ 0034 ] FIG . 3 is a schematic flowchart showing a sparse\n\nsynthesized video file .\n\nrepresentation based flicker distortion measurement method\n\n[ 0022 ]\n\nPreferably , determining an extent of spatial - tem\n\nin one embodiment of the invention ;\n\nporal activity distortion of the synthesized video file\n\n[ 0035 ] FIG . 4 is a schematic flowchart illustrating visual\n\nincludes : determining respective extents of spatial - temporal\n\neffects of flicker distortion in temporal layer in the method\n\nactivity distortion for each temporal frame of the synthe\n\nof FIG . 3 ;\n\nsized video file ; and determining an overall extent of spatial\n\n[ 0036 ] FIG . 5A is a video frame synthesized with original\n\ntemporal activity distortion of the synthesized video file\n\nvideo and depth video ;\n\nbased on the respective extents of spatial - temporal activity\n\n[ 0037 ] FIG . 5B is a temporal layer generated by the\n\ndistortion .\n\nhorizontal line in FIG . 5A ;\n\n[ 0023 ]\n\nPreferably , determining an extent of spatial - tem\n\n[ 0038 ] FIG . 5C is a gradient map ( with both horizontal and\n\nporal activity distortion of the synthesized video file further\n\nvertical gradients ) corresponding to the temporal layer of\n\nincludes : weighting the respective extents of spatial - tempo\n\nFIG . 5B ;\n\nral activity distortion for each temporal frame of the syn\n\n[ 0039 ] FIG . 5D is a gradient map ( with horizontal gradient\n\nthesized video file to determine the overall extent of spatial\n\nonly ) corresponding to the temporal layer of FIG . 5B ;\n\ntemporal activity distortion .\n\n[ 0040 ] FIG . 5E is a gradient map ( with vertical gradient\n\n[ 0024 ]\n\nPreferably , the method further includes determin\n\nonly ) corresponding to the temporal layer of FIG . 5B ;\n\ning a quality of the synthesized video file based on the\n\n[ 0041 ] FIG . 6A is a video frame of an original video ;\n\ndetermined extent of flicker distortion and the determined\n\n[ 0042 ] FIG . 6B is temporal layer generated by the hori\n\nextent of spatial - temporal activity distortion .\n\nzontal line in FIG . 6A , along with zoomed - in views of the\n\n[ 0025 ]\n\nIn accordance with a second aspect of the inven\n\nrectangles marked on the temporal layer ;\n\ntion , there is provided a system for determining a quality of\n\n[ 0043 ] FIG . 6C is a gradient feature map corresponding to\n\na synthesized video file . The system includes one or more\n\nthe temporal layer of FIG . 6B , along with zoomed - in views\n\nprocessors arranged to perform any of the methods of the\n\nof the rectangles marked on the temporal layer ;\n\nfirst aspect . In particular , the one or more processors are\n\narranged to : process a reference video file and a synthesized\n\n[ 0044 ] FIG . 6D is a video frame of a synthesized video\n\ncorresponding to the original video of FIG . 6A ,\n\nvideo file associated with the reference video file to compare\n\n[ 0045 ] FIG . 6E is temporal layer generated by the hori\n\nthe reference video file and the synthesized video file ; and\n\ndetermine an extent of flicker distortion of the synthesized\n\nzontal line in FIG . 6D , along with zoomed - in views of the\n\nvideo file based on the processing . The synthesized video\n\nrectangles marked on the temporal layer ;\n\nfile may be a 3D video file containing 3D video data , or it\n\n[ 0046 ] FIG . 6F is a gradient feature map corresponding to\n\nmay be a virtual reality video file containing virtual reality\n\nthe temporal layer of FIG . 6E , along with zoomed - in views\n\nof the rectangles marked on the temporal layer ;\n\nvideo data .\n\n[ 0026 ]\n\nPreferably , the system further includes a display\n\n[ 0047 ] FIG . 7A shows a learned spatial dictionary that can\n\noperably connected with the one or more processors for\n\nbe used in the method of FIG . 3 ;\n\ndisplaying the determined extent of flicker distortion .\n\n[ 0048 ] FIG . 7B shows a learned temporal dictionary that\n\n[ 0027 ]\n\nPreferably , the one or more processors are further\n\ncan be used in the method of FIG . 3 ;\n\narranged to determine an extent of spatial - temporal activity\n\n[ 0049 ] FIG . 8 is a schematic flowchart showing a sparse\n\ndistortion of the synthesized video file .\n\nrepresentation based 3D view quality assessment ( SR\n\n3DVQA ) method in one embodiment of the invention ;\n\n[ 0028 ]\n\nPreferably , the one or more processors are further\n\narranged to : determine a quality of the synthesized video file\n\n[ 0050 ]\n\nFIG . 9 is a schematic illustrating a spatio - temporal\n\ntube and Group of Pictures ( GoP ) quality assessment\n\nbased on the determined extent of flicker distortion and the\n\ndetermined extent of spatial - temporal activity distortion .\n\nmethod in one embodiment of the invention ;\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\n[0029] In accordance with a third aspect of the invention, there is provided a non-transitory computer readable medium for storing computer instructions that, when executed by one or more processors, causes the one or more processors to perform a method of the first aspect.\n\nvideo file and the weighted temporal gradient layers associated with the synthesized video file using sparse representation processing techniques to determine flicker distortion in each of the weighted temporal gradient layers of the synthesized video file.\n\n[0019] Preferably, processing the reference video file and the synthesized video file further includes: processing the weighted temporal gradient layers associated with the origi- nal video file and the weighted temporal gradient layers associated with the synthesized video file using sparse representation processing techniques to determine phase distortion and amplitude distortion associated with flicker distortion in each of the weighted temporal gradient layers of the synthesized video file.\n\nBRIEF DESCRIPTION OF THE DRAWINGS\n\n[0030] Embodiments of the invention will now be described, by way of example, with reference to the accom- panying drawings in which:\n\n[0031] FIG. 1 is a block diagram of a method for deter- mining a quality of a synthesized video file in one embodi- of the invention;\n\n[0032] FIG. 2A shows a video frame in an original video entitled \u201cUndodancer\u201d, its zoom-in view, and an intensity along the line in the video frame;\n\n[0020] Preferably, processing the reference video file and the synthesized video file further includes: weighting the phase distortion and the amplitude distortion respectively associated with flicker distortion in each of the weighted temporal gradient layers of the synthesized video file.\n\n[0033] FIG. 2B shows a video frame in a synthesize video corresponding to the original video of FIG. 2A, its zoom-in view, and an intensity plot along the line in the video frame;\n\n[0021] Preferably, the method further includes determin- an extent of spatial-temporal activity distortion of synthesized video file.\n\n[0034] FIG. 3 is a schematic flowchart showing a sparse representation based flicker distortion measurement method one embodiment of the invention;\n\n[0022] Preferably, determining an extent of spatial-tem- poral activity distortion of the synthesized video file includes: determining respective extents of spatial-temporal activity distortion for each temporal frame of the synthe- sized video file; and determining an overall extent of spatial- temporal activity distortion of the synthesized video file based on the respective extents of spatial-temporal activity distortion.\n\n[0035] FIG. 4 is a schematic flowchart illustrating visual effects of flicker distortion in temporal layer in the method FIG. 3;\n\n[0036] FIG. 5A is a video frame synthesized with origina\u2019 video and depth video;\n\n[0037] FIG. 5B is a temporal layer generated by horizontal line in FIG. 5A;\n\n[0023] Preferably, determining an extent of spatial-tem- poral activity distortion of the synthesized video file further includes: weighting the respective extents of spatial-tempo- ral activity distortion for each temporal frame of the syn- thesized video file to determine the overall extent of spatial- temporal activity distortion.\n\n[0038] FIG. 5C is a gradient map (with both horizontal an vertical gradients) corresponding to the temporal layer FIG. 5B;\n\n[0039] FIG. 5D is a gradient map (with horizontal gradient corresponding to the temporal layer of FIG. 5B;\n\n[0040] FIG. SE is a gradient map (with vertical gradient corresponding to the temporal layer of FIG. 5B;\n\n[0024] Preferably, the method further includes determin- a quality of the synthesized video file based on the determined extent of flicker distortion and the determined extent of spatial-temporal activity distortion.\n\n[0041] FIG. 6A is a video frame of an original video;\n\n[0042] FIG. 6B is temporal layer generated by the hori- zontal line in FIG. 6A, along with zoomed-in views of the rectangles marked on the temporal layer;\n\n[0025] In accordance with a second aspect of the inven- tion, there is provided a system for determining a quality of a synthesized video file. The system includes one or more processors arranged to perform any of the methods of the first aspect. In particular, the one or more processors are arranged to: process a reference video file and a synthesized video file associated with the reference video file to compare the reference video file and the synthesized video file; and determine an extent of flicker distortion of the synthesized video file based on the processing. The synthesized video file may be a 3D video file containing 3D video data, or it may be a virtual reality video file containing virtual reality video data.\n\n[0043] FIG. 6C is a gradient feature map corresponding temporal layer of FIG. 6B, along with zoomed-in views the rectangles marked on the temporal layer;\n\n[0044] FIG. 6D is a video frame of a synthesized video corresponding to the original video of FIG. 6A,\n\n[0045] FIG. 6E is temporal layer generated by the hori- zontal line in FIG. 6D, along with zoomed-in views of rectangles marked on the temporal layer;\n\n[0046] FIG. 6F is a gradient feature map corresponding temporal layer of FIG. 6E, along with zoomed-in views the rectangles marked on the temporal layer;\n\n[0026] Preferably, the system further includes a display operably connected with the one or more processors for displaying the determined extent of flicker distortion.\n\n[0047] FIG. 7A shows a learned spatial dictionary that used in the method of FIG. 3;\n\n[0048] FIG. 7B shows a learned temporal dictionary be used in the method of FIG. 3;\n\n[0027] Preferably, the one or more processors are further arranged to: determine an extent of spatial-temporal activity distortion of the synthesized video file.\n\n[0049] FIG. 8 is a schematic flowchart showing a sparse representation based 3D view quality assessment (SR- 3DVQA) method in one embodiment of the invention;\n\n[0028] Preferably, the one or more processors are further arranged to: determine a quality of the synthesized video file based on the determined extent of flicker distortion and the determined extent of spatial-temporal activity distortion.\n\n[0050] FIG. 9 is a schematic illustrating a spatio-temporal tube and Group of Pictures (GoP) quality assessment method in one embodiment of the invention;\n\nnal\n\ning\n\nthe\n\ning\n\nment\n\nplot\n\nin\n\nof\n\nthe\n\no\n\nonly)\n\nonly)\n\nto\n\nthe of\n\nthe\n\nto\n\nthe of\n\ncan\n\nbe\n\nthat\n\ncan\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n3\n\n[ 0079 ] FIG . 15 is a table showing statistical significance\n\n[ 0051 ] FIG . 10A is a video frame of a synthesized video\n\ntest results including variance ratios between existing bench\n\nentitled \u201c Balloons \" ; FIG . 10B is a depth map of the video\n\nmark methods and the SR - 3DVQA method of FIG . 8 on the\n\nframe of FIG . 10A ;\n\n[ 0052 ] FIG . 10C is an edge map of the depth map of FIG .\n\nfour datasets ;\n\n[ 0080 ] FIG . 16 is a table showing performance metrics of\n\n10B with canny threshold 0.03 ;\n\nthe SR - 3DVQA method ( in particular the pooling methods\n\n[ 0053 ] 25\n\nof flicker distortion measurement and spatial - temporal activ\n\n[ 0054 ] FIG . 10D is an edge map of the depth map of FIG .\n\nity distortion measurement ) of FIG . 8 ;\n\n10B with canny threshold 0.07 ;\n\n[ 0081 ] FIG . 17 is a table showing performance metrics of\n\n[ 0055 ] FIG . 10E is an edge map of the depth map of FIG .\n\nthe SR - 3DVQA method ( in particular the impacts from the\n\n10B with canny threshold 0.1 ;\n\nreference depth video ) of FIG . 8 ; and\n\n[ 0056 ] FIG . 10F is an edge map of the depth map of FIG .\n\n[ 0082 ] FIG . 18 is a block diagram of a system for deter\n\n10B with canny threshold 0.2 ;\n\nmining a quality of a synthesized video file in one embodi\n\n[ 0057 ] FIG . 10G is an edge map of the depth map of FIG .\n\nment of the invention .\n\n10B with canny threshold 0.3 ;\n\n[ 0058 ] FIG . 10H is an edge map of the depth map of FIG .\n\nDETAILED DESCRIPTION OF THE\n\n10B with canny threshold 0.4 ;\n\nPREFERRED EMBODIMENT\n\n[ 0059 ] FIG . 101 is an edge map of the depth map of FIG .\n\n[ 0083 ] Temporal flicker distortion is one of the most\n\n10B with canny threshold 0.5 ;\n\nannoying noises in synthesized virtual view videos when\n\n[ 0060 ] FIG . 11A is a video frame of a synthesized video\n\nthey are rendered by compressed multi - view video plus\n\nentitled \u201c Lovebirds \u201d ; FIG . 11B is a depth map of the video\n\ndepth in 3D or VR video system . The inventors of the\n\nframe of FIG . 11A ;\n\npresent application have realized the need of an objective\n\n[ 0061 ] FIG . 11C is an edge map of the depth map of FIG .\n\nvideo quality assessment which can accurately measure the\n\n11B with canny threshold 0.03 ;\n\nflicker distortion to assess the synthesized view video qual\n\n[ 0062 ] FIG . 11D is an edge map of the depth map of FIG .\n\nity and further optimize the compression techniques in 3D or\n\n11B with canny threshold 0.07 ;\n\nVR video system is highly needed .\n\n[ 0063 ] FIG . 11E is an edge map of the depth map of FIG .\n\n[ 0084 ] FIG . 1 shows a method 100 for determining a\n\n11B with canny threshold 0.1 ;\n\nquality of a synthesized video file . The method includes , in\n\n[ 0064 ] FIG . 11F is an edge map of the depth map of FIG .\n\nstep 102 , processing a reference video file and a synthesized\n\n11B with canny threshold 0.2 ;\n\nvideo file to compare the reference video file and the\n\n[ 0065 ] FIG . 116 is an edge map of the depth map of FIG .\n\nsynthesized video file . Here the synthesized video file is\n\n11B with canny threshold 0.3 ;\n\nassociated with the reference video file . Then , in step 104A ,\n\n[ 0066 ] FIG . 11H is an edge map of the depth map of FIG .\n\nan extent of flicker distortion of the processed video file is\n\n11B with canny threshold 0.4 ;\n\ndetermined based on the processing . Additionally or alter\n\nnatively , in step 104B , an extent of spatial - temporal activity\n\n[ 0067 ] FIG . 111 is an edge map of the depth map of FIG .\n\n11B with canny threshold 0.5 ;\n\ndistortion of the synthesized video file is determined based\n\non the processing . Finally , in step 106 , a quality of the\n\n[ 0068 ] FIG . 12A is a video frame of a synthesized video\n\nsynthesized video file is determined based on one or both of\n\nentitled \u201c Undodancer \u201d ;\n\nthe extent of flicker distortion of the processed video file and\n\n[ 0069 ] FIG . 12B is a depth map of the video frame of FIG .\n\nthe extent of spatial - temporal activity distortion of the\n\n12A ;\n\nsynthesized video file . In this embodiment , the synthesized\n\n[ 0070 ] FIG . 12C is an edge map of the depth map of FIG .\n\nvideo file is a 3D video file containing 3D video data , or a\n\n12B with canny threshold 0.03 ;\n\nvirtual reality video file containing virtual reality video data .\n\n[ 0071 ] FIG . 12D is an edge map of the depth map of FIG .\n\n[ 0085 ]\n\nIn some embodiments , step 102 includes : segment\n\n12B with canny threshold 0.07 ;\n\ning the reference video file into a plurality of temporal\n\n[ 0072 ] FIG . 12E is an edge map of the depth map of FIG .\n\nlayers ; segmenting the synthesized video file into a plurality\n\n12B with canny threshold 0.1 ;\n\nof temporal layers , and processing the temporal layers of the\n\n[ 0073 ] FIG . 12F is an edge map of the depth map of FIG .\n\nreference video file and the temporal layers of the synthe\n\n12B with canny threshold 0.2 ;\n\nsized video file to identify flicker distortion in the synthe\n\n[ 0074 ] FIG . 12G is an edge map of the depth map of FIG .\n\nsized video file . Processing the temporal layers of the\n\n12B with canny threshold 0.3 ;\n\nreference video file and the temporal layers of the synthe\n\n[ 0075 ] FIG . 12H is an edge map of the depth map of FIG .\n\nsized video file may include : processing the temporal layers\n\n12B with canny threshold 0.4 ;\n\nof the reference video file to determine temporal gradient\n\n[ 0076 ] FIG . 121 is an edge map of the depth map of FIG .\n\nlayers associated with the temporal layers of the reference\n\n12B with canny threshold 0.5 ;\n\nvideo file , and / or processing the temporal layers of the\n\n[ 0077 ] FIG . 13 is a table showing properties of the training\n\nsynthesized video file to determine temporal gradient layers\n\nsequences for different sample videos entitled \u201c Basketball\n\nassociated with the temporal layers of the synthesized video\n\nDrive \u201d , \u201c FourPeople \u201d , \u201c Flowervase \u201d ,\n\n\u201c Johnny \u201d , \u201c Kriste\n\nfile . Processing the temporal layers of the reference video\n\nfile and the temporal layers of the synthesized video file may\n\nnAndSara \u201d , \u201c ParkScene \u201d , \u201c RaceHorses \u201d , and \u201c Vidyo3 \u201d ;\n\n[ 0078 ] FIG . 14 a table showing the performance compari\n\nfurther include : filtering the temporal gradient layers of the\n\nbetween existing benchmark methods and the\n\nreference video file to remove gradient features with values\n\nson\n\nbelow a threshold ; and filtering the temporal gradient layers\n\nSR - 3DVQA method of FIG . 8 on the SIAT database , which\n\nof the synthesized video file to remove gradient features\n\nincludes three subsets U CD , CUD , C / CD , a combination of\n\nwith values below a threshold . Step 102 may further include :\n\nthe three subsets ( ALL dataset ) ;\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021\n\n[0051] FIG. 10A is a video frame of a synthesized video entitled \u201cBalloons\u201d; FIG. 10B is a depth map of the video frame of FIG. 10A;\n\n[0079] FIG. 15 is a table showing statistical significance test results including variance ratios between existing bench- mark methods and the SR-3DVQA method of FIG. 8 on the four datasets;\n\n[0052] FIG. 10C is an edge map of the depth map of FIG. 10B with canny threshold 0.03;\n\n[0080] FIG. 16 is a table showing performance metrics SR-3DVQA method (in particular the pooling methods flicker distortion measurement and spatial-temporal activ- distortion measurement) of FIG. 8;\n\n[0053] 25\n\n[0054] FIG. 10D is an edge map of the depth map of 10B with canny threshold 0.07; \u53ef\n\n[0081] FIG. 17 is a table showing performance metrics SR-3DVQA method (in particular the impacts from reference depth video) of FIG. 8; and\n\n[0055] FIG. 10E is an edge map of the depth map of with canny threshold 0.1; \u53ef\n\n[0056] FIG. 10F is an edge map of the depth map of F with canny threshold 0.2;\n\n[0082] FIG. 18 is a block diagram of a system for deter- mining a quality of a synthesized video file in one embodi- ment of the invention.\n\n[0057] FIG. 10G is an edge map of the depth map of 10B with canny threshold 0.3; \u53ef\n\n[0058] FIG. 10H is an edge map of the depth map of with canny threshold 0.4; 4\n\nDETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT\n\n[0059] FIG. 101 is an edge map of the depth map of F 10B with canny threshold 0.5;\n\n[0083] Temporal flicker distortion is one of the most annoying noises in synthesized virtual view videos when they are rendered by compressed multi-view video plus depth in 3D or VR video system. The inventors of the present application have realized the need of an objective video quality assessment which can accurately measure the flicker distortion to assess the synthesized view video qual- ity and further optimize the compression techniques in 3D or VR video system is highly needed.\n\n[0060] FIG. 11A is a video frame of a synthesized video entitled \u201cLovebirds\u201d; FIG. 11B is a depth map of the video frame of FIG. 11A;\n\n[0061] FIG. 11C is an edge map of the depth map of F with canny threshold 0.03;\n\n[0062] FIG. 11D is an edge map of the depth map of F 11B with canny threshold 0.07;\n\n[0063] FIG. 11E is an edge map of the depth map of with canny threshold 0.1; \u672c\n\n[0084] FIG. 1 shows a method 100 for determining a quality of a synthesized video file. The method includes, in step 102, processing a reference video file and a synthesized video file to compare the reference video file and the synthesized video file. Here the synthesized video file is associated with the reference video file. Then, in step 104A, an extent of flicker distortion of the processed video file is determined based on the processing. Additionally or alter- natively, in step 104B, an extent of spatial-temporal activity distortion of the synthesized video file is determined based. on the processing. Finally, in step 106, a quality of the synthesized video file is determined based on one or both of the extent of flicker distortion of the processed video file and the extent of spatial-temporal activity distortion of the synthesized video file. In this embodiment, the synthesized video file is a 3D video file containing 3D video data, or a virtual video file virtual video data.\n\n[0064] FIG. 11F is an edge map of the depth map of F with canny threshold 0.2;\n\n[0065] FIG. 11G is an edge map of the depth map of 11B with canny threshold 0.3; 4\n\n[0066] FIG. 11H is an edge map of the depth map of 11B with canny threshold 0.4; \u53ef\n\n[0067] FIG. 111 is an edge map of the depth map of with canny threshold 0.5; \u53ef\n\n[0068] FIG. 12A is a video frame of a synthesized entitled \u201cUndodancer\u201d;\n\n[0069] FIG. 12B is a depth map of the video frame of 12A; \u53ef\n\n[0070] FIG. 12C is an edge map of the depth map of F 12B with canny threshold 0.03;\n\n[0071] FIG. 12D is an edge map of the depth map of with canny threshold 0.07; \u53ef\n\ning the reference video file into a plurality of temporal layers; segmenting the synthesized video file into a plurality of temporal layers, and processing the temporal layers of the reference video file and the temporal layers of the synthe- sized video file to identify flicker distortion in the synthe- sized video file. Processing the temporal layers of the reference video file and the temporal layers of the synthe- sized video file may include: processing the temporal layers of the reference video file to determine temporal gradient layers associated with the temporal layers of the reference video file, and/or processing the temporal layers of the synthesized video file to determine temporal gradient layers associated with the temporal layers of the synthesized video file. Processing the temporal layers of the reference video file and the temporal layers of the synthesized video file may further include: filtering the temporal gradient layers of the reference video file to remove gradient features with values below a threshold; and filtering the temporal gradient layers of the synthesized video file to remove gradient features\n\n[0072] FIG. 12E is an edge map of the depth map of F 12B with canny threshold 0.1;\n\n[0073] FIG. 12F is an edge map of the depth map of 12B with canny threshold 0.2; \u53ef\n\n[0074] FIG. 12G is an edge map of the depth map of F with canny threshold 0.3;\n\n[0075] FIG. 12H is an edge map of the depth map of F with canny threshold 0.4;\n\n[0076] FIG. 121 is an edge map of the depth map of 12B with canny threshold 0.5; 4\n\n[0077] FIG. 13 is a table showing properties of the training sequences for different sample videos entitled \u201cBasketball- Drive\u201d, \u201cFourPeople\u201d, \u201cFlowervase\u201d, \u201cJohnny\u201d, \u201cKriste- nAndSara\u201d, \u201cParkScene\u201d, \u201cRaceHorses\u201d, and \u201cVidyo3\u201d;\n\n[0078] FIG. 14 a table showing the performance compari- son between existing benchmark methods and the SR-3DVQA method of FIG. 8 on the SIAT database, which includes three subsets U;Cp, C7U,, C,Cp, a combination of three subsets (ALL dataset);\n\n10B\n\n10B\n\n10B\n\n11B\n\n11B\n\n11B\n\n11B\n\nvideo\n\n12B\n\n12B\n\n12B\n\nthe\n\nof\n\nthe\n\nof ity\n\nof the\n\nthe\n\nreality containing reality [0085] In some embodiments, step 102 includes: segment-\n\nwith values below a threshold. Step 102 may further include:\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n4\n\nprocessing a reference depth video file associated with the\n\nTABLE I - continued\n\nreference video file for facilitating comparison of the refer\n\nence video file and the synthesized video file . Processing the\n\nDefinitions of Key Symbols or Variables\n\nreference depth video file may include : processing the\n\nDescriptions\n\nVariables\n\nreference depth video file to detect edges of the reference\n\ndepth video file to generate a depth edge video file ; and\n\nThe i - th gradient temporal layer of the video V\n\nG ;\n\nsegmenting the depth edge video file into a plurality of\n\nThe original and distorted gradient video\n\n{ G. , i } , { Gd , i }\n\ntemporal depth layers . Optionally , the temporal depth layers\n\ndenoted by temporal layers set , respectively\n\nare processed to expand the detected edge width in the\n\nThe effective patch index set in the i - th\n\nSv , i\n\ngradient temporal layer of video V , and Va\n\ntemporal depth layers . Step 102 may further include : pro\n\nThe pixel , edge , dilated edge pixel value of\n\nD ( x , y , t ) ,\n\ncessing the temporal gradient layers of the reference video\n\nDedge ( x , y , t ) ,\n\n( x , y , t ) of depth video D , respectively\n\nfile based on the temporal depth layers to obtain weighted\n\nDedge ' ( x , y , t )\n\ntemporal gradient layers associated with the reference video\n\nThe k - th edge frame , dilated edge frame of\n\nE. , k , E'o , k\n\nfile ; and processing the temporal gradient layers of the\n\ndepth video D\n\nsynthesized video file based on the temporal depth layers to\n\nThe i - th edge temporal layer in depth video D\n\nMo , i\n\nobtain weighted temporal gradient layers associated with the\n\nThe effective patch index set in the i - th edge\n\nSe , i\n\ntemporal layer of depth video D\n\nsynthesized video file . Step 102 may further include : pro\n\nThe flicker distortion area patch index set in\n\ncessing the weighted temporal gradient layers associated\n\nS ;\n\nthe i - th temporal layer of video V , and Va\n\nwith the reference video file and the weighted temporal\n\nThe patch at ( u , v ) in the i - th temporal\n\no , i\n\nYu v\n\ngradient layers associated with the synthesized video file\n\nlayer of video V , and Vd , respectively\n\nd . i\n\nusing sparse representation processing techniques to deter\n\nYu , v\n\nThe weight map of the temporal layer of\n\nmine flicker distortion in each of the weighted temporal\n\na synthesized video\n\ngradient layers of the synthesized video file . Step 102 may\n\nfurther include : further includes : processing the weighted\n\ntemporal gradient layers associated with the reference video\n\n[ 0089 ] For a synthesized view or video generated via\n\nfile and the weighted temporal gradient layers associated\n\ndepth - image - based rendering ( DIBR ) technology in Multi\n\nwith the synthesized video file using sparse representation\n\nview Video plus Depth ( MVD ) based 3D video system , the\n\nprocessing techniques to determine phase distortion and\n\nrendering process usually consists of two steps . The first step\n\namplitude distortion associated with flicker distortion in\n\nis warping pixels from existing neighboring views to another\n\neach of the weighted temporal gradient layers of the syn\n\nthesized video file . The phase distortion and the amplitude\n\nnew virtual view via DIBR . Due to dis - occlusion and\n\ndistortion respectively associated with flicker distortion may\n\nrounding operations in warping , small holes may exist in the\n\nbe weighted in each of the weighted temporal gradient layers\n\nrendered image . The second step is hole filling and post\n\nof the synthesized video file .\n\nprocessing , namely inpainting the dis - occluded areas or\n\n[ 0086 ]\n\nIn some embodiments , step 104A may include :\n\nholes from their surrounding pixels . However , due to imper\n\ndetermining respective extents of flicker distortion for each\n\nfect depth images and misalignment between color and\n\ntemporal frame of the synthesized video file , and determin\n\ndepth image , some distortions such as contour artifacts and\n\ning an overall extent of flicker distortion of the synthesized\n\ngeometrical distortions , may occur during the DIBR process .\n\nvideo file based on the respective extents of flicker distor\n\nIn addition , the compression distortion in color images will\n\ntion . Optionally , the respective extents of flicker distortion\n\nbe transferred to the rendered images , while the depth\n\nfor each temporal frame of the synthesized video file are\n\ndistortions will induce the displacement of pixels , i.e. ,\n\nweighted to determine the overall extent of flicker distortion .\n\ngeometrical distortion . The inconsistency between tempo\n\n[ 0087 ]\n\nIn some embodiments , step 104B includes : deter\n\nrally successive depth images caused by depth image gen\n\nmining respective extents of spatial - temporal activity dis\n\neration and compression may also induce inconsistent geo\n\ntortion for each temporal frame of the synthesized video file ;\n\nand determining an overall extent of spatial - temporal activ\n\nmetrical distortions among frames ( also known as temporal\n\nflicker distortion ) .\n\nity distortion of the synthesized video file based on the\n\nrespective extents of spatial - temporal activity distortion .\n\n[ 0090 ] FIGS . 2A and 2B illustrate an example of the\n\nOptionally , the respective extents of spatial - temporal activ\n\nflicker distortion along the rim of a pillar in the synthesized\n\nity distortion for each temporal frame of the synthesized\n\nvideo with depth compression distortion of Undodancer . It\n\nvideo file are weighted to determine the overall extent of\n\ncan be observed that the intensity of point A in the original\n\nspatial - temporal activity distortion .\n\nview ( FIG . 2A ) changes little and relatively smoothly in the\n\n[ 0088 ] One specific implementation of the method of FIG .\n\ntwenty frames . In contrast , the intensity of corresponding\n\n1 is presented below . Table I shows the definitions of key\n\npoint in synthesized view ( FIG . 2B ) , i.e. , point Asm in the\n\nsymbols or variables used in the following disclosure .\n\nsynthesized view , fluctuated drastically in temporal domain .\n\nThis would appear unnatural to the viewers , potentially\n\nTABLE I\n\nannoying them . Existing 2D and 3D view quality assessment\n\nDefinitions of Key Symbols or Variables\n\n( VQA ) metrics have not well considered the properties of\n\nthe synthesized virtual view videos and the flicker distortion .\n\nDescriptions\n\nVariables\n\nTherefore , there is a need for a more effective quality\n\nThe i - th temporal layer of the video V\n\nL ;\n\nassessment method for application in synthesized videos .\n\nTemporal layers set denotation of video V\n\n{ L ; ll sis H }\n\nLa } , { Lai\n\nThe original and distorted video V , and Va\n\n[ 0091 ]\n\nSparse representation can be used to represent\n\ndenoted by temporal layers set , respectively\n\nvisual features and receptive field . In this embodiment ,\n\nThe pixel value ( x , y , t ) in video V\n\nV ( x , y , t )\n\nsparse representation is used to represent temporal flickering\n\nThe gradient value of pixel ( x , y , t ) in video V\n\nV8 ( x , y , t )\n\nand to evaluate the 3D video quality .\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\nTABLE I-continued\n\nDefinitions of Key Symbols or Variables Variables Descriptions G, The i-th gradient temporal layer of the video V {G,,3.{Ga,4} The original and distorted gradient video denoted by temporal layers set, respectively Sri \u2018The effective patch index set in the i-th gradient temporal layer of video V, and Vy De y. 0, The pixel, edge, dilated edge pixel value of D(x, y, 1), {X, y, D of depth video D, respectively Dee\" (x, y, t) Baw E ee The k-th edge frame, dilated edge frame of depth video D Me The i-th edge temporal layer in depth video D Spi The effective patch index set in the i-th edge temporal layer of depth video D Si: The flicker distortion area patch index set in the i-th temporal layer of video V, and V, Yur The patch at (u, v) in the i-th temporal Yu?! layer of video V,, and Va, respectively w The weight map of the temporal layer of a synthesized video\n\nence video file and the synthesized video file. Processing the reference depth video file may include: processing the reference depth video file to detect edges of the reference depth video file to generate a depth edge video file; and segmenting the depth edge video file into a plurality o temporal depth layers. Optionally, the temporal depth layers are processed to expand the detected edge width in the temporal depth layers. Step 102 may further include: pro- cessing the temporal gradient layers of the reference video file based on the temporal depth layers to obtain weighted temporal gradient layers associated with the reference video file; and processing the temporal gradient layers of the synthesized video file based on the temporal depth layers to obtain weighted temporal gradient layers associated with the synthesized video file. Step 102 may further include: pro- cessing the weighted temporal gradient layers associated with the reference video file and the weighted temporal gradient layers associated with the synthesized video file using sparse representation processing techniques to deter- mine flicker distortion in each of the weighted temporal gradient layers of the synthesized video file. Step 102 may further include: further includes: processing the weighted temporal gradient layers associated with the reference video file and the weighted temporal gradient layers associated with the synthesized video file using sparse representation processing techniques to determine phase distortion and amplitude distortion associated with flicker distortion in each of the weighted temporal gradient layers of the syn- thesized video file. The phase distortion and the amplitude distortion respectively associated with flicker distortion may be weighted in each of the weighted temporal gradient layers\n\ndepth-image-based rendering (DIBR) technology in Multi- view Video plus Depth (MVD) based 3D video system, the rendering process usually consists of two steps. The first step is warping pixels from existing neighboring views to another new virtual view via DIBR. Due to dis-occlusion and rounding operations in warping, small holes may exist in the rendered image. The second step is hole filling and post- processing, namely inpainting the dis-occluded areas or holes from their surrounding pixels. However, due to imper- fect depth images and misalignment between color and depth image, some distortions such as contour artifacts and. geometrical distortions, may occur during the DIBR process. In addition, the compression distortion in color images will be transferred to the rendered images, while the depth distortions will induce the displacement of pixels, ie. geometrical distortion. The inconsistency between tempo- rally successive depth images caused by depth image gen- eration and compression may also induce inconsistent geo- metrical distortions among frames (also known as temporal\n\n[0086] In some embodiments, step 104A may include: determining respective extents of flicker distortion for each temporal frame of the synthesized video file, and determin- ing an overall extent of flicker distortion of the synthesized video file based on the respective extents of flicker distor- tion. Optionally, the respective extents of flicker distortion for each temporal frame of the synthesized video file are weighted to determine the overall extent of flicker distortion.\n\n[0087] In some embodiments, step 104B includes: deter- mining respective extents of spatial-temporal activity dis- tortion for each temporal frame of the synthesized video file; and determining an overall extent of spatial-temporal activ- ity distortion of the synthesized video file based on the respective extents of spatial-temporal activity distortion. Optionally, the respective extents of spatial-temporal activ- ity distortion for each temporal frame of the synthesized video file are weighted to determine the overall extent of spatial-temporal activity distortion.\n\n[0090] an example flicker distortion along the rim of a pillar in the synthesized video with depth compression distortion of Undodancer. It can be observed that the intensity of point A in the original view (FIG. 2A) changes little and relatively smoothly in the twenty frames. In contrast, the intensity of corresponding point in synthesized view (FIG. 2B), ie., point A,,,, in the synthesized view, fluctuated drastically in temporal domain. This would appear unnatural to the viewers, potentially annoying them. Existing 2D and 3D view quality assessment (VQA) metrics have not well considered the properties of the synthesized virtual view videos and the flicker distortion. Therefore, there is a need for a more effective quality assessment method for application in synthesized videos.\n\n[0088] One specific implementation of the method of FIG. is presented below. Table I shows the definitions of key symbols or variables used in the following disclosure.\n\nTABLE I\n\nDefinitions of Key Symbols or Variables Variables Descriptions \u4e86 The i-th temporal layer of the video V {Lill sis H} Tempormal layers set denotation of video V {Ls, {Lai} \u2018The original and distorted video V, and Vy denoted by temporal layers set, respectively Vix, y \u65e5 \u2018The pixel value (x, yb in video V VE (x, y, 1) \u2018The gradient value of pixel (x, y, t) in video V\n\n[0091] Sparse representation can be used to represent visual features and receptive field. In this embodiment, sparse representation is used to represent temporal flickering and to evaluate the 3D video quality.\n\nProcessing a reference depth video file associated with the reference video file for facilitating comparison of the refer-\n\nof the synthesized video file.\n\n1\n\n[0089]\n\nFor\n\na\n\nsynthesized view\n\nor\n\nvideo generated via\n\nflicker distortion).\n\nFIGS.\n\n2A and 2B\n\nillustrate\n\nof the\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n5\n\n[ 0092 ]\n\nConventional sparse representation and dictionary\n\ntion A ) , respectively . Then , each layer will be transformed to\n\ngradient feature map after gradient feature extraction ( sub\n\nlearning based on 2D spatial patches were used to represent\n\nthe 2D image features . Currently , 3D dictionary learning ,\n\nsection B ) . Afterwards , for each layer , the location of\n\npossible flicker distortion is identified through flicker dis\n\nwhich further include the temporal or depth dimension , has\n\nbeen employed in several video applications , such as online\n\ntortion detection module with the help of the associated\n\ndepth video ( subsection C ) . Subsequently , flicker distortion\n\nsequence de - noising in M. Protter and M. Elad ,\n\n\u201c Image\n\nSequence Denoising via Sparse and Redundant Represen\n\nstrengths are measured through the sparse coefficients fea\n\ntations , \u201d IEEE Trans . Image Process . , vol . 18 , no . 1 , pp .\n\ntures at the identified flicker distortion location , and the\n\nsparse representation is based on the learned temporary\n\n27-35 , January 2009 , video super - resolution in H. Xiang , Z.\n\ndictionary through training stage ( subsection D ) . Finally , the\n\nPan , X. Ye and C. W. Chen , \u201c Sparse spatio - temporal rep\n\nresentation with adaptive regularized dictionary learning\n\noverall flicker distortion score is obtained by weighted layer\n\npooling ( subsection E ) .\n\nfor Low bitrate video coding , \u201d IEEE Trans . Circuits Syst .\n\nVideo Technol . , vol . 23 , no . 4 , pp . 710-728 , April 2013 , and\n\nhuman action identification in S. C. W. Tim , M. Rombaut\n\nA. Temporal Layer Conversion\n\nand D. Pellerin , \u201c Dictionary of gray - level 3D patches for\n\naction recognition , \u201d 2014 IEEE Int . Workshop Mach . Learn .\n\n[ 0096 ] A video can generally be considered as 3D volu\n\nSignal Process . ( MLSP ) , September 2014 , pp . 1-6 . The 3D\n\nmetric data V = { V ( x , y , t ) 1 < x < W , 1 < y < H ,\n\n1 < t < T }\n\ndictionary learning objective function can be formulated as\n\nwhere H , W , T represent video height ( Y ) , width ( X ) , and\n\nframe length ( T ) , respectively . By dividing the 3D - XYT\n\nvideo into multiple 2D - XT layers , the video could be\n\n3D 3D || 3D -Y3D 230 | + fella ? ||| + 4p ( x3D )\n\nredefined as V = { L ; < i < H } , where L = { V ( x , y , t ) 1\n\nx\n\n( 1 )\n\nmin\n\n< W , y = i , 1 < t < T } is the i - th temporal layer , and the\n\nheight H is also the total amount of temporal layers , i.e. ,\n\n2D - XT planes . For VQA of synthesized video , there are\n\nwhere X3D = [ x1 , x2 , ... , Xi ,\n\nX ] denotes the 3D training\n\nthree main advantages provided by segmenting a video into\n\npatches set from video data . X ; ER\u00ae is a 3D volumetric data ,\n\ntemporal layers : ( 1 ) visuality : it helps visualize the temporal\n\nconsisting of horizontal x , vertical y , and temporal t dimen\n\nfeatures and can provide an explicit and intuitional cues ; ( 2 )\n\nsions , which is denoted as 3D - XYT .\n\ncapability : the temporal layer picture , a surface formed by\n\n[ 0093 ] 3D is the learned 3D dictionary . a3D is a simpli\n\nspace lines varying with time , can be used to present the\n\nfied form of the sparse coefficients of the overall 3D patches .\n\nlong - term temporal features of the video ; ( 2 ) simplicity : it\n\nII - II , refers to the L , norm of the vector . || 0 ; 30 || denotes that\n\navoids employing motion estimation method to match the\n\nthe sparse coefficient a ;\n\nof patch i should satisfy the\n\n3D\n\npatch between t - i - th frame and t - th frame to capture the\n\nsparsity constraint , where u regulates the sparsity . P ( X3D )\n\nmotion features .\n\nrepresents the task - driven constraint , and \u00e0 regulates the\n\n[ 0097 ]\n\nTherefore , to assess the flicker distortion more\n\nweight of this term . The advantage of 3D sparse represen\n\neffectively , this embodiment converts the distorted 3D syn\n\ntation is that it can learn 3D dictionaries for better repre\n\nthesized video V into temporal layers { L ; } , i.e. , 2D - XT\n\nsentation ability . However , the computational complexity of\n\nplane , as shown in the left part of the FIG . 4. It can be\n\nlearning 3D dictionaries increases dramatically . One alter\n\nobserved that the intense movement of the human and the\n\nnative solution is degrading 3D dictionary learning and\n\nvariable motion of one point along the rim of the pillar are\n\napproximating it with multi - layer 2D dictionaries . In fact ,\n\nrepresented\n\na drastically twisted stripe and a smooth\n\n2D sparse representation for 2D image ( i.e. , 2D - XY data )\n\ncurve line respectively . This illustrates that the temporal\n\ncan be regarded as a special case or degradation of 3D sparse\n\nlayer can capture the temporal features . In addition , the\n\nrepresentation ( i.e. , 3D - XYT data ) by fixing the temporal\n\ndistortion in the patches with flicker distortion is obvious ,\n\ndimension . In order to represent the flicker distortion in the\n\ne.g. , the crumbling and disorderly edges , while the non\n\ntemporal domain of synthesized video , this embodiment\n\nflicker patch has clear edges . This phenomenon implies that\n\nattempts to keep the temporal dimension and fix either X or\n\nthe flicker distortion could be captured in the temporal layer .\n\nY in the 3D sparse representation , i.e. , 2D - XT or 2D - YT\n\nThus , the original view V , and the distorted synthesized\n\nplane . Then , the sparse representation is customized to\n\nview Va are converted to sets of temporal layers { Lo , i } and\n\nrepresent the temporal flickering features for the synthesized\n\n{ Ld , 1 } , respectively .\n\nvideo .\n\nI.\n\nSparse Representation Based Flicker Distortion\n\nB. Gradient Feature Extraction\n\nMeasurement ( SR - FDM )\n\n[ 0098 ] The gradient features are more suitable to extract\n\n[ 0094 ] One embodiment of the invention provides a sparse\n\nthe flicker distortion as compared with the pixel intensity\n\nrepresentation based flicker distortion measurement ( SR\n\nitself because : ( 1 ) human eyes are sensitive to the change\n\nFDM ) framework . The framework mainly consists of five\n\nrate of the intensity which leads to the significant change in\n\nmain steps / modules : temporal layer conversion , gradient\n\nthe gradient ; and ( 2 ) the flicker distortion caused by tem\n\nporal inconsistency of depth map usually locate at edges or\n\nfeature ex traction , flicker distortion detection , sparse rep\n\nresentation for flicker distortion features , and weighted layer\n\nregions with gradient . In this embodiment , vertical gradient\n\npooling\n\nfeatures of the temporal layers are used to capture the flicker\n\n[ 0095 ] FIG . 3 demonstrates the flowchart of the flicker\n\ndistortion to avoid the interference of the static situation . For\n\na static object , an arbitrary point on its boundaries as time\n\ndistortion assessment . As shown in FIG . 3 , first , an original\n\nvideo and the distorted synthesized video are converted to\n\nvaries would be a vertical line , which would result in large\n\nhorizontal gradient in temporal layer .\n\nthe temporal layers via temporal layer conversion ( subsec\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\ntion A), respectively. Then, each layer will be transformed to gradient feature map after gradient feature extraction (sub- section B). Afterwards, for each layer, the location of possible flicker distortion is identified through flicker dis- tortion detection module with the help of the associated depth video (subsection C). Subsequently, flicker distortion strengths are measured through the sparse coeflicients fea- tures at the identified flicker distortion location, and the sparse representation is based on the learned temporary dictionary through training stage (subsection D). Finally, the overall flicker distortion score is obtained by weighted layer pooling (subsection E).\n\n[0092] Conventional sparse representation and dictionary learning based on 2D spatial patches were used to represent the 2D image features. Currently, 3D dictionary learning, which further include the temporal or depth dimension, has been employed in several video applications, such as online sequence de-noising in M. Protter and M. Elad, \u201c/mage Sequence Denoising via Sparse and Redundant Represen- tations,\u201d\u2019 IEEE Trans. Image Process., vol. 18, no. 1, pp. 27-35, January 2009, video super-resolution in H. Xiang, Z. Pan, X. Ye and C. W. Chen, \u201cSparse spatio-temporal rep- resentation with adaptive regularized dictionary learning for Low bitrate video coding,\u201d IEEE Trans. Circuits Syst. Video Technol., vol. 23, no. 4, pp. 710-728, April 2013, and human action identification in S. C. W. Tim, M. Rombaut and D. Pellerin, \u201cDictionary of gray-level 3D patches for action recognition,\u201d 2014 IEEE Int. Workshop Mach. Learn. Signal Process. (MLSP), September 2014, pp. 1-6. The 3D dictionary learning objective function can be formulated as\n\nA. Temporal Layer Conversion\n\n[0096] A video can generally be considered as 3D volu- metric data V={V(x,y,t)|] SxSW, 1Sy<H, 1<t<T} where H, W, T represent video height (Y), width (X), and frame length (T), respectively. By dividing the 3D-XYT video into multiple 2D-XT layers, the video could be redefined as V={L,) iH}, where L={V(xy.Hl1 <x <W, y=i, 1<t<T} is the i-th temporal layer, and the eight H is also the total amount of temporal layers, i.e., 2D-XT planes. For VQA of synthesized video, there are three main advantages provided by segmenting a video into temporal layers: (1) visuality: it helps visualize the temporal features and can provide an explicit and intuitional cues; (2) capability: the temporal layer picture, a surface formed by space lines varying with time, can be used to present the long-term temporal features of the video; (2) simplicity: it avoids employing motion estimation method to match the atch between t-i-th frame and t-th frame to capture the motion features.\n\nsin 8? \u2014 WP aI + allel, + 200K?) 43D\n\nwhere X*?=[x1, x2,...,X;,...,X,] denotes the 3D training patches set from video data. x,ER* is a 3D volumetric data, consisting of horizontal x, vertical y, and temporal t dimen- sions, which is denoted as 3D-XYT.\n\n[0093] 3\u201d is the learned 3D dictionary. oa2 is a simpli- fied form of the sparse coeflicients of the overall 3D patches. ||, refers to the Li norm of the vector. ||a,3||, denotes that the sparse coefficient a7\u201d of patch i should satisfy the sparsity constraint, where u regulates the sparsity. p(X*\u201d) represents the task-driven constraint, and \u5165 regulates the weight of this term. The advantage of 3D sparse represen- tation is that it can learn 3D dictionaries for better repre- sentation ability. However, the computational complexity of learning 3D dictionaries increases dramatically. One alter- native solution is degrading 3D dictionary learning and approximating it with multi-layer 2D dictionaries. In fact, 2D sparse representation for 2D image (i.e., 2D-XY data) can be regarded as a special case or degradation of 3D sparse representation (i.e., 3D-XYT data) by fixing the temporal dimension. In order to represent the flicker distortion in the temporal domain of synthesized video, this embodiment attempts to keep the temporal dimension and fix either X or Y in the 3D sparse representation, 1.e., 2D-XT or 2D-YT plane. Then, the sparse representation is customized to represent the temporal flickering features for the synthesized video.\n\n[0097] Therefore, to assess the flicker distortion more effectively, this embodiment converts the distorted 3D syn- thesized video V into temporal layers {L,}, ie., 2D-XT Jane, as shown in the left part of the FIG. 4. It can be observed that the intense movement of the human and the variable motion of one point along the rim of the pillar are represented as a drastically twisted stripe and a smooth curve line respectively. This illustrates that the temporal layer can capture the temporal features. In addition, the distortion in the patches with flicker distortion is obvious, e.g., the crumbling and disorderly edges, while the non- flicker patch has clear edges. This phenomenon implies that the flicker distortion could be captured in the temporal layer. Thus, the original view V, and the distorted synthesized view Vz are converted to sets of temporal layers {L,,} and respectively.\n\nB. Gradient Feature Extraction\n\nI. Sparse Representation Based Flicker Distortion Measurement (SR-FDM)\n\n[0098] The gradient features are more suitable to extract the flicker distortion as compared with the pixel intensity itself because: (1) human eyes are sensitive to the change rate of the intensity which leads to the significant change in the gradient; and (2) the flicker distortion caused by tem- poral inconsistency of depth map usually locate at edges or regions with gradient. In this embodiment, vertical gradient features of the temporal layers are used to capture the flicker distortion to avoid the interference of the static situation. For a static object, an arbitrary point on its boundaries as time varies would be a vertical line, which would result in large horizontal gradient in temporal layer.\n\n[0094] One embodiment of the invention provides a sparse representation based flicker distortion measurement (SR- FDM) framework. The framework mainly consists of five main steps/modules: temporal layer conversion, gradient feature ex traction, flicker distortion detection, sparse rep- resentation for flicker distortion features, and weighted layer pooling.\n\n[0095] FIG. 3 demonstrates the flowchart of the flicker distortion assessment. As shown in FIG. 3, first, an original video and the distorted synthesized video are converted to the temporal layers via temporal layer conversion (subsec-\n\na\n\n3D\n\n{Lai},\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n6\n\ncorresponding to depth discontinuities , e.g. , strong edges or\n\n[ 0099 ] FIGS . 5A to 5E show a video frame synthesized\n\nwith original video and depth video , a temporal layer\n\nborders ( marked as rectangles ) . Therefore , depth map and its\n\ngenerated by the horizontal line in FIG . 5A , and related\n\ndiscontinuities can be utilized to detect the flicker distortion .\n\ngradient maps with and without horizontal gradients in\n\nWe use the edge detection operator , e.g. , canny , to detect the\n\ntemporal layer of static objects . It can be observed that the\n\ndepth edges of the synthesized depth image and a large\n\ngradient map with horizontal and vertical gradients in FIG .\n\nthreshold is used to get the strong depth edges . The depth\n\n5C and the gradient map with horizontal gradients in FIG .\n\nedges in the depth map is presented as Ex = { Dedge ( x , y , t ) / 1\n\n5D have strong strengths in rectangle areas while the gra\n\n< x < W , 1 < y < H , tek } .\n\ndient map with vertical gradients in FIG . 5E has very weak\n\n[ 0104 ]\n\nIn addition , especially for distorted synthesized\n\nfeatures . As shown in FIGS . 5C to 5E , the static objects can\n\nvideo with depth compression , the location of flicker dis\n\ngenerate strong horizontal gradients that reflect no mean\n\ntortion in synthesized video usually deviates at the texture\n\ningful motion information , thus affecting the true flicker\n\nedges by a few pixels . This may be mainly due to the\n\ndistortion detection . In addition , if flicker distortion exists\n\nmisalignment between color texture and depth videos and\n\nalong the boundaries of the static object , the vertical gradient\n\nthe depth errors induced by compression along the depth\n\ncan captures it .\n\nedges would easily generate the contour artifacts and neigh\n\n[ 0100 ]\n\nIn this embodiment , for the pixel ( x , i , t ) in Li , the\n\nborhood misplacement in synthesized views . To avoid the\n\nvertical gradient can be computed as\n\nmissed detection of the flicker area , in one embodiment ,\n\nimage dilation is employed to expand the detected edges\n\n18 ( x , 1,1 ) = V ( x , i , t = 1 ) -V ( x , i , 1 )\n\n( 2 )\n\nwidth for the depth edge map E. , k , and the dilated depth edge\n\n[ 0101 ] The temporal gradient G = { V? ( x , i , t ) | 1 < x < W , 1\n\nmap Enk = { Dedge ' ( x , y , t ) | 1 < x < W , 1 < y < H , t = k } is\n\n< t < T } is thus acquired . Therefore , the temporal gradient\n\nobtained by using a squared dilation mask . Since temporal\n\nset { 6. } and ( Gdj } for the original and distorted synthe\n\nlayer images are divided into patches as processing units in\n\nsized videos are obtained . FIGS . 6A to 6F show a related\n\nthis embodiment , dilation radius of 2 is enough to capture\n\nexample . In the rectangles in FIGS . 6B , 6C , 6E , and 6F ,\n\nthe patches with the flicker distortion .\n\ntaking the video entitled \u201c Undodancer \u201d as an example , the\n\ndistorted synthesized view ( FIG . 6D ) , when compared with\n\n[ 0105 ]\n\nAfter the edge detection and dilation , the areas\n\nthe original view ( FIG . 6A ) , has more obvious black floc\n\nwhere flicker distortion would take place could be detected\n\nculus along the motion trajectory in the gradient temporal\n\nroughly . Since the flicker distortion on the temporal layer is\n\nlayer . This implies that the flicker distortion corresponds to\n\nmeasured , the edge maps set { Eo ' } is converted into\n\nthe pattern changes in gradients . In practice , the gradient\n\ntemporal edge layers { M. , } , where { M. , 1 } = { Dedge = ( x , y , t ) | 1\n\nmap needs patch refinement to exclude noises . The random\n\n< x < W , y = i , 1 < t < T } . If the distortion along the edges\n\nnoises in the original video captured by the cameras may\n\nflicks or changes in a very short time , i.e. , the number of\n\ncause some small changes in gradients in temporal layer ,\n\nedge pixels in temporal layer is very small , the human eyes\n\nwhich are actually not the flicker distortion . In order to\n\ncan hardly perceive this flickering . In this case , it is assumed\n\nreduce the influence of this noise , the patches in the temporal\n\nthat possible flicker perception is caused only if the number\n\nlayer are excluded if their gradient variance is small . The\n\nof edge pixels in patches of the temporal layer Mo , i exceeds\n\neffective patches set in G layer can be defined as\n\na threshold B in a predetermined period . The edge patches\n\nset in the i - th temporal layer are refined as\n\n( 3 )\n\nv\n\n1\n\nu + w - 1 v + w - 1\n\nE ( 18 ( x , i , t ) \u2013 V $ ( x , i , 1 ) ) ?\n\n( 5 )\n\n( u , v ) E Dedge ' ( x , i , t ) > B\n\ntry\n\nx = U\n\nSv , i = ( u , v ) |\n\nW2\n\n( 4 )\n\nu\n\n+ wy + w\n\n???? ( x , 1 , 1 )\n\nV ( x , i , 1 ) = x = u t = y\n\nwhere ( u , v ) indicates the index of edge patch in the i - th edge\n\nW2\n\ntemporal layer Mo , of the original depth video . In this\n\nexample B is set as 1 .\n\nwhere ( u , v ) denotes the patch index , indicating the location\n\n[ 0106 ] The final flicker distortion area S ; in the i - th tem\n\nof one patch in the i - th temporal layer of the distorted\n\nporal layer could be obtained based on the textural gradient\n\nsynthesized video . w is the patch size . In this example , the\n\nand depth edges , which is S = Sv.inse , i . The flicker distor\n\nvariance threshold is set as 5 .\n\ntion area of whole video consist of flicker area all the\n\ntemporal layers , i.e. , { S ; } . In addition , the binarization\n\nC. Depth Image Based Flicker Distortion Area Detection\n\nweight map W ; in each temporal layer G ; can be obtained\n\naccordingly as\n\n[ 0102 ]\n\nSince not all the areas in the temporal layer include\n\nflicker distortion , a flicker distortion area detection algo\n\nrithm is included in this embodiment to locate the flicker\n\ndistortion more precisely . In fact , the flicker distortion of\n\n( 6 )\n\n1 ,\n\nif ( u , v ) ES ;\n\nW ; ( u , v ) { 0 , otherwise\n\nsynthesized videos usually locates at the object edges , which\n\nis mainly caused by the depth temporal inconsistency among\n\nframes and misalignment between depth and color videos at\n\nthe depth edges or discontinuous regions .\n\nwhere W ( u , v ) is element of W ;. With the assistance of the\n\ndepth map , the flicker distortion area can be located more\n\n[ 0103 ] As shown in the right part of FIG . 4 , the flicker\n\ndistortion mainly exists in the areas of synthesized view\n\naccurately .\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\ncorresponding to depth discontinuities, e.g., strong edges or borders (marked as rectangles). Therefore, depth map and its discontinuities can be utilized to detect the flicker distortion. We use the edge detection operator, e.g., canny, to detect the depth edges of the synthesized depth image and a large threshold is used to get the strong depth edges. The depth edges in the depth map is presented as EB, y={D\u00b0*\"(x.y,DIL <x <W, 1<y <H, tk}.\n\n[0099] a synthesized with original video and depth video, a temporal layer generated by the horizontal line in FIG. 5A, and related gradient maps with and without horizontal gradients in temporal layer of static objects. It can be observed that the gradient map with horizontal and vertical gradients in FIG. 5C and the gradient map with horizontal gradients in FIG. 5D have strong strengths in rectangle areas while the gra- dient map with vertical gradients in FIG. 5E has very weak features. As shown in FIGS. 5C to 5E, the static objects can generate strong horizontal gradients that reflect no mean- ingful motion information, thus affecting the true flicker distortion detection. In addition, if flicker distortion exists along the boundaries of the static object, the vertical gradient\n\nvideo with depth compression, the location of flicker dis- tortion in synthesized video usually deviates at the texture edges by a few pixels. This may be mainly due to the misalignment between color texture and depth videos and. the depth errors induced by compression along the depth edges would easily generate the contour artifacts and neigh- borhood misplacement in synthesized views. To avoid the missed detection of the flicker area, in one embodiment, image dilation is employed to expand the detected edges width for the depth edge map E., ,, and the dilated depth edge map E,,'-{D\u00b0**(\u00aby,DIL<x=W, 1Sy<H, tek} is obtained by using a squared dilation mask. Since temporal layer images are divided into patches as processing units in this embodiment, dilation radius of 2 is enough to capture\n\n[0100] In this embodiment, for the pixel (x,i,t) in Li the vertical gradient can be computed as\n\nVED =VGiH1)- Mit) Q)\n\ntT} is thus acquired. Therefore, the temporal gradient set {G,,,} and {G,,} for the original and distorted synthe- sized videos are obtained. FIGS. 6A to 6F show a related example. In the rectangles in FIGS. 6B, 6C, 6E, and 6F, taking the video entitled \u201cUndodancer\u201d as an example, the distorted synthesized view (FIG. 6D), when compared with the original view (FIG. 6A), has more obvious black floc- culus along the motion trajectory in the gradient temporal layer. This implies that the flicker distortion corresponds to the pattern changes in gradients. In practice, the gradient map needs patch refinement to exclude noises. The random noises in the original video captured by the cameras may cause some small changes in gradients in temporal layer, which are actually not the flicker distortion. In order to reduce the influence of this noise, the patches in the temporal layer are excluded if their gradient variance is small. The\n\n[0105] edge dilation, areas where flicker distortion would take place could be detected roughly. Since the flicker distortion on the temporal layer is measured, the edge maps set {E,,'} is converted into temporal edge layers {M\u3002 ,}, where {M, ,}={D\u00b0#*(xy,t)I1 <=x<W, y=i, 1<t<T}. If the distortion along the edges flicks or changes in a very short time, i.e., the number o edge pixels in temporal layer is very small, the human eyes can hardly perceive this flickering. In this case, it is assumed that possible flicker perception is caused only if the number of edge pixels in patches of the temporal layer Mi exceeds a threshold B in a predetermined period. The edge patches in the i-th refined\n\nsewed vl QB) SY ee io-Vie boy Sy = 4, \u00a5)| Se >g at vie (4) >\u00bb > V(x, iD VE i= 2\n\nwehwel tH } (5) > > Dee (x, i, > B xu EY Sei = {i 3)\n\n(u, v) indicates the index of edge patch in the i-th temporal layer M,, of the original depth video. In B is set as 1.\n\n(u, v) denotes the patch index, indicating the one patch in the i-th temporal layer of the distorted synthesized video. w is the patch size. In this example, variance threshold g is set as 5.\n\n[0106] The final flicker distortion area S in the i-th tem- poral layer could be obtained based on the textural gradient and depth edges, which is S-S,,S,,. The flicker distor- tion area of whole video consist of flicker area all the temporal layers, ic., {S,}. In addition, the binarization weight map W, in each temporal layer G, can be obtained. accordingly as\n\nC. Depth Image Based Flicker Distortion Area Detection\n\n[0102] Since not all the areas in the temporal layer include flicker distortion, a flicker distortion area detection algo- rithm is included in this embodiment to locate the flicker distortion more precisely. In fact, the flicker distortion of synthesized videos usually locates at the object edges, which is mainly caused by the depth temporal inconsistency among frames and misalignment between depth and color videos at the depth edges or discontinuous regions.\n\n1, if, ves, 0, otherwise of\n\nW,(u, v) is element of W,. With the assistance of map, the flicker distortion area can be located\n\n[0103] As shown in the right part of FIG. 4, the flicker distortion mainly exists in the areas of synthesized view\n\nFIGS. 5A to 5E show\n\nvideo frame\n\ncan captures it.\n\n[0101]\n\nThe temporal gradient G={V%(x,i,0I1 <x = W, 1\n\neffective patches set in G layer can be defined as\n\nwhere of\n\nlocation\n\nthe\n\n[0104]\n\nIn addition, especially for distorted synthesized\n\nthe patches with the flicker distortion.\n\nAfter the\n\ndetection and\n\nthe\n\nset\n\ntemporal layer are\n\nas\n\nwhere\n\nedge this\n\nexample\n\n(6)\n\nWilu,\n\nwhere depth accurately.\n\nthe\n\nmore\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n7\n\nD. Sparse Representation for Flicker Distortion Features\n\nflicker distortion ; the other is amplitude distortion , which\n\ncan capture the strength of the flicker distortion . For patches\n\n[ 0107 ] The sparse representation can be used to measure\n\nin the original video and its corresponding patch in the\n\nthe distortion between the original and distorted videos in\n\ndistorted synthesized video in the i - th temporal layer , the\n\nthe detected flicker distortion areas , which includes temporal\n\ntwo features can be written as\n\ndictionary learning phase and sparse representation phase .\n\n[ 0108 ]\n\n1 ) Temporal Dictionary Learning : To represent the\n\nflicker distortion in the synthesized video , the dictionary that\n\nKate\n\n( 8 )\n\nif W ; ( u , v ) = 1\n\naims to learn the temporal flicker features of 2D - XT or\n\nPO\n\n) = { || 0u ; i | l2 + || 004 ; || 2 + e\n\n2D - YT plane could be learned . Since the 2D - XT and 2D - YT\n\nelse\n\n1 ,\n\nplane have the similar effects in capturing the motion\n\nfeatures , the dictionary learning function for 2D - XT plane is\n\n||| e2 ; / I , \u2013 || af ; il , l + e\n\n( 9 )\n\nif W ; ( u , v ) = 1\n\nused and can be derived from Equation ( 1 ) as\n\nAO\n\n) =\n\n|| a\u00e4 ; || 2 + || 04 ; b || 2 + e\n\n2\n\nelse\n\nmin || X2D \u2013 42D @ 2012 s.t. || 0z9 | 1 , < ?\n\n( 7 )\n\n42D 42D\n\nwhere duv 0.1\n\nare the sparse coefficients of the\n\ndi\n\nand\n\noriginal patch yu ,\n\nand the distorted patch yu ,\n\ndi with\n\nrespect to the learned dictionary Y2D by Equation ( 7 ) ,\n\nwhere X2D denotes the training patches set of 2D - XT from\n\nrespectively . ( denotes the inner product . c is a constant\n\nvideo data . Y2D is the learned 2D dictionary for temporal\n\nwith a small value added to prevent the denominator to be\n\nzero which is set as 0.02 . Plyu , v\u00bast , yu , d.i ) computes the\n\nlayers .\n\nI'llo means the number of nonzero entries in the\n\nvector . During dictionary learning , the number of nonzero\n\n0,1 and auzy d .\n\nphase similarity between sparse coefficients Qu , va\n\nentries of should not be greater than a given ? . In this\n\ni , and can be used to measure the structural similarity\n\nexample , ? is set as 6. The dictionary is learned by using\n\ndi . Ay 0 ,\n\n0 , and du ,\n\ndi ) measures the\n\nbetween Yu ,\n\nUV ?u , ?\n\namplitude similarity between yu , \u00baf and y , d through sparse\n\nK - SVD illustrated in M. Aharon , M. Elad , and A. Bruck\n\nstein , \u201c K - SVD : an algorithm for designing overcomplete\n\ncoefficients . P ( Yuva Yu , di ) and A ( Yuva Yu , vdi ) are both\n\n0,2\n\n0,1\n\n?\n\ndictionaries for sparse representation , \" IEEE Trans . Signal\n\namong the range [ 0 , 1 ] . The combination of Equations ( 8 )\n\nand ( 9 ) can be used to measure the integral similarity\n\nProcess . , vol . 54 , no . 11 , pp . 4311-4322 , November 2006 .\n\nDuring learning , the sparse coefficients are solved by OMP\n\nbetween patch y . , and yu , dui . In fact , P ( y2.0 , y.d. ) and\n\nYu , vd.1 ) represent the non - flicker features and they\n\nalgorithm presented in Y. C. Pati , R. Rezaiifar and P. S.\n\n0,1\n\nA ( Yu , 2\n\nKrishnaprasad , \" Orthogonal matching pursuit : recursive\n\nmay be large in representing flicker distortions .\n\nfunction approximation with applications to wavelet decom\n\n[ 0111 ]\n\nSince human eyes tend to perceive the flicker\n\nposition , \" Proc . Conf . Rec . 27th Asilomar Conf . Signals ,\n\ndistortion in the form of regions instead of lines , the flicker\n\ndistortion can be computed over multiple temporal layers\n\nSyst . Comput . , vol . 1 , 1993 , pp . 40-44 . The learned temporal\n\ndictionary is 64x256 .\n\ninstead of a single layer . For simplicity , the sparse coeffi\n\nYu . , d \u00bb ) and A ( Yu , Yu.vd. ) of a group of\n\ncients P ( Yu , 0,1\n\n[ 0109 ] To demonstrate the difference between the learned\n\n0,1\n\ntemporal layers , i.e. , Ux = { L ; lh , ( k - 1 ) +1 < i < hk } ,\n\ntemporal dictionary with the normal spatial dictionary ,\n\nFIGS . 7A and 7B illustrate the two types of dictionaries for\n\ncomparison . As shown in FIGS . 7A and 7B , both diction\n\nke [ 1 . hs\n\naries were learned with the same training sequences and\n\nH\n\nlearning parameters . The difference is that the spatial dic\n\ntionary was learned from the conventional image patches\n\nwhile the temporal dictionary was learned from the patches\n\nare averagely merged , and the integral similarity for patches\n\nextracted from temporal layers . It is observed from FIGS .\n\nlocating ( u , v ) at\n\n7A and 7B that the learned temporal dictionary , when\n\ncompared with the spatial dictionary , has more regular and\n\nsharper edges which reflect the motion information of the\n\n( 10 )\n\n1\n\nobjects in the video . The atom with the vertical edges\n\n[ A ; ( V ) , y\n\n) ] \u00b0 [ P : ( yei , yali ) ]\n\nCk ( u , v )\n\nhs i = hs ( k - 1 ) +1\n\nrepresents a motionless pattern while atoms with straight\n\nedges of other directions represent uniform motion patterns ,\n\nand atoms with bending edges denote motion patterns with\n\nwhere h , is the number of temporal layers for averaging , k\n\nvariable velocity . Therefore , the learned temporal dictionary\n\nis an index of Uk and a and b are parameters denoting\n\nis suitable to capture the temporal activity of the video . The\n\nweights of amplitude and phase distortion . In this example ,\n\ngoal of temporal dictionary learning is to learn the normal\n\na and b are set as 1 respectively , meaning that the amplitude\n\ntemporal activity non - flicker distortion . Thus , when the\n\nand phase distortions are of equivalent importance in the\n\nlearned dictionary is employed upon the original and dis\n\nquality assessment . Finally , the score of the flicker distortion\n\ntorted synthesized videos via the sparse representation , the\n\nof Uk can be obtained as\n\nflicker distortion existing in the distorted videos could be\n\ndistinguished .\n\n[ 0110 ]\n\n2 ) Sparse Representation for Flicker Distortion : In\n\nSFK = ?.? ( 1 - C ( ? , ? ) )\n\n( 11 )\n\nthis embodiment , to represent the flicker distortion , two\n\ntypes of features based on sparse representation are used as\n\nEu , W ( u , v )\n\nthe distortion features . One is phase distortion , which is\n\nemployed to measure the flocculus shape features of the\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021\n\nD. Sparse Representation for Flicker Distortion Features\n\ndistortion; amplitude distortion, can capture the strength of the flicker distortion. For patches the original video and its corresponding patch in the distorted synthesized video in the i-th temporal layer, the two features can be written as\n\n[0107] The sparse representation can be used to measure the distortion between the original and distorted videos in the detected flicker distortion areas, which includes temporal dictionary learning phase and sparse representation phase.\n\n[0108] 1) Temporal Dictionary Learning: To represent the flicker distortion in the synthesized video, the dictionary that aims to learn the temporal flicker features of 2D-XT or 2D-YT plane could be learned. Since the 2D-XT and 2D-YT plane have the similar effects in capturing the motion features, the dictionary learning function for 2D-XT plane is used and can be derived from Equation (1) as\n\n\u4eba . 1 Piss Se) =) es + Moll te 1, else\n\nel, \u4e00 las + \u4eba 9) He wu yo ad ai 7 llatevll, +llowol + Aye. vib = | \u4e00 1, else\n\n- 2 min 1IX22 \u2014wa?P|i> st lo \u5404 lb <e (7\n\noriginal \u2018patch Yun rae and the distorted patch ye 4 with respect to the learned dictionary ye? by Equation (7), respectively. (*) denotes the inner product. c is a constant with a small value added to prevent the denominator to be zero which is set as 0.02. PO ays Yaw) computes the phase similarity between sparse coefficients a,,,\u00b0\" and ou \uff0c and can be used to measure the structural similarity between y,,,\u00b0% and de AG ue\u201d yo \u4eba Imeasures the amplitude similarity between y,,,\u00b0\u201d and Yor \u2019 \u201cthrough sparse coefficients. PCy,\u201d Vu, V) and AQ Yay @) are both among the range [0, 1]. The combination of Equations (8) and (9) can be used to measure the integral similarity between patch Yuy\u201d and y,, In fact, POs Yun we) and AQ Year #*) represent the non-flicker features and they\n\nvideo data. W*\u201d is the learned 2D dictionary for temporal layers. ||.||) means the number of nonzero entries in the vector. During dictionary learning, the number of nonzero entries of should not be greater than a given sg. In this example, \u00a2 is set as 6. The dictionary is learned by using K-SVD illustrated in M. Aharon, M. Elad, and A. Bruck- stein, \u201cK-SVD: an algorithm for designing overcomplete dictionaries for sparse representation,\u201d IEEE Trans. Signal Process., vol. 54, no. 11, pp. 4311-4322, November 2006. During learning, the sparse coefficients are solved by OMP algorithm presented in Y. C. Pati, R. Rezaiifar and P. S. Krishnaprasad, \u201cOrthogonal matching pursuit: recursive function approximation with applications to wavelet decom- position,\u201d\u2019 Proc. Conf. Rec. 27th Asilomar Conf. Signals, Syst. Comput., vol. 1, 1993, pp. 40-44. The learned temporal\n\n[0111] Since human eyes tend to perceive the flicker distortion in the form of regions instead of lines, the flicker distortion can be computed over multiple temporal layers instead of a single layer. For simplicity, the sparse coefli- cients POY. Yaw\") and A(y Yuu\u201d) of a group of temporal layers, Le U,={L,lh,(k-1)+1 <i <h,k},\n\ntemporal dictionary with the normal spatial dictionary, FIGS. 7A and 7B illustrate the two types of dictionaries for comparison. As shown in FIGS. 7A and 7B, both diction- aries were learned with the same training sequences and learning parameters. The difference is that the spatial dic- tionary was learned from the conventional image patches while the temporal dictionary was learned from the patches extracted from temporal layers. It is observed from FIGS. 7A and 7B that the learned temporal dictionary, when compared with the spatial dictionary, has more regular and sharper edges which reflect the motion information of the objects in the video. The atom with the vertical edges represents a motionless pattern while atoms with straight edges of other directions represent uniform motion patterns, and atoms with bending edges denote motion patterns with variable velocity. Therefore, the learned temporal dictionary is suitable to capture the temporal activity of the video. The goal of temporal dictionary learning is to learn the normal temporal activity non-flicker distortion. Thus, when the learned dictionary is employed upon the original and dis- torted synthesized videos via the sparse representation, the flicker distortion existing in the distorted videos could be\n\nkell, ih\n\naveragely merged, and the integral similarity for patches locating (u, v) at\n\nFisk (10) Guys a (aoe. \u5929 \u751f, \u5c0f SDH1\n\nwhere h is the number of temporal layers for averaging, k is an index of U;,, and a and b are parameters denoting weights of amplitude and phase distortion. In this example, aand b are set as | respectively, meaning that the amplitude and phase distortions are of equivalent importance in the quality assessment. Finally, the score of the flicker distortion of U;, can be obtained as\n\n[0110] 2) Sparse Representation for Flicker Distortion: In this embodiment, to represent the flicker distortion, two types of features based on sparse representation are used as the distortion features. One is phase distortion, which is employed to measure the flocculus shape features of the\n\nDe dy b= Ce ay La Ly We, \u00a5) Si =\n\n\u3010\n\n0\n\nwhere X2P denotes the training patches set of 2D-XT from\n\ndictionary is 64256. [0109] To demonstrate the difference between the learned\n\ndistinguished.\n\nflicker\n\nthe other is\n\nwhich\n\nin\n\n)\n\nwhere \u00ab,,,\u00b0\" and a,,,%\" are the sparse coefficients of the\n\ni,\n\nmay be large i in representing flicker distortions.\n\nare\n\nUS 2021/0044791 A1\n\nFeb. 11 , 2021\n\n8\n\nwhere the weight map W ( u , v ) is obtained by\n\nabove ) and the spatial - temporal activity distortion measure\n\nment . Both the original video and the distorted synthesized\n\nvideo are input into the two modules . Additionally , the\n\nsynthesized depth video is input into the SR - FDM module .\n\n( 12 )\n\nS ;\n\n[ 0115 ] The overall quality score of a compressed synthe\n\ni = hs ( k - 1 ) +1\n\nW ( u , v ) =\n\nsized video is predicted by pooling the flicker distortion\n\n0 , otherwise\n\nscore and the spatial - temporal activity distortion score .\n\nA. Spatial - Temporal Activity Distortion Measurement\n\nE. Weighted Pooling for Temporal Layers\n\n[ 0116 ]\n\nThis embodiment employs the same method to\n\n[ 0112 ]\n\nSince the group of temporal layers Uk may con\n\nassess spatial activity distortion as that in X. Liu , Y. Zhang ,\n\ntribute unevenly to the visual perception , in this embodiment\n\nS. Hu , S. Kwong , C. C. J. Kuo and Q. Peng , \u201c Subjective and\n\na weighted pooling scheme is provided for the temporal\n\nobjective video quality assessment of 3D synthesized views\n\nlayers . It is determined that the temporal layer with more\n\nwith textureldepth compression distortion , \" IEEE Trans .\n\nedge patches probably makes more contribution to the final\n\nImage Process . , vol . 24 , no . 12 , pp . 4847-4861 , December\n\nperception of the flicker distortion . Therefore , a rank - based\n\n2015 , which mainly measures the variance difference of the\n\nmethod as presented in L. Li , Y. Zhou , K Gu , W. Lin and S.\n\npixel gradients in a spatiotemporal tube . The concept of\n\nWang , \u201c Quality assessment of DIBR - synthesized images by\n\nQuality Assessment Group of Pictures ( QA - GOP ) and Spa\n\nmeasuring local geometric distortions and global sharp\n\ntial - temporal ( S - T ) tube are introduced in the spatial - tem\n\nness , \" IEEE Trans . Multimedia , vol . 20 , no . 4 , pp . 914-926 ,\n\nporal activity distortion measurement method , which is\n\nApril 2018 and K. Gu , S. Wang , G. Zhai , W. Lin , X. Yang\n\nillustrated in FIG . 9. A video is divided into several QA\n\nand W. Zhang , \u201c Analysis of distortion distribution for pool\n\nGoPs , which is made up of a number of frames , e.g. , 2N + 1\n\ning in image quality prediction , \u201d IEEE Trans . Broadcast . ,\n\nframes as shown in FIG . 9. A QA - GOP consists of multiple\n\nvol . 62 , no . 2 , pp . 446-456 , June 2016 is applied to pool the\n\nS - T tubes , which are concatenated by matched blocks via\n\nscores among temporal layers . The flicker score of the whole\n\nmotion estimation algorithms in adjacent frames and denote\n\ndistorted video SFy is\n\nthe motion trajectory . Given the original video V , and\n\ndistorted synthesized video V? , first of all , the pixel gradi\n\nents G . $ ( x , y , t ) , and G _ $ x , y , t ) , are computed by calculating\n\ns , WkSFk\n\n( 13 )\n\nHs\n\nthe norm of the pixel gradient vector composed of horizontal\n\nk = 1\n\nSFy =\n\nand vertical spatial gradients at frame t , respectively .\n\nIls u wk\n\nG $ ( x , y , 1 ) = VIVG . ( x , y , 0 ) / 2 + IVG $ ( x , y , 0 ) 12\n\n( 15 )\n\nwhere we represents the weight of each layer ,\n\nwhere qe { o , d } , VG @ x * ( x , y , t ) , VGpx \" ( x , y , t ) are gradients in\n\nthe horizontal and vertical directions , respectively . Then , the\n\ngradients are organized by S - T tube , and the standard\n\nH\n\ndeviation of the gradients in the i - th S - T tube in a QA - GOP\n\nHg =\n\nhs\n\nis computed as\n\nSFk represents the flicker score of the k - th group temporal\n\nlayer Uk This SF , score is normalized to range [ 0 , 1 ]\n\n( 16 )\n\notube ( Xn , Yn , in ) =\n\nthrough the normalization of the summation of the weight of\n\nin + N Xn + w - 1 ynth - 1\n\nWk , which is calculated as\n\nIII ( C $ ( x , y , t ) \u2013\n\ntube ( x , y , t ) |\n\nt = tn - N xshn y = yn\n\nwxhx ( 2N + 1 )\n\nlog : ( 1\n\nRankk\n\n( 14 )\n\nWk = log2 | 1 +\n\n( 17 )\n\nin + N Xn + w - l yn th - 1\n\nH ,\n\n? ? ? G ( x , y , 1 )\n\nt = tn - N x = \\ n\n\ny = yn\n\nGS tube ( x , y , 1 )\n\nwhere Rank , represents the rank of the UK among all layers\n\n,\n\nwxhx ( 2N + 1 )\n\nin term of the importance , i.e. , the number of edge patches .\n\nIn this way , the flicker distortion score SF y of the distorted\n\nwhere w and h are width and height of the tube in spatial\n\nsynthesized video can be obtained .\n\ndomain . N is the number of forward or backward frames\n\ninvolved in a S - T tube . The spatial - temporal activity Rp\n\ntube\n\nII . The Sparse Representation Based 3D View\n\ncan be then obtained by clipping on ( XnYntn ) , where i is\n\ntube\n\nQuality Assessment ( SR - 3DVQA ) Model\n\nthe index of for tube { xmYnotn } . They are calculated as\n\n[ 0113 ] The distortions of synthesized video mainly have\n\ntwo categories . One is the flicker distortion , the other is the\n\nconventional spatial - temporal activity distortions ( such as\n\notube ( Xn , Yn , yn ) ,\n\nif otube ( Xn , Yn , in ) > T\n\n( 18 )\n\nRube\n\ncompression artifacts , rendering distortion , contour and hole\n\notherwise\n\nT ,\n\nartifacts ) in synthesized video .\n\n[ 0114 ] FIG . 8 shows a sparse representation based 3D\n\nwhere t is the perceptible threshold for spatial - temporal\n\nview quality assessment ( SR - 3DVQA ) method in one\n\ngradient standard deviation , and is set as 180 in this\n\nembodiment of the invention . The SR - 3DVQA model\n\nexample . Afterwards , the distortion score of a QA - GOP is\n\nmainly includes two modules : SR - FDM ( as presented\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\nwhere the weight map W(Uv) is obtained by\n\nabove) and the spatial-temporal activity distortion measure- ment. Both the original video and the distorted synthesized video are input into the two modules. Additionally, synthesized depth video is input into the SR-FDM module.\n\n. isk (12) wan {#9 cS 0, otherwise\n\n[0115] The overall quality score of a compressed synthe- sized video is predicted by pooling the flicker distortion score and the spatial-temporal activity distortion score.\n\nA, Spatial-Temporal Activity Distortion Measurement\n\nE. Weighted Pooling for Temporal Layers\n\n[0116] This embodiment employs the same method to assess spatial activity distortion as that in X. Liu, Y. Zhang, S. Hu, S. Kwong, C. C. J. Kuo and Q. Peng, \u201cSubjective and objective video quality assessment of 3D synthesized views with texture/depth compression distortion,\u201d\u201d IEEE Trans. Image Process., vol. 24, no. 12, pp. 4847-4861, December 2015, which mainly measures the variance difference of the pixel gradients in a spatiotemporal tube. The concept of Quality Assessment Group of Pictures (QA-GoP) and Spa- tial-temporal (S-T) tube are introduced in the spatial-tem- poral activity distortion measurement method, which is illustrated in FIG. 9. A video is divided into several QA- GoPs, which is made up of a number of frames, e.g., 2N+1 frames as shown in FIG. 9. A QA-GoP consists of multiple S-T tubes, which are concatenated by matched blocks via motion estimation algorithms in adjacent frames and denote the motion trajectory. Given the original video V, and. distorted synthesized video V first of all, the pixel gradi- ents G,S(x,y,t), and G(x,y,t), are computed by calculating the norm of the pixel gradient vector composed of horizontal and vertical spatial gradients at frame t, respectively.\n\n[0112] Since the group of temporal layers U, may con- tribute unevenly to the visual perception, in this embodiment a weighted pooling scheme is provided for the temporal layers. It is determined that the temporal layer with more edge patches probably makes more contribution to the final perception of the flicker distortion. Therefore, a rank-based method as presented in L. Li, Y. Zhou, K Gu, W. Lin and S. Wang, \u201cQuality assessment of DIBR-synthesized images by measuring local geometric distortions and global sharp- ness,\u201d IEEE Trans. Multimedia, vol. 20, no. 4, pp. 914-926, April 2018 and K. Gu, S. Wang, G. Zhai, W. Lin, X. Yang and W. Zhang, \u201cAnalysis of distortion distribution for pool- ing in image quality prediction,\u201d IEEE Trans. Broadcast., vol. 62, no. 2, pp. 446-456, June 2016 is applied to pool the scores among temporal layers. The flicker score of the whole distorted video SF ,- is\n\n= Hs Dee Me ys vy SF (13) Shy = Ss\n\nSteyn Syne Saupe GSox.O-V IYG 58.0? 4iVGy 5,30 (15)\n\nwhere w, represents the weight of each layer,\n\nwhere ~E{o,d}, VG, yst), VGw (CoybD are gradients in the horizontal and vertical directions, respectively. Then, the gradients are organized by S-T tube, and the standard deviation of the gradients in the i-th S-T tube in a QA-GoP computed as\n\nSF, represents the flicker score of the k-th group temporal layer U,. This SF, score is normalized to range [0, 1] through the normalization of the summation of the weight of We Which is calculated as\n\nOf ns Ys tn) = (16) Tat Samoa \u4e00 \u570b (cgoo De \u53ef ch Sinn wxhx(2QN +1) Ra 1 (7) Dy Ge yD ON hy Yn wxhx(2N +1) CG nbe YD =\n\nRank ] a4) Is = tog,(1 +\n\nwhere Rank, represents the rank of the U, among all layers in term of the importance, i.e., the number of edge patches. In this way, the flicker distortion score SF, of the distorted synthesized video can be obtained.\n\nwhere w and h are width and height of the tube in spatial domain. N is the number of forward or backward frames involved in a S-T tube. The spatial-temporal activity Ry\u201d can be then obtained by clipping Og KV nsty)s where i the index of for tube {x,,,y,,t,,}. They are calculated as\n\nIl. The Sparse Representation Based 3D View Quality Assessment (SR-3DVQA) Model\n\n[0113] The distortions of synthesized video mainly have two categories. One is the flicker distortion, the other is the conventional spatial-temporal activity distortions (such as compression artifacts, rendering distortion, contour and hole artifacts) in synthesized video.\n\npeste \u4eba if OB (ny Yas bn) > (18) pi = otherwise\n\n[0114] FIG. 8 shows a sparse representation based 3D view quality assessment (SR-3DVQA) method in one embodiment of the invention. The SR-3DVQA model mainly includes two modules: SR-FDM (as presented\n\nwhere t is the perceptible threshold for spatial-temporal gradient standard deviation, and is set as 180 in example. Afterwards, the distortion score of a QA-GoP\n\nWhe\n\nthe\n\nis\n\njs\n\nz\n\nthis is\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n9\n\nrectangles ( corresponding to flicker areas ) , shall all be\n\ncalculated through worstcase pooling strategy , and the over\n\nall spatial - temporal distortion score of the whole video is\n\ndetected , and the edges outside the rectangles shall not be\n\nincluded ( or included as few as possible ) . It can be observed\n\nobtained as\n\nthat edge maps generated by thresholds from 0.1 to 0.5 are\n\nmore consistent to this flicking area for sequence \u201c Bal\n\nloons \u201d , where strong edges of balloons and ribbons inside\n\n10gio Rube di\n\n( 19 )\n\nthe rectangles are detected and less edges of plants and the\n\nRtube\n\nman outside the rectangles are detected . In this case , thresh\n\nolds among range 0.1 to 0.5 seem suitable for sequence\n\n\u201c Balloons \u201d .\n\nSimilar\n\nresult\n\nfor\n\n\" Lovebirds \u201d .\n\nFor\n\nwhere o denotes the set of the worst 5 % percent S - T tubes\n\n\u201c Undodancer \u201d , the flicker mainly concentrates on the upper\n\nin a QA - GOP , No denotes the number of tubes in set 0 , No\n\nall\n\nbody of the dancer and the rightmost and foremost pillar , so\n\nrepresents the number of QA - GOP in a test video .\n\nthresholds among range 0.07 to 0.2 are feasible . According\n\nto results among the test sequences , threshold 0.2 is selected\n\nB. Pooling\n\nfor the edge detection in this example .\n\n[ 0117 ] A general pooling method combining the summa\n\ntion and the multiplication is explored to integrate the\n\nB. Quality Prediction Performance Comparisons\n\nflickering and spatial - temporal distortions in assessing the\n\n[ 0122 ]\n\nIn this subsection , the training dataset , testing\n\nsynthesized video quality , which can be written as\n\ndataset , and settings of the SR - 3DVQA model are first\n\nSy - cx ( wiSF + W2S4v ) = dxf ( SF v ) xSA v\n\nintroduced . Then , the quality prediction performances\n\n( 20 )\n\nwhere c , d are weighted parameters to balance the relative\n\namong different methods ( including the SR - 3DVQA model )\n\nare compared .\n\nimportance of the summation and multiplication pooling\n\n[ 0123 ]\n\n1 ) Settings for Temporal Dictionary Learning : Due\n\nitems . f ( . ) denotes the nonlinear map function of the flicker\n\nto the temporal inconsistency in the generated depth video ,\n\ndistortion score in multiplication pooling . W1 , W2 are used to\n\nweigh the flicker distortion score and the spatial - temporal\n\nthe synthesized video rendered from the original texture\n\nactivity distortion score in summation pooling . When d is set\n\nvideos and depth videos may also have noticeable flicker\n\nto zero , Equation ( 20 ) is degenerated to the summation .\n\ndistortion . Therefore , the original view instead of the origi\n\nnal synthesized video is preferred for temporal dictionary\n\nSimilarly , when c is set to zero , Equation ( 20 ) is degenerated\n\nto the multiplication .\n\nlearning . For the original texture video in MVD system that\n\nhas similar temporal operties with the conventional single\n\n[ 0118 ]\n\nIn this embodiment , f = * = x , c , d , w1 , W2 are set as\n\n1,0,0.5 , 0.5 , respectively , which denotes the flicker distor\n\nview video , the conventional videos from HEVC test\n\ntion and spatiotemporal activity distortion are summated in\n\nsequences were selected as training sequences so as to\n\nseparate the training sequences from the test sequences . To\n\nthe pooling stage . The impacts of the pooling method ,\n\nweight parameters and mapping function f ( . ) are discussed\n\ncover different spatial resolution and content , eight 2D video\n\nsequences were selected in the temporal dictionary learning .\n\nin Section III subsection D.\n\nThese videos are entitled \u201c BasketballDrive \u201d , \u201c FourPeople \u201d ,\n\nIII . Experimental Results and Analyses\n\n\u201c Flowervase \u201d , \u201c Johnny \u201d , \u201c KristenAndSara \u201d , \u201c ParkScene \" ,\n\n\u201c RaceHorses \u201d , and \u201c Vidyo3 \u201d . The properties of the training\n\n[ 0119 ]\n\nExperiments and simulations have been performed\n\nsequences are shown in FIG . 13. The first 300 or 240 frames\n\nto assess the performance of the method of the above\n\nof each sequence were kept in training . For each sequence ,\n\nembodiments . This section first presents the Canny thresh\n\nfour temporal layers were extracted at a uniform sampling\n\nold determination for edge detection . Then , the quality\n\nway along the frame height . Then 32 different temporal\n\nassessment performance is compared among the proposed\n\nlayers in total were extracted . Temporal layer images with\n\nSR - 3DVQA test is conducted subsequently . Finally , the\n\npixel intensity are directly employed as the feature maps in\n\nimpacts of the pooling method and the reference depth video\n\ndictionary learning . These images were then divided into\n\nare evaluated .\n\npatches with size 8x8 and one - pixel overlap , which were\n\ncollected as the training dataset for temporal dictionary\n\nA. Canny Threshold Determination\n\nlearning .\n\n[ 0120 ]\n\nIt is important to choose a suitable Canny threshold\n\n[ 0124 ]\n\n2 ) Dataset and Settings for SR - 3DVQA Prediction :\n\nin depth edge detection for flicker area detection . In this\n\nThe SIAT synthesized video database of X. Liu , Y. Zhang ,\n\nexample , the edge detection effects among different canny\n\nS. Hu , S. Kwong , C. C. J. Kuo and Q. Peng , \u201c Subjective and\n\nthresholds are compared .\n\nobjective video quality assessment of 3D synthesized views\n\n[ 0121 ] FIGS . 10A to 121 demonstrate the relationship\n\nwith texture / depth compression distortion , IEEE Trans .\n\nImage Process . , vol . 24 , no . 12 , pp . 4847-4861 , December\n\nbetween the flicker areas in the synthesized view and the\n\nCanny thresholds in depth edge detection on sequence\n\n2015 was adopted as the testing dataset . This testing dataset\n\nis totally different from the learning dataset . This testing\n\n\u201c Balloons \" ( FIGS . 10A to 101 ) , \u201c Lovebird1 \" ( FIGS . 11A to\n\ndataset consists of 10 MVD sequences and 140 synthesized\n\n111 ) , and \" Undodancer \u201d ( FIGS . 12A to 121 ) . FIGS . 10A ,\n\nHA , and 12A are the respective synthesized textural image ,\n\nvideos in 1024x768 and 1920x1088 resolution which were\n\nobtained by 14 different combinations of compressed tex\n\nFIGS . 10B , 11B , and 12B are the respective depth image ,\n\nture / depth videos , namely generated with different quanti\n\nand FIGS . 10C - 101 , 11C - 111 , 12C - 121 are edge maps gen\n\nzation parameters . Each video was synthesized by two views\n\nerated by Canny edge detectors with different thresholds\n\ncomposed of two texture videos and their corresponding\n\nfrom 0.03 to 0.5 , respectively . The marked rectangles in the\n\ndepth videos . Depending on whether the texture / depth video\n\nFIGS . 10A to 121 are the area with the flicker distortion .\n\nis \u201c compressed \u201d or \u201c uncompressed \u201d , the generated distorted\n\nWhile selecting an optimal threshold , the depth edges in the\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\ncalculated through worstcase pooling strategy, and the over- all spatial-temporal distortion score of the whole video is obtained as\n\nrectangles (corresponding to flicker areas), shall all be detected, and the edges outside the rectangles shall not be included (or included as few as possible). It can be observed that edge maps generated by thresholds from 0.1 to 0.5 are more consistent to this flicking area for sequence \u201cBal- loons\u201d, where strong edges of balloons and ribbons inside the rectangles are detected and less edges of plants and the man outside the rectangles are detected. In this case, thresh- olds among range 0.1 to 0.5 seem suitable for sequence \u201cBalloons\u201d. Similar result for \u201cLovebirds\u201d. For \u201cUndodancer\u201d, the flicker mainly concentrates on the upper body of the dancer and the rightmost and foremost pillar, so thresholds among range 0.07 to 0.2 are feasible. According to results among the test sequences, threshold 0.2 is selected\n\nsay = \u4e8c 1 \u3010 \u201dRD el Rae ied (19)\n\nwhere \u4e2d denotes the set of the worst 5% percent S-T tubes a QA-GoP, No denotes the number of tubes in set \u00ae, Ni represents the number of QA-GoP in a test video.\n\nB. Pooling\n\n[0117] A general pooling method combining the summa- tion and the multiplication is explored to integrate the flickering and spatial-temporal distortions in assessing the synthesized video quality, which can be written as\n\nB. Quality Prediction Performance Comparisons\n\n[0122] In this subsection, the training dataset, testing dataset, and settings of the SR-3DVQA model are introduced. Then, the quality prediction performances among different methods (including the SR-3DVQA model) compared.\n\nSp=cx(w SF y+ woSA pax f (SF yxSA yp (20)\n\nwhere c, d are weighted parameters to balance the relative importance of the summation and multiplication pooling items. f(.) denotes the nonlinear map function of the flicker distortion score in multiplication pooling. w,, wa are used to weigh the flicker distortion score and the spatial-temporal activity distortion score in summation pooling. When d is set to zero, Equation (20) is degenerated to the summation. Similarly, when c is set to zero, Equation (20) is degenerated to the multiplication.\n\nto the temporal inconsistency in the generated depth video, the synthesized video rendered from the original texture videos and depth videos may also have noticeable flicker distortion. Therefore, the original view instead of the origi- nal synthesized video is preferred for temporal dictionary learning. For the original texture video in MVD system that has similar temporal properties with the conventional single- view video, the conventional videos from HEVC test sequences were selected as training sequences so as to separate the training sequences from the test sequences. To cover different spatial resolution and content, eight 2D video sequences were selected in the temporal dictionary learning. These videos are entitled \u201cBasketballDrive\u201d, \u201cFourPeople\u201d, \u201cFlowervase\u201d, \u201cJohnny\u201d, \u201cKristenAndSara\u201d, \u201cParkScene\u201d, \u201cRaceHorses\u201d, and \u201cVidyo3\u201d. The properties of the training sequences are shown in FIG. 13. The first 300 or 240 frames of each sequence were kept in training. For each sequence, four temporal layers were extracted at a uniform sampling way along the frame height. Then 32 different temporal layers in total were extracted. Temporal layer images with pixel intensity are directly employed as the feature maps in dictionary learning. These images were then divided into patches with size 8x8 and one-pixel overlap, which were collected as the training dataset for temporal dictionary\n\n[0118] In this embodiment, f=(*)=x, c, d, w,, wa are set as 1, 0, 0.5, 0.5, respectively, which denotes the flicker distor- tion and spatiotemporal activity distortion are summated in the pooling stage. The impacts of the pooling method, weight parameters and mapping function f(.) are discussed in Section III subsection D.\n\nIll. Experimental Results and Analyses\n\n[0119] Experiments and simulations have been performed to assess the performance of the method of the above embodiments. This section first presents the Canny thresh- old determination for edge detection. Then, the quality assessment performance is compared among the proposed SR-3DVQA test is conducted subsequently. Finally, the impacts of the pooling method and the reference depth video are evaluated.\n\nA. Canny Threshold Determination\n\n[0120] It is important to choose a suitable Canny threshold depth edge detection for flicker area detection. In this example, the edge detection effects among different canny thresholds are compared.\n\n[0124] 2) Dataset and Settings for SR-3DVQA Prediction: The SIAT synthesized video database of X. Liu, Y. Zhang, S. Hu, S. Kwong, C. C. J. Kuo and Q. Peng, \u201cSubjective and objective video quality assessment of 3D synthesized views with texture/depth compression distortion,\u201d IEEE Trans. Image Process., vol. 24, no. 12, pp. 4847-4861, December 2015 was adopted as the testing dataset. This testing dataset is totally different from the learning dataset. This testing dataset consists of 10 MVD sequences and 140 synthesized videos in 1024x768 and 1920x1088 resolution which were obtained by 14 different combinations of compressed tex- ture/depth videos, namely generated with different quanti- zation parameters. Each video was synthesized by two views composed of two texture videos and their corresponding depth videos. Depending on whether the texture/depth video\n\n[0121] FIGS. 10A to 12I demonstrate the relationship between the flicker areas in the synthesized view and the Canny thresholds in depth edge detection on sequence \u201cBalloons\u201d (FIGS. 10A to 101), \u201cLovebird1\u201d (FIGS. 1A to 111), and \u201cUndodancer\u201d (FIGS. 12A to 121). FIGS. 10A, HA, and 12A are the respective synthesized textural image, FIGS. 10B, 11B, and 12B are the respective depth image, and FIGS. 10C-101, 11C-111, 12C-12I are edge maps gen- erated by Canny edge detectors with different thresholds from 0.03 to 0.5, respectively. The marked rectangles in the FIGS. 10A to 121 are the area with the flicker distortion. While selecting an optimal threshold, the depth edges in the\n\nin\n\nin\n\nfor the edge detection in this example.\n\nfirst\n\nare\n\n[0123]\n\n1) Settings for Temporal Dictionary Learning: Due\n\nlearning.\n\nis \u201ccompressed\u201d or \u201cuncompressed\u201d, the generated distorted\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n10\n\nsynthesized videos are categorized into four subsets : UJUD ,\n\n[ 0137 ] MP - PSNRr illustrated in D. Sandi? - Stankovi? ,\n\nD. Kukolj , and P. Le Callet , \u201c DIBR - synthesized image\n\nU CD , CUD , and C CD . ' C ' and ' U ' mean the videos are\n\n\" compressed \" and \" uncompressed \u201d respectively while the\n\nquality assessment based on morphological multi - scale\n\napproach , \" EURASIP J. Image Video Process . , vol . 1 ,\n\nsubscripts \u2018 T ' and ' D ' denote texture videos and depth\n\nvideos respectively . Taking CC for example , it represents\n\npp . 1-23 , March 2017\n\nthe synthesized video subset were synthesized from the\n\n[ 0138 ] MW - PSNRr illustrated in D. Sandi? - Stankovi? ,\n\nD. Kukolj , and P. Le Callet , \u201c DIBR - synthesized image\n\ntexture and depth videos with compression distortions . The\n\nsubjective experiment was conducted by single stimulus\n\nquality assessment based on morphological multi - scale\n\napproach , \" EURASIP J. Image Video Process . , vol . 1 ,\n\nparadigm with continuous score . Difference Mean Opinion\n\nScores ( DMOS ) were provided . The parameter settings in\n\npp . 1-23 , March 2017\n\n[ 0139 ] 3DSWIM illustrated in F. Battisti , E. Bosc , M.\n\nthe SR - 3DVQA method of the one embodiment are as\n\nfollows : the dictionary size is 64x256 , the sparsity ? for\n\nCarli , and P. L. Callet , \u201c Objective image quality assess\n\ntraining and testing are both 6 , the patch size in the temporal\n\nment of 3D synthesized views , \u201d Signal Process . Image\n\nlayer is 8x8 , the layer size h , is 8 , the patch variance\n\nCommun . , vol . 30 , pp . 78-88 , January 2015\n\nthreshold g is 5 , the edge number threshold B is 1 , the canny\n\n[ 0140 ] LOGS illustrated in L. Li , Y. Zhou , K. Gu , W.\n\nLin and S. Wang , \u201c Quality assessment of DIBR - syn\n\nthreshold is set as 0.2 , and the dilation mask is 2x2 .\n\nthesized images by measuring local geometric distor\n\n[ 0125 ] The comparison methods include three categories :\n\ntions and global sharpness , \u201d IEEE Trans . Multimedia ,\n\neight conventional 2D IQANVQA metrics :\n\nvol . 20 , no . 4 , pp . 914-926 , April 2018\n\n[ 0126 ] Peak signal - to - noise ratio ( PSNR )\n\nand three synthesized VQA metrics :\n\n[ 0127 ] SSIM illustrated in Z. Wang , A. C. Bovik , H. R.\n\n[ 0141 ] PSPTNR illustrated in Y. Zhao and L. Yu , \u201c A\n\nSheikh , and E. P. Simoncelli , \u201c Image quality assess\n\nperceptual metric for evaluating quality of synthesized\n\nment : From error visibility to structural similarity ,\n\nsequences in 3DV system , \u201d Proc . SPIE , vol . 7744 , pp .\n\nIEEE Trans . Image Process . , vol . 13 , no . 4 , pp . 600\n\n77440X , August 2010\n\n612 , April 2004\n\n[ 0142 ]\n\nLiu illustrated in X. Liu , Y. Zhang , S. Hu , S.\n\n[ 0128 ] WSNR illustrated in N. Damera - Venkata , T. D.\n\nKwong , C. C. J. Kuo and Q. Peng , \u201c Subjective and\n\nKite , W. S. Geisler , B. L. Evans , and A. C. Bovik ,\n\nobjective video quality assessment of 3D synthesized\n\n\u201c Image quality assessment based on a degradation\n\nviews with texture / depth compression distortion ,\n\nmodel , \u201d IEEE Trans . Image Process . , vol . 9 , no . 4 , pp .\n\nIEEE Trans . Image Process . , vol . 24 , no . 12 , pp .\n\n636-650 , April 2000\n\n4847-4861 , December 2015\n\n[ 0129 ] MSSSIM illustrated in Z. Wang , E. P. Simon\n\n[ 0143 ] FDI illustrated in Y. Zhou , L. Li , S. Wang , J. Wu ,\n\ncelli , and A. C. Bovik , \u201c Multiscale structural similarity\n\nY. Zhang , \u201c No - reference quality assessment of DIBR\n\nfor image quality assessment , \" in Proc . Conf . Rec . 37th\n\nsynthesized videos by measuring temporal flickering , \"\n\nAsilomar Conf . Signals , Syst . Comput . , vol . 2. Novem\n\nJ. Vis . Commun . Image R. , vol . 55 , pp . 30-39 , August\n\nber 2003 , pp . 1398-1402\n\n2018\n\n[ 0130 ]\n\nIW - SSIM illustrated in Z. Wang and Q. Li ,\n\nNote for the IQA metrics , the score of each video was\n\n\" Information content weighting for perceptual image\n\nobtained by averaging the scores of all the frames in the\n\nquality assessment , \u201d IEEE Trans\n\nvideo . In addition , the results of FDI , a non - reference\n\n[ 0131 ]\n\nIW - PSNR illustrated in Z. Wang and Q. Li ,\n\nmethod , are referred from Y. Zhou , L. Li , S. Wang , J. Wu ,\n\n\" Information content weighting for perceptual image\n\nY. Zhang , \u201c No - reference quality assessment of DIBR - syn\n\nquality assessment , \u201d IEEE Trans\n\nthesized videos by measuring temporal flickering , \" J. Vis .\n\n[ 0132 ] VQM illustrated in M. H. Pinson and S. Wolf , \u201c A\n\nCommun . Image R. , vol . 55 , pp . 30-39 , August 2018 with the\n\nnew standardized method for objectively measuring\n\nwhole SIAT synthesized video database .\n\nvideo quality , \u201d IEEE Trans . Broadcast . , vol . 50 , no . 3 ,\n\n[ 0144 ] To measure the performance of the quality assess\n\npp . 312-322 , September 2004\n\nment , Spearman Rank Order Correlation Coefficient\n\n( SROCC ) , Pearson Linear Correlation Coefficient ( PLCC ) ,\n\n[ 0133 ] MOVIE illustrated in K Seshadrinathan and A.\n\nC. Bovik ,\n\n\u201c Motion tuned spatio - temporal quality\n\nand Root Mean Squared Error ( RMSE ) were used . The\n\nfive - parameter nonlinear regression function was used ,\n\nassessment of natural videos , \u201d IEEE Trans . Image\n\nProcess . , vol . 19 , no . 2 , pp . 335-350 , February 2010\n\nwhich is\n\nseven 3D synthesized IQA metrics :\n\n[ 0134 ] Bosc illustrated in E. BoscR . Pepion , P. L. Cal\n\n= n ( - 1 + ezavas + 14x + 75\n\n( 21 )\n\nlet , M. Koppel , P. Ndjiki - Nya , M. Pressigout and L.\n\nf ( x ) = ni\n\n1 + \u20ac 172 ( x \u2013 175 )\n\nMorin , \u201c Towards a new quality metric for 3D synthe\n\nsized view assessment , \u201d IEEE J. Sel . Topics Signal\n\nProcess . , vol . 5 , no . 7 , PP . 1332-1343 , November 2011\n\nwhere ni to ns are fitting parameters , x denotes the objective\n\n[ 0135 ] MP - PSNR illustrated in D. Sandi? - Stankovi? , D.\n\nscore of the quality metrics , fix ) is the predicted subjective\n\nKukolj , and P. Le Callet , \u201c Multi - scale synthesized view\n\nscore obtained by nonlinearly fitting x to range [ 0 , 1 ] . The\n\nassessment based on morphological pyramids , \" J.\n\nscores of all the comparison methods would employ Equa\n\nElect . Eng . , vol . 67 , no . 1 , pp . 3-11 , 2016\n\ntion ( 21 ) to map the objective scores to the predicted DMOS .\n\n[ 0145 ] FIG . 14 shows the performance comparison\n\n[ 0136 ] MW - PSNR illustrated in D. Sandi? - Stankovi? ,\n\nD. Kukolj , and P. Le Callet , \u201c DIBR synthesized image\n\nbetween the benchmark methods ( listed above ) and the\n\nquality assessment based on morphological wavelets ,\n\nSR - 3DVQA method of the present embodiment on the SIAT\n\nin Proc . IEEE 7th Int . Workshop Quality Multimedia\n\ndatabase , which includes three subsets U / CD , CUD CCD ,\n\nExper . ( QOMEX ) , May 2015\n\nand the ALL dataset consists of the three subsets . Video\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\nsynthesized videos are categorized into four subsets: UzUm\uff0c U,Cp, C,Up, and CrCp \u2018C\u2019 and \u2018U\u2019 mean the videos are \u201ccompressed\u201d and \u201cuncompressed\u201d respectively while the subscripts \u2018T\u2019 and \u2018D\u2019 denote texture videos and depth videos respectively. Taking C;C, for example, it represents the synthesized video subset were synthesized from the texture and depth videos with compression distortions. The subjective experiment was conducted by single stimulus paradigm with continuous score. Difference Mean Opinion Scores (DMOS) were provided. The parameter settings in the SR-3DVQA method of the one embodiment are as follows: the dictionary size is 64256, the sparsity ge for training and testing are both 6, the patch size in the temporal layer is 8x8, the layer size h, is 8, the patch variance threshold g is 5, the edge number threshold B is 1, the canny threshold is set as 0.2, and the dilation mask is 2x2.\n\nD. Kukolj, and P. Le Callet, \u201cD/BR-synthesized image quality assessment based on morphological multi-scale approach,\u201d EURASIP J. Image Video Process., vol. 1, pp. 1-23, March 2017\n\nD. Kukolj, and P. Le Callet, \u201cD/BR-synthesized image quality assessment based on morphological multi-scale approach,\u201d EURASIP J. Image Video Process., vol. 1, 1-23, March 2017\n\n[0139] 3DSwIM illustrated in F. Battisti, E. Bosc, M. Carli, and P. L. Callet, \u201cObjective image quality assess- ment of 3D synthesized views,\u201d Signal Process. Image Commun., vol. 30, pp. 78-88, January 2015\n\n[0140] LOGS illustrated in L. Li, Y. Zhou, K. Gu, W. Lin and S. Wang, \u201cQuality assessment of DIBR-syn- thesized images by measuring local geometric distor- tions and global sharpness,\u201d IEEE Trans. Multimedia, vol. 20, no. 4, pp. 914-926, April 2018\n\n[0125] The comparison methods include three categories: conventional 2D IQA/VQA metrics:\n\n[0126] Peak signal-to-noise ratio (PSNR)\n\nand three synthesized VQA metrics:\n\nWang, Sheikh, and E. P. Simoncelli, \u201cJmage quality assess- ment: From error visibility to structural similarity,\u201d IEEE Trans. Image Process., vol. 13, no. 4, pp. 600- 612, April 2004\n\n[0141] PSPTNR illustrated in Y Zhao and L. Yu, \u201c4 perceptual metric for evaluating quality of synthesized sequences in 3DV system,\u201d Proc. SPIE, vol. 7744, pp. 77440X, August 2010\n\n[0142] Liu illustrated in X. Liu, Y. Zhang, S. Hu, S. Kwong, C. C. J. Kuo and Q. Peng, \u201cSubjective and objective video quality assessment of 3D synthesized views with texture/depth compression distortion,\u201d IEEE Trans. Image Process., vol. 24, no. 12, pp. 4847-4861, December 2015\n\nKite, W. S. Geisler, B. L. Evans, and A. C. Bovik, \u201cImage quality assessment based on a degradation model,\u201d IEEE Trans. Image Process., vol. 9, no. 4, pp. 636-650, April 2000\n\ncelli, and A. C. Bovik, \u201cMultiscale structural similarity for image quality assessment,\u201d in Proc. Conf. Rec. 37th Asilomar Conf: Signals, Syst. Comput., vol. 2. Novem- ber 2003, pp. 1398-1402\n\n[0143] FDI illustrated in Y. Zhou, L. Li, S. Wang, J. Wu, Y. Zhang, \u201cNo-reference quality assessment of DIBR- synthesized videos by measuring temporal flickering,\u201d J. Vis. Commun. Image R., vol. 55, pp. 30-39, August 2018\n\n[0130] IW-SSIM illustrated in Z. Wang and Q. Li, \u201cInformation content weighting for perceptual image quality assessment,\u201d IEEE Trans\n\nNote for the IQA metrics, the score of each video was obtained by averaging the scores of all the frames in the video. In addition, the results of FDI, a non-reference method, are referred from Y. Zhou, L. Li, S. Wang, J. Wu, Y. Zhang, \u201cNo-reference quality assessment of DIBR-syn- thesized videos by measuring temporal flickering,\u201d J. Vis. Commun. Image R., vol. 55, pp. 30-39, August 2018 with the whole SIAT synthesized video database.\n\n[0131] IW-PSNR illustrated in Z. Wang and Q. Li, \u201cInformation content weighting for perceptual image quality assessment,\u201d IEEE Trans\n\n[0132] VQM illustrated in M. H. Pinson and S. Wolf, \u201c4 new standardized method for objectively measuring video quality,\u201d IEEE Trans. Broadcast., vol. 50, no. 3, pp. 312-322, September 2004\n\n[0144] To measure the performance of the quality assess- ment, Spearman Rank Order Correlation Coefficient (SROCC), Pearson Linear Correlation Coefficient (PLCC), and Root Mean Squared Error (RMSE) were used. The five-parameter nonlinear regression function was used, which is\n\n[0133] MOVIE illustrated in K Seshadrinathan and A. C. Bovik, \u201cMotion tuned spatio-temporal quality assessment of natural videos,\u201d IEEE Trans. Image Process., vol. 19, no. 2, pp. 335-350, February 2010\n\n[0134] Bosc illustrated in E. BoscR. Pepion, P. L. Cal- let, M. Koppel, P. Ndjiki-Nya, M. Pressigout and L. Morin, \u201cTowards a new quality metric for 3D synthe- sized view assessment,\u201d IEEE J. Sel. Topics Signal Process., vol. 5, no. 7, PP. 1332-1343, November 2011\n\n1 Te CD) sor=m(5 Jena\n\nwhere 1), to 1), are fitting parameters, x denotes the objective score of the quality metrics, fix) is the predicted subjective score obtained by nonlinearly fitting x to range [0, 1]. The scores of all the comparison methods would employ Equa- (21) to the objective scores to the predicted DMOS.\n\n[0135] MP-PSNR illustrated in D. Sandi\u00e9-Stankovi\u00e9, Kukolj, and P. Le Callet, \u201cMulti-scale synthesized view assessment based on morphological pyramids,\u201d Elect. Eng., vol. 67, no. 1, pp. 3-11, 2016\n\n[0136] MW-PSNR illustrated in D. Sandi\u00e9-Stankovic, D. Kukolj, and P. Le Callet, \u201cDIBR synthesized image quality assessment based on morphological wavelets,\u201d in Proc. IEEE 7th Int. Workshop Quality Multimedia Exper. (QoMEX), May 2015\n\n[0145] FIG. 14 shows the performance comparison between the benchmark methods (listed above) and the SR-3DVQA method of the present embodiment on the SIAT database, which includes three subsets U,;Cp, C7Up, CzCp, and the ALL dataset consists of the three subsets. Video\n\neight\n\n[0127]\n\nSSIM illustrated in Z.\n\nA. C. Bovik, H. R.\n\n[0128]\n\nWSNR illustrated in N. Damera-Venkata, T. D.\n\n[0129]\n\nMSSSIM illustrated in Z. Wang, E. P Simon-\n\nseven 3D synthesized IQA metrics:\n\nD.\n\nJ.\n\n[0137]\n\nMP-PSNRr illustrated in D. Sandi\u00e9-Stankovic,\n\n[0138]\n\n_MW-PSNRr illustrated in D. Sandi\u00e9-Stankovic,\n\npp.\n\ntion\n\nmap\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n11\n\nsamples in U7U are included into C / CD . For all perfor\n\nmethod of the present embodiment on the four datasets , the\n\nsignificance test results can be obtained . The variance ratios\n\nmance indices SROCC , PLCC , and RMSE on different\n\nand significance results are listed in column as \u2018 R ; , / sig . ' in\n\nsubsets and ALL dataset , the best method is marked in bold .\n\nAs shown in FIG . 14 , for U Cp dataset where the depth\n\nFIG . 15 , where Rij is the variance ratio and sig . is the\n\nvideo was distorted , conventional 2D metrics IW - SSIM and\n\nsignificance result . The symbol \u2018 l \u2019 ,\n\n\u2018 -1 ' , or \u201c O\u2019denote the\n\nMSSSIM perform better than other benchmark schemes .\n\nproposed method is \u2018 significantly superior \u2019 , \u2018 significantly\n\nThis is because the depth distortion causes the geometrical\n\ninferior \u2019 , or ' insignificant ' to the benchmark method in the\n\ndistortion in the rendered view , and conventional 2D metric ,\n\nrow , respectively . It can be observed that on CUD , the\n\nsuch as PSNR , may overestimate the geometrical distortion .\n\nSR - 3DVQA method of the present embodiment is signifi\n\nThe performance of LOGS , a metric proposed for 3D\n\ncantly superior to all benchmark schemes except for Liu's\n\nsynthesized image , follows the IW - SSIM and MSSSIM . The\n\nscheme . On C Co subset , except for Liu , MSSSIM , and\n\nPLCC and SROCC values of Liu and the SR - 3DVQA\n\nIW - SSIM , the SR - 3DVQA method of the present embodi\n\nmethod of the present embodiment are much higher than all\n\nment is significantly superior to all other methods . Liu ,\n\nthe rest methods . The method of the present embodiment has\n\nMSSSIM , and IW - SSIM may be comparable in evaluating\n\nthe highest performance with dominant superiority . This\n\nthe synthesized videos whose distortions are coming from\n\nindicates that the method of the present embodiment can\n\ncolor videos . In addition , the SR - 3DVQA method of the\n\npredict the flicker distortion very well and have better\n\npresent embodiment has significantly superior performance\n\nconsistency with human perception . For C Up , three 2D\n\nthan all the other methods on the ALL dataset and subset\n\nquality metrics IW - SSIM , WSNR , and VQM are good , since\n\nU C . It is because the SR - 3DVQA method of the present\n\nthey are designed for compression and structural distortion\n\nembodiment is mainly proposed to evaluate the flicker\n\nfor 2D images / videos which are probably the main distor\n\ndistortion in synthesized video caused by the depth map and\n\ntions in C UD . All the 3D synthesized image / video metrics\n\nit works very well . Overall , the significance test has further\n\ndidn't perform well except Liu and the method of the present\n\nvalidates the superiority of the SR - 3DVQA method of the\n\nembodiment . This is because the other methods have not\n\npresent embodiment in predicting the quality of the synthe\n\nconsidered the distortion induced by the compressed texture\n\nsized videos .\n\nvideos . Similar to U Cd , the SR - 3DVQA method of the\n\npresent embodiment performs the best in term of the PLCC ,\n\nD. The Impacts of Pooling Methods\n\nSROCC and RMSE . Similarly , on C , CD , the SR - 3DVQA\n\nmethod of the present embodiment performs the best among\n\n[ 0148 ] The pooling methods of flicker distortion measure\n\nthem while Liu and IW - SSIM have very similar perfor\n\nment and spatial - temporal activity distortion measurement\n\nmance , and MSSSIM and SSIM perform fairly good and are\n\nare analyzed in this subsection . For Equation ( 20 ) , when ( c ,\n\nbetter than other methods . In the ALL dataset , IW - SSIM\n\nd ) was set as ( 1 , 0 ) , i.e. , the summation pooling , the effects\n\nranks third after Liu and the SR - 3DVQA method of the\n\nof weight parameter pair ( W1 , W2 ) on the performance in\n\npresent embodiment . Liu's method ranks the second while\n\nquality assessment were explored . ( W1 , W2 ) were set as ( 0.5 ,\n\nthe SR - 3DVQA method of the present embodiment is the\n\n0.5 ) , ( 0.6 , 0.4 ) , ( 0.8 , 0.2 ) , ( 0.4 , 0.6 ) , and ( 0.2 , 0.8 ) . The\n\nbest among all benchmark schemes considered .\n\nperformance measured by PLCC , SROCC , and RMSE of the\n\nfive combinations of ( W1 , W2 ) is listed in the second to sixth\n\nC. Statistical Significance Test for SR - 3DVQA\n\nrow in the table of FIG . 16. It can be observed that ( 0.5 , 0.5 )\n\nhas better performance than other ( W1 , W2 ) pairs in term of\n\n[ 0146 ] Moreover , to further verify the effectiveness of the\n\nPLCC , SROCC and RMSE . For the multiplication pooling ,\n\nproposed method , statistical significance test was per\n\ni.e. , ( c , d ) was set as ( 0 , 1 ) , the role of the map function f ( )\n\nformed . F - test based on the result of the variance ratio of the\n\nwas analyzed by comparing the performance of six types of\n\npredicted residuals between two methods was used to indi\n\nf ( . ) , i.e. , f ( x ) = x , \u2018 log 10 ' ,\n\n' log 2 \u2019 \u2018 cubic \u2019 , \u201c square ' , ' square\n\ncate the significance . The predicted residual is obtained from\n\nroot . The corresponding results are demonstrated in the\n\nDMOSp predicted by test model and the ground truth\n\neighth to fifteen rows in FIG . 16. It is noted that function\n\nDMOS , which can be described as\n\n\u2018 square ' excels all the other five functions . But the multi\n\nres ( k ) = DMOSP ( k ) -DMOS ( k )\n\n( 22 )\n\nplication pooling is a little inferior to the summation pooling\n\neven with different mapping function f ( x ) . Based on the best\n\nwhere res ( k ) represents the predicted residual of the test\n\nperformance of summation and multiplication pooling , the\n\nmodel on video k . The variance of the residuals , termed as\n\nweight parameters ( c , d ) in the combination of the two\n\nVar ;, of the test model i on all the videos could be calculated .\n\nmethods had also been investigated . The values of ( c , d )\n\nThen , the variance ratio Rij between test models i and j\n\nwere set the same range as ( W1 , W2 ) . The last five rows in\n\ncould be computed , which could be written as\n\nFIG . 16 show the performance . It can be found that when the\n\nvalue of c is equal or greater than d , it achieves the best\n\nperformance among the five ( c , d ) combinations . In fact , the\n\nVari\n\n( 23 )\n\nRij\n\nbest performance is obtained via the summation pooling\n\nVar ;\n\nwith ( w , W2 ) as ( 0.5 , 0.5 ) or the combination pooling when\n\nc is larger than 0.6 . Overall , the pooling methods have\n\nnoticeable impacts on the final performance ; the best per\n\n[ 0147 ]\n\nIf Ri , j is greater than the F - ratio threshold which is\n\ndetermined by the sample size and the significance level , it\n\nformance is obtained via the summation pooling with ( W1 ,\n\nmeans the performance of test model j is significantly\n\nW2 ) as ( 0.5 , 0.5 ) and it is simpler form as compared the\n\nsuperior to that of test model i ; otherwise , the difference is\n\ncombination pooling . Therefore , the summation pooling is\n\ninsignificant . Based on the variance ratios between the\n\nemployed and ( W1 , W2 ) is set as ( 0.5 , 0.5 ) in the SR - 3DVQA\n\nmethod of the present embodiment .\n\nabove listed benchmark schemes and the SR - 3DVQA\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\nsignificance test results can be obtained. The variance ratios and significance results are listed in column as \u2018R, /sig.\u2019 in FIG, 15, where Riy is the variance ratio and sig. is the significance result. The symbol \u20181\u2019, \u2018-1\u2019, or \u20180\u2019 denote the proposed method is \u2018significantly superior\u2019, \u2018significantly inferior\u2019, or \u2018insignificant\u2019 to the benchmark method in the row, respectively. It can be observed that on C,U,, the SR-3DVQA method of the present embodiment is signifi- cantly superior to all benchmark schemes except for Liu\u2019s scheme. On C,C, subset, except for Liu, MSSSIM, and IW-SSIM, the SR-3DVQA method of the present embodi- ment is significantly superior to all other methods. Liu, MSSSIM, and IW-SSIM may be comparable in evaluating the synthesized videos whose distortions are coming from color videos. In addition, the SR-23DVQA method of the present embodiment has significantly superior performance than all the other methods on the ALL dataset and subset U,Cp. It is because the SR-3DVQA method of the present embodiment is mainly proposed to evaluate the flicker distortion in synthesized video caused by the depth map and. it works very well. Overall, the significance test has further validates the superiority of the SR-3DVQA method of the present embodiment in predicting the quality of the synthe-\n\nmance SROCC, PLCC, on subsets and ALL dataset, the best method is marked in bold. As shown in FIG. 14, for U,C,, dataset where the depth video was distorted, conventional 2D metrics TW-SSIM and MSSSIM perform better than other benchmark schemes. This is because the depth distortion causes the geometrical distortion in the rendered view, and conventional 2D metric, such as PSNR, may overestimate the geometrical distortion. The performance of LOGS, a metric proposed for 3D synthesized image, follows the IW-SSIM and MSSSIM. The PLCC and SROCC values of Liu and the SR-3DVQA method of the present embodiment are much higher than all the rest methods. The method of the present embodiment has the highest performance with dominant superiority. This indicates that the method of the present embodiment can predict the flicker distortion very well and have better consistency with human perception. For CU, three 2D quality metrics |W-SSIM, WSNR, and VQM are good, since they are designed for compression and structural distortion for 2D images/videos which are probably the main distor- tions in C,U,. All the 3D synthesized image/video metrics didn\u2019t perform well except Liu and the method of the present embodiment. This is because the other methods have not considered the distortion induced by the compressed texture videos. Similar to U;Cp, the SR-3DVQA method of the present embodiment performs the best in term of the PLCC, SROCC and RMSE. Similarly, on C7C,, the SR-3DVQA method of the present embodiment performs the best among them while Liu and IW-SSIM have very similar perfor- mance, and MSSSIM and SSIM perform fairly good and are better than other methods. In the ALL dataset, [W-SSIM ranks third after Liu and the SR-3DVQA method of the present embodiment. Liu\u2019s method ranks the second while\n\nD. The Impacts of Pooling Methods\n\nspatial-temporal activity are analyzed in this subsection. For Equation (20), when (c, d) was set as (1, 0), i.e., the summation pooling, the effects of weight parameter pair (wi, w2) on the performance in quality assessment were explored. (wi, w.) were set as (0.5, 0.5), (0.6, 0.4), (0.8, 0.2), (0.4, 0.6), and (0.2, 0.8). The performance measured by PLCC, SROCC, and RMSE of the five combinations of (w,, wa) is listed in the second to sixth row in the table of FIG. 16. It can be observed that (0.5, 0.5) has better performance than other (wi, w) pairs in term of PLCC, SROCC and RMSE. For the multiplication pooling, je (c, d) was set as (0, 1), the role of the map function f(.) was analyzed by comparing the performance of six types of (), Le, \u4e86 CO)=x\uff0c \u2018log 10\u2019, \u2018log 2\u2019 \u2018cubic\u2019, \u2018square\u2019, \u2018square root\u2019. The corresponding results are demonstrated in the eighth to fifteen rows in FIG. 16. It is noted that function \u2018square\u2019 excels all the other five functions. But the multi- plication pooling is a little inferior to the summation pooling even with different mapping function f(x). Based on the best erformance of summation and multiplication pooling, the weight parameters (c, d) in the combination of the two methods had also been investigated. The values of (c, d) were set the same range as (W,, w2). The last five rows in FIG. 16 show the performance. It can be found that when the value of c is equal or greater than d, it achieves the best performance among the five (c, d) combinations. In fact, the best performance is obtained via the summation pooling with (w,, wa) as (0.5, 0.5) or the combination pooling when cis larger than 0.6. Overall, the pooling methods have noticeable impacts on the final performance; the best per- formance is obtained via the summation pooling with (w,, wa) as (0.5, 0.5) and it is simpler form as compared the combination pooling. Therefore, the summation pooling is\n\nC. Statistical Significance Test for SR-3DVQA\n\n[0146] Moreover, to further verify the effectiveness of the proposed method, statistical significance test was per- formed. F-test based on the result of the variance ratio of the predicted residuals between two methods was used to indi- cate the significance. The predicted residual is obtained from DMOS, predicted by test model and the ground truth DMOS, which can be described as\n\nres(k)=DMOS,(4)-DMOS(k) (22)\n\nwhere res(k) represents the predicted residual of the test model on video k. The variance of the residuals, termed as Var,, of the test model i on all the videos could be calculated. Then, the variance ratio Riy between test models i and j could be computed, which could be written as\n\n_ Var; (23) \u201ciS Var;\n\n[0147] IfR,,, is greater than the F-ratio threshold which is determined by the sample size and the significance level, it means the performance of test model j is significantly superior to that of test model i; otherwise, the difference is insignificant. Based on the variance ratios between the above listed benchmark schemes and the SR-3DVQA\n\nsamples in UzrUp are included into C;C,. For all perfor- indices and RMSE different\n\nthe SR-3DVQA method of the present embodiment is the best among all benchmark schemes considered.\n\nmethod of the present embodiment on the four datasets, the\n\nsized videos.\n\n[0148] The pooling methods of flicker distortion measure- ment and distortion measurement\n\nemployed and (w,, w,) is set as (0.5, 0.5) in the SR-3DVQA method of the present embodiment.\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n12\n\nmonitor ) , speakers , disk drives , headphones , earphones ,\n\nE. Impacts of the Reference Depth Video\n\nprinters , 3D printers , etc. The display may include a LCD\n\n[ 0149 ] The impacts from the reference depth video\n\ndisplay , a LED / OLED display , or any other suitable display\n\nemployed in the SR - 3DVQA model of the present embodi\n\nthat may or may not be touch sensitive . The information\n\nment were also investigated . The depth videos employed can\n\nhandling system 200 may further include one or more disk\n\nbe the original depth video or the synthesized depth video at\n\ndrives 212 which may encompass solid state drives , hard\n\nthe virtual viewpoint . The advantage of using the original\n\ndisk drives , optical drives , flash drives , and / or magnetic tape\n\ndepth video is it has better picture quality as compared with\n\ndrives . A suitable operating system may be installed in the\n\nusing the synthesized depth video . However , the disadvan\n\ninformation handling system 200 , e.g. , on the disk drive 212\n\ntage is the original depth video at the virtual viewpoint may\n\nor in the memory unit 204. The memory unit 204 and the\n\nnot available . Using the synthesized depth video may be\n\ndisk drive 212 may be operated by the processor 202. The\n\nmore practical in some applications .\n\ninformation handling system 200 also preferably includes a\n\n[ 0150 ] A comparative experiment was conducted to ana\n\ncommunication device 210 for establishing one or more\n\nlyze the influence from different reference depth videos used\n\ncommunication links ( not shown ) with one or more other\n\nin the method of the present embodiment . Since sequence\n\ncomputing devices such as servers , personal computers ,\n\n\u201c Lovebirds \u201d and \u201c Newspaper \u201d do not have corresponding\n\nterminals , tablets , phones , or other wireless or handheld\n\noriginal depth video at the virtual viewpoint , all the rest\n\ncomputing devices . The communication device 210 may be\n\neight sequences in the database were used for comparison .\n\na modem , a Network Interface Card ( NIC ) , an integrated\n\nSimilarly , the testing database is also categorized as four\n\nnetwork interface , a radio frequency transceiver , an optical\n\ndatasets , U / CD , CUDC CD , and ALL dataset . The PLCC ,\n\nport , an infrared port , a USB connection , or other wired or\n\nSROCC , RMSE results are demonstrated in FIG . 17. To\n\nwireless communication interfaces . The communication\n\ndistinguish these reduced datasets from those in Section III\n\nlinks may be wired or wireless for communicating com\n\nsubsection B , they are marked with \u201c * \u201d . The left three\n\nmands , instructions , information and / or data . Preferably , the\n\ncolumns are the results from the synthesized depth video . As\n\nprocessor 202 , the memory unit 204 , and optionally the\n\nshown in FIG . 17 , the values of most PLCC , SROCC and\n\ninput devices 206 , the output devices 208 , the communica\n\nRMSE on the test datasets are a little better than the results\n\ntion device 210 and the disk drives 212 are connected with\n\nof using the original depth video . Basically , they are com\n\neach other through a bus , a Peripheral Component Intercon\n\nparable . It indicates that although the original depth video\n\nnect ( PCI ) such as PCI Express , a Universal Serial Bus\n\nhas more precise depth values , the synthesized depth video ,\n\n( USB ) , an optical bus , or other like bus structure . In one\n\nwhich are generated through DIBR from two original depth\n\nembodiment , some of these components may be connected\n\nvideos , is comparable or a little better in the synthesized\n\nthrough a network such as the Internet or a cloud computing\n\nvideo quality prediction . Moreover , using the synthesized\n\nnetwork . A person skilled in the art would appreciate that the\n\ndepth video is more practical . The main reason is the depth\n\ninformation handling system 200 shown in FIG . 18 is merely\n\nvideo is used to help locate the flicker area by using edge\n\nexemplary and different information handling systems 200\n\ndetection and dilation . The original depth video has more\n\nwith different configurations may be applicable to imple\n\nprecise depth values but may have geometrical misalign\n\nment the method of the present invention .\n\nment with the synthesized texture video . Therefore , the\n\n[ 0152 ] Although not required , the embodiments described\n\nsynthesized depth video may be better to be used for both\n\nwith reference to the Figures can be implemented as an\n\npractical usage and better performance .\n\napplication programming interface ( API ) or as a series of\n\nlibraries for use by a developer or can be included within\n\nIV . Exemplary System\n\nanother software application , such as a terminal or personal\n\ncomputer operating system or a portable computing device\n\n[ 0151 ] FIG . 18 shows a schematic diagram of an exem\n\noperating system . Generally , as program modules include\n\nplary information handling system 200 that can be used , in\n\nroutines , programs , objects , components and data files\n\nsingle or in multiple , to implement one or more of the above\n\nassisting in the performance of particular functions , the\n\nmethod embodiments in one embodiment of the invention .\n\nskilled person will understand that the functionality of the\n\nThe information handling system 200 may have different\n\nsoftware application may be distributed across a number of\n\nconfigurations , and it generally includes suitable compo\n\nroutines , objects or components to achieve the same func\n\nnents necessary to receive , store , and execute appropriate\n\ntionality desired herein .\n\ncomputer instructions , commands , or codes . The main com\n\n[ 0153 ]\n\nIt will be appreciated that where the methods and\n\nponents of the information handling system 200 are a\n\nsystems of the invention are either wholly implemented by\n\nprocessor 202 and a memory unit 204. The processor 202\n\ncomputing system or partly implemented by computing\n\nmay be formed by one or more CPU , MCU , controllers ,\n\nsystems then any appropriate computing system architecture\n\nlogic circuits , Raspberry Pi chip , etc. The memory unit 204\n\nmay be utilized . This will include stand - alone computers ,\n\nmay include one or more volatile memory unit ( such as\n\nnetwork computers , dedicated or non - dedicated hardware\n\nRAM , DRAM , SRAM ) , one or more non - volatile unit ( such\n\ndevices . Where the terms \u201c computing system \u201d and \u201c com\n\nas ROM , PROM , EPROM , EEPROM , FRAM , MRAM ,\n\nputing device \u201d are used , these terms are intended to include\n\nFLASH , SSD , NAND , and NVDIMM ) , or any of their\n\nany appropriate arrangement of computer or information\n\ncombinations . Preferably , the information handling system\n\nprocessing hardware capable of implementing the function\n\n200 further includes one or more input devices 206 such as\n\ndescribed .\n\na keyboard , a mouse , a stylus , an image scanner , a micro\n\n[ 0154 ] The expression original video \u201d used in this dis\n\nphone , a tactile input device ( e.g. , touch sensitive screen ) ,\n\nclosure may refer to 1 ) Captured video : The source captured\n\nand an image / video input device ( e.g. , camera ) . The infor\n\nvideo without compression distortion ; 2 ) Captured video at\n\nmation handling system 200 may further include one or\n\nmore output devices 208 such as one or more displays ( e.g. ,\n\nvirtual viewpoint : The captured source video at the virtual\n\nUS 2021/0044791 Al\n\nFeb. 11, 2021\n\nE. Impacts of the Reference Depth Video\n\nprinters, 3D printers, etc. The display may include a LCD display, a LED/OLED display, or any other suitable display that may or may not be touch sensitive. The information handling system 200 may further include one or more disk drives 212 which may encompass solid state drives, hard disk drives, optical drives, flash drives, and/or magnetic tape drives. A suitable operating system may be installed in the information handling system 200, e.g., on the disk drive 212 or in the memory unit 204. The memory unit 204 and the disk drive 212 may be operated by the processor 202. The information handling system 200 also preferably includes a communication device 210 for establishing one or more communication links (not shown) with one or more other computing devices such as servers, personal computers, terminals, tablets, phones, or other wireless or handheld computing devices. The communication device 210 may be a modem, a Network Interface Card (NIC), an integrated network interface, a radio frequency transceiver, an optical port, an infrared port, a USB connection, or other wired or wireless communication interfaces. The communication links may be wired or wireless for communicating com- mands, instructions, information and/or data. Preferably, the processor 202, the memory unit 204, and optionally the input devices 206, the output devices 208, the communica- tion device 210 and the disk drives 212 are connected with each other through a bus, a Peripheral Component Intercon- nect (PCI) such as PCI Express, a Universal Serial Bus (USB), an optical bus, or other like bus structure. In one embodiment, some of these components may be connected. through a network such as the Internet or a cloud computing network. A person skilled in the art would appreciate that the information handling system 200 shown in FIG. 18 is merely exemplary and different information handling systems 200 with different configurations may be applicable to imple-\n\n[0149] The impacts from the reference depth video employed in the SR-3DVQA model of the present embodi- ment were also investigated. The depth videos employed can be the original depth video or the synthesized depth video at the virtual viewpoint. The advantage of using the original depth video is it has better picture quality as compared with using the synthesized depth video. However, the disadvan- tage is the original depth video at the virtual viewpoint may not available. Using the synthesized depth video may be more practical in some applications.\n\nlyze the influence from different reference depth videos used. in the method of the present embodiment. Since sequence \u201cLovebirds\u201d and \u201cNewspaper\u201d do not have corresponding original depth video at the virtual viewpoint, all the rest eight sequences in the database were used for comparison. Similarly, the testing database is also categorized as four datasets, U;C,, CrUp: C;C,, and ALL dataset. The PLCC, SROCC, RMSE results are demonstrated in FIG. 17. To distinguish these reduced datasets from those in Section III subsection B, they are marked with \u201c*\u201d. The left three columns are the results from the synthesized depth video. As shown in FIG. 17, the values of most PLCC, SROCC and RMSE on the test datasets are a little better than the results of using the original depth video. Basically, they are com- parable. It indicates that although the original depth video has more precise depth values, the synthesized depth video, which are generated through DIBR from two original depth videos, is comparable or a little better in the synthesized video quality prediction. Moreover, using the synthesized depth video is more practical. The main reason is the depth video is used to help locate the flicker area by using edge detection and dilation. The original depth video has more precise depth values but may have geometrical misalign- ment with the synthesized texture video. Therefore, the\n\n[0152] Although not required, the embodiments described with reference to the Figures can be implemented as an application programming interface (API) or as a series of libraries for use by a developer or can be included within another software application, such as a terminal or personal computer operating system or a portable computing device operating system. Generally, as program modules include routines, programs, objects, components and data files assisting in the performance of particular functions, the skilled person will understand that the functionality of the software application may be distributed across a number of routines, objects or components to achieve the same func- tionality desired herein.\n\nIV. Exemplary System\n\nplary information handling system 200 that can be used, in single or in multiple, to implement one or more of the above method embodiments in one embodiment of the invention. The information handling system 200 may have different configurations, and it generally includes suitable compo- nents necessary to receive, store, and execute appropriate computer instructions, commands, or codes. The main com- ponents of the information handling system 200 are a processor 202 and a memory unit 204. The processor 202 may be formed by one or more CPU, MCU, controllers, logic circuits, Raspberry Pi chip, etc. The memory unit 204 may include one or more volatile memory unit (such as RAM, DRAM, SRAM), one or more non-volatile unit (such as ROM, PROM, EPROM, EEPROM, FRAM, MRAM, FLASH, SSD, NAND, and NVDIMM), or any of their combinations. Preferably, the information handling system 200 further includes one or more input devices 206 such as a keyboard, a mouse, a stylus, an image scanner, a micro- phone, a tactile input device (e.g., touch sensitive screen), and an image/video input device (e.g., camera). The infor- mation handling system 200 may further include one or\n\n[0153] It will be appreciated that where the methods and systems of the invention are either wholly implemented by computing system or partly implemented by computing systems then any appropriate computing system architecture may be utilized. This will include stand-alone computers, network computers, dedicated or non-dedicated hardware devices. Where the terms \u201ccomputing system\u201d and \u201ccom- puting device\u201d are used, these terms are intended to include any appropriate arrangement of computer or information processing hardware capable of implementing the function described.\n\n[0154] The expression \u201coriginal video\u201d used in this closure may refer to 1) Captured video: The source captured video without compression distortion; 2) Captured video virtual viewpoint: The captured source video at the virtual\n\n[0150]\n\nA comparative experiment was conducted to ana-\n\nsynthesized depth video may be better to be used for both practical usage and better performance.\n\n[0151]\n\nFIG. 18 shows a schematic diagram of an\n\nexem-\n\nmore output devices 208 such as one or more displays (e.g.,\n\nmonitor), speakers, disk\n\ndrives, headphones, earphones,\n\nment the method of the present invention.\n\ndis-\n\nat\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n13\n\nviewpoint and without compression / rendering distortion ;\n\nprediction . In addition , the real captured videos of neigh\n\nand 3 ) Captured video at real viewpoint : The source cap\n\nboring views can be used as reference if the real captured\n\ntured video at the real viewpoints . The system and method\n\nvideos at the synthesized view are unavailable . When using\n\nthe neighboring views , a rendering process is required to\n\nof the invention is not limited to the use of such \" original\n\nproject the neighboring views to the synthesized view . For\n\nvideo \u201d , but any \u201c reference video \u201d that is a video , preferably\n\ncoloured , either the source or with some compression dis\n\ndepth reference video , either the synthesized view or neigh\n\nboring views can be used as reference input .\n\ntortion , and either at the real views or the virtual views . In\n\nthe experiment validation presented above , the reference\n\n[ 0156 ] Some of the embodiments of the invention which\n\ndetermine perceptual quality of synthesized videos can be\n\nvideo is selected as the captured source colour video at the\n\nvirtual viewpoint of synthesized videos , i.e. , the original\n\nused for perceptual video coding , transmission , video pre\n\nvideo at virtual viewpoint , for illustration purpose only .\n\nand post - processing and video enhancement . Also , some of\n\nGenerally , the reference video is input to video quality\n\nthe embodiments of the invention can be adopted in the\n\npredictor . And the predictor is to predict the quality of the\n\nadvanced 3D video and VR applications , such as three\n\nsynthesized video with respect to the reference video , which\n\ndimensional TV and Free - view point TV , and six - degree of\n\ncan be either the source captured video without distortion or\n\nfreedom ( 6DoF ) omnidirectional ( 360 degree ) videos . Some\n\nwith some compression distortions .\n\nof the embodiments of the invention can be used to measure\n\nthe quality of 3D synthesized video , which is important and\n\nuseful in 3D video / image processing , coding , transmission\n\nV. REMARKS\n\nand 3D content production . Compared with existing image /\n\n[ 0155 ] The above embodiments of the invention have\n\nvideo quality predictors / metrics , the embodiments can\n\nprovided a new video quality assessment metric for 3D\n\nachieve more consistency with human vision system and can\n\nsynthesized videos , which can be based on three major\n\nbe more accurate for synthesized videos , e.g. , videos in 3D\n\ncomponents :\n\nspatio - temporal distortion measurement ,\n\nor VR systems . In particular , some embodiments of the\n\nflicker distortion measurement , and pooling algorithm . The\n\ninvention can predict or determine quality of synthesized\n\npooling algorithm may integrate the spatio - temporal distor\n\nvideos more accurately than existing methods , especially on\n\ntion and flicker distortion ( and their measurements ) of the\n\nthe view synthesis distortions induced from compressed\n\nsynthesized video to assess the quality of synthesized video .\n\ndepth videos . Some embodiments of the invention can\n\nMeasurement of flicker distortion includes considering the\n\nmeasure quality degradation of synthesized video caused by\n\ngeometrical distortion and temporal flickering artifacts from\n\ndistortion in color and / or depth videos and the rendering\n\ncolor and depth video distortions . The flicker distortion\n\nalgorithms .\n\nmeasurement may include 5 main stages , namely : Temporal\n\n[ 0157 ]\n\nIt will be appreciated by persons skilled in the art\n\nLayer Conversion , Gradient Feature Extraction , Depth\n\nthat numerous variations and / or modifications may be made\n\nImage based Flicker Distortion Area Detection , Sparse Rep\n\nto the invention as shown in the specific embodiments\n\nresentation for Flicker Distortion Features , and Weighted\n\nwithout departing from the spirit or scope of the invention as\n\nPooling for Temporal Layers . The synthesized video can be\n\nbroadly described . The described embodiments of the inven\n\nregarded as 3D volumetric data with the spatial , temporal\n\ntion should therefore be considered in all respects as illus\n\nand time dimensions ( X , Y , T ) . Then Temporal Layer\n\ntrative , not restrictive .\n\nConversion can be used to extract X - T or Y - T planes for\n\n1. A computer - implemented method for determining a\n\ntemporal flickering detection . The synthesized video data\n\nquality of a synthesized video file , comprising :\n\n( XYT ) can be decomposed as multiple XT or YT planes . The\n\nprocessing a reference video file and a synthesized video\n\nGradient Feature Extraction and Depth Image based Flicker\n\nDistortion Area Detection detect candidate flickering\n\nfile associated with the reference video file to compare\n\nregions and / or flickering features in the synthesized videos\n\nthe referencevideo file and the synthesized video file ;\n\nin XT or YT planes . The gradient feature of the depth map\n\nand\n\nmay be used as the feature to locate the flickering regions .\n\ndetermining an extent of flicker distortion of the synthe\n\nSparse representation can be adopted to represent the flick\n\nsized video file based on the processing .\n\nering features in 3D synthesized videos , in which the ampli\n\n2. The computer - implemented method of claim 1 ,\n\ntude and phase information can be jointly used to X - T or Y - T\n\nwherein determining an extent of flicker distortion of the\n\nplanes . The sparse representation can be 3D sparse repre\n\nsynthesized video file based on the comparison comprises :\n\nsentation , which is more complex , or 2D sparse represen\n\ndetermining respective extents of flicker distortion for\n\ntation ( to X - T or Y - T planes to capture the temporal flick\n\neach temporal frame of the synthesized video file ; and\n\nering features in synthesized video ) , which is less complex\n\ndetermining an overall extent of flicker distortion of the\n\nhence more computationally efficient . Amplitude and phase\n\nsynthesized video file based on the respective extents of\n\ninformation the learned 2D sparse representation can be\n\nflicker distortion .\n\nused to represent the temporal flickering features . A\n\n3. The computer - implemented method of claim 2 ,\n\nweighted function can be used for balancing amplitude and\n\nwherein determining an extent of flicker distortion of the\n\nphase information to improve effectiveness . An integration\n\nsynthesized video file based on the comparison further\n\nalgorithm can be applied to combine each block of the\n\ncomprises :\n\ntemporal layers . A weighted pooling algorithm integrates\n\nweighting the respective extents of flicker distortion for\n\nscores from each temporal layer , in which the importance of\n\neach temporal frame of the synthesized video file to\n\neach temporal layer can be measured and considered . The\n\nnumber of edge patches can be used to rank the importance\n\ndetermine the overall extent of flicker distortion .\n\n4. The computer - implemented method of claim 2 ,\n\n( which affects the weighting ) of each temporal layer . For\n\nwherein processing the reference video file and the synthe\n\ncolor reference video , the real captured videos at the syn\n\nsized video file comprises :\n\nthesized view can be used as reference for visual quality\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\nprediction. In addition, the real captured videos of neigh- boring views can be used as reference if the real captured videos at the synthesized view are unavailable. When using the neighboring views, a rendering process is required to project the neighboring views to the synthesized view. For depth reference video, either the synthesized view or neigh- boring views can be used as reference input.\n\nviewpoint and without compression/rendering distortion; and 3) Captured video at real viewpoint: The source cap- tured video at the real viewpoints. The system and method of the invention is not limited to the use of such \u201coriginal video\u201d, but any \u201creference video\u201d that is a video, preferably coloured, either the source or with some compression dis- tortion, and either at the real views or the virtual views. In the experiment validation presented above, the reference video is selected as the captured source colour video at the virtual viewpoint of synthesized videos, i.e., the original video at virtual viewpoint, for illustration purpose only. Generally, the reference video is input to video quality predictor. And the predictor is to predict the quality of the synthesized video with respect to the reference video, which can be either the source captured video without distortion or with some compression distortions.\n\n[0156] determine perceptual quality of synthesized videos can be used for perceptual video coding, transmission, video pre- and post-processing and video enhancement. Also, some of the embodiments of the invention can be adopted in the advanced 3D video and VR applications, such as three- dimensional TV and Free-view point TV, and six-degree of freedom (6DoF) omnidirectional (360 degree) videos. Some of the embodiments of the invention can be used to measure the quality of 3D synthesized video, which is important and useful in 3D video/image processing, coding, transmission and 3D content production. Compared with existing image/ video quality predictors/metrics, the embodiments can achieve more consistency with human vision system and can be more accurate for synthesized videos, e.g., videos in 3D or VR systems. In particular, some embodiments of the invention can predict or determine quality of synthesized videos more accurately than existing methods, especially on the view synthesis distortions induced from compressed. depth videos. Some embodiments of the invention can measure quality degradation of synthesized video caused by distortion in color and/or depth videos and the rendering\n\nV. REMARKS\n\nsynthesized videos, which can be based on three major components: spatio-temporal distortion measurement, flicker distortion measurement, and pooling algorithm. The pooling algorithm may integrate the spatio-temporal distor- tion and flicker distortion (and their measurements) of the synthesized video to assess the quality of synthesized video. Measurement of flicker distortion includes considering the geometrical distortion and temporal flickering artifacts from color and depth video distortions. The flicker distortion measurement may include 5 main stages, namely: Temporal Layer Conversion, Gradient Feature Extraction, Depth Image based Flicker Distortion Area Detection, Sparse Rep- resentation for Flicker Distortion Features, and Weighted Pooling for Temporal Layers. The synthesized video can be regarded as 3D volumetric data with the spatial, temporal and time dimensions (X, Y, T). Then Temporal Layer Conversion can be used to extract X-T or Y-T planes for temporal flickering detection. The synthesized video data (XYT) can be decomposed as multiple XT or YT planes. The Gradient Feature Extraction and Depth Image based Flicker Distortion Area Detection detect candidate flickering regions and/or flickering features in the synthesized videos in XT or YT planes. The gradient feature of the depth map may be used as the feature to locate the flickering regions. Sparse representation can be adopted to represent the flick- ering features in 3D synthesized videos, in which the ampli- tude and phase information can be jointly used to X-T or Y-T planes. The sparse representation can be 3D sparse repre- sentation, which is more complex, or 2D sparse represen- tation (to X-T or Y-T planes to capture the temporal flick- ering features in synthesized video), which is less complex hence more computationally efficient. Amplitude and phase information the learned 2D sparse representation can used to represent the temporal flickering features. A weighted function can be used for balancing amplitude and phase information to improve effectiveness. An integration algorithm can be applied to combine each block of the temporal layers. A weighted pooling algorithm integrates scores from each temporal layer, in which the importance of each temporal layer can be measured and considered. The number of edge patches can be used to rank the importance (which affects the weighting) of each temporal layer. For color reference video, the real captured videos at the syn-\n\n[0157] It will be appreciated by persons skilled in the art that numerous variations and/or modifications may be made to the invention as shown in the specific embodiments without departing from the spirit or scope of the invention as broadly described. The described embodiments of the inven- tion should therefore be considered in all respects as illus- trative, not restrictive.\n\n1. A computer-implemented method for determining a quality of a synthesized video file, comprising:\n\nprocessing a reference video file and a synthesized video file associated with the reference video file to compare the referencevideo file and the synthesized video file; and\n\ndetermining an extent of flicker distortion of the synthe- sized video file based on the processing.\n\n2. The computer-implemented method of claim 1, wherein determining an extent of flicker distortion of the synthesized video file based on the comparison comprises: determining respective extents of flicker distortion for each temporal frame of the synthesized video file; and determining an overall extent of flicker distortion of the synthesized video file based on the respective extents of\n\n3. The computer-implemented method of claim 2, wherein determining an extent of flicker distortion of the synthesized video file based on the comparison further comprises:\n\nthe respective extents of flicker distortion each temporal frame of the synthesized video file determine the overall extent of flicker distortion.\n\n4. The computer-implemented method of claim wherein processing the reference video file and the synthe- video file comprises:\n\n[0155] The above embodiments provided a video quality assessment\n\nof the invention have metric for 3D\n\nnew\n\nbe\n\nthesized view can be used as reference for visual quality\n\nSome of the embodiments of the invention which\n\nalgorithms.\n\nflicker distortion.\n\nweighting\n\nfor\n\nto\n\n2,\n\nsized\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n14\n\nsegmenting the reference video file into a plurality of\n\ntechniques to determine flicker distortion in each of the\n\nweighted temporal gradient layers of the synthesized\n\ntemporal layers ;\n\nsegmenting the synthesized video file into a plurality of\n\nvideo file .\n\ntemporal layers ; and\n\n12. The computer - implemented method of claim 10 ,\n\nprocessing the temporal layers of the reference video file\n\nwherein processing the reference video file and the synthe\n\nand the temporal layers of the synthesized video file to\n\nsized video file further comprises :\n\nidentify flicker distortion in the synthesized video file .\n\nprocessing the weighted temporal gradient layers associ\n\n5. The computer - implemented method of claim 4 ,\n\nated with the reference video file and the weighted\n\nwherein processing the temporal layers of the reference\n\ntemporal gradient layers associated with the synthe\n\nvideo file and the temporal layers of the synthesized video\n\nsized video file using sparse representation processing\n\nfile comprises :\n\ntechniques to determine phase distortion and amplitude\n\nprocessing the temporal layers of the reference video file\n\ndistortion associated with flicker distortion in each of\n\nto determine temporal gradient layers associated with\n\nthe weighted temporal gradient layers of the synthe\n\nthe temporal layers of the reference video file ; and / or\n\nsized video file .\n\nprocessing the temporal layers of the synthesized video\n\n13. The computer - implemented method of claim 12 ,\n\nfile to determine temporal gradient layers associated\n\nwherein processing the reference video file and the synthe\n\nwith the temporal layers of the synthesized video file .\n\nsized video file further comprises :\n\n6. The computer - implemented method of claim 5 ,\n\nweighting the phase distortion and the amplitude distor\n\nwherein processing the temporal layers of the reference\n\ntion respectively associated with flicker distortion in\n\nvideo file and the temporal layers of the synthesized video\n\neach of the weighted temporal gradient layers of the\n\nfile further comprises :\n\nsynthesized video file .\n\nfiltering the temporal gradient layers of the reference\n\n14. The computer - implemented method of claim 13 , fur\n\nvideo file to remove gradient features with values\n\nther comprising :\n\nbelow a threshold ; and\n\ndetermining an extent of spatial - temporal activity distor\n\nfiltering the temporal gradient layers of the synthesized\n\ntion of the synthesized video file .\n\nvideo file to remove gradient features with values\n\n15. The computer - implemented method of claim 14 ,\n\nbelow a threshold .\n\nwherein determining an extent of spatial - temporal activity\n\n7. The computer - implemented method of claim\n\n5 ,\n\ndistortion of the synthesized video file comprises :\n\nwherein processing the reference video file and the synthe\n\ndetermining respective extents of spatial - temporal activ\n\nsized video file further comprises :\n\nity distortion for each temporal frame of the synthe\n\nprocessing a reference depth video file associated with the\n\nreference video file for facilitating comparison of the\n\nsized video file ; and\n\nreference video file and the synthesized video file .\n\ndetermining an overall extent of spatial - temporal activity\n\n8. The computer - implemented method of claim 7 ,\n\ndistortion of the synthesized video file based on the\n\nwherein processing the reference depth video file comprises :\n\nrespective extents of spatial - temporal activity distor\n\nprocessing the reference depth video file to detect edges\n\ntion .\n\nof the reference depth video file to generate a depth\n\n16. The computer - implemented method of claim 15 ,\n\nwherein determining an extent of spatial - temporal activity\n\nedge video file ; and\n\nsegmenting the depth edge video file into a plurality of\n\ndistortion of the synthesized video file further comprises :\n\ntemporal depth layers .\n\nweighting the respective extents of spatial - temporal activ\n\n9. The computer - implemented method of claim 8 ,\n\nity distortion for each temporal frame of the synthe\n\nwherein processing the reference depth video file further\n\nsized video file to determine the overall extent of\n\ncomprises :\n\nspatial - temporal activity distortion .\n\nprocessing the temporal depth layers to expand the\n\n17. The computer - implemented method of claim 16 , fur\n\ndetected edge width in the temporal depth layers .\n\nther comprising :\n\n10. The computer - implemented method of claim 9 ,\n\ndetermining a quality of the synthesized video file based\n\nwherein processing the reference video file and the synthe\n\non the determined extent of flicker distortion and the\n\nsized video file further comprises :\n\ndetermined extent of spatial - temporal activity distor\n\nprocessing the temporal gradient layers of the reference\n\ntion .\n\nvideo file based on the temporal depth layers to obtain\n\n18. The computer - implemented method of claim 1 ,\n\nweighted temporal gradient layers associated with the\n\nwherein the synthesized video file is a 3D video file con\n\nreference video file ; and\n\ntaining 3D video data .\n\nprocessing the temporal gradient layers of the synthesized\n\n19. The computer - implemented method of claim 1 ,\n\nvideo file based on the temporal depth layers to obtain\n\nwherein the synthesized video file is a virtual reality video\n\nweighted temporal gradient layers associated with the\n\nfile containing virtual reality video data .\n\nsynthesized video file .\n\n20. A system for determining a quality of a synthesized\n\n11. The computer - implemented method of claim 10 ,\n\nvideo file , comprising one or more processors arranged to :\n\nwherein processing the reference video file and the synthe\n\nprocess a reference video file and a synthesized video file\n\nsized video file further comprises :\n\nassociated with the reference video file to compare the\n\nprocessing the weighted temporal gradient layers associ\n\noriginal video file and the synthesized video file ; and\n\nated with the reference video file and the weighted\n\ntemporal gradient layers associated with the synthe\n\ndetermine an extent of flicker distortion of the synthesized\n\nsized video file using sparse representation processing\n\nvideo file based on the processing .\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\ntechniques to determine flicker distortion in each of the weighted temporal gradient layers of the synthesized video file.\n\nsegmenting the reference video file into a plurality temporal layers;\n\nsegmenting the synthesized video file into a plurality temporal layers; and\n\n12. The computer-implemented method of claim wherein processing the reference video file and the synthe- sized video file further comprises:\n\nprocessing the temporal layers of the reference video file and the temporal layers of the synthesized video file identify flicker distortion in the synthesized video file.\n\nprocessing the weighted temporal gradient layers associ- ated with the reference video file and the weighted temporal gradient layers associated with the synthe- sized video file using sparse representation processing techniques to determine phase distortion and amplitude distortion associated with flicker distortion in each of the weighted temporal gradient layers of the synthe- sized video file.\n\n5. The computer-implemented method of claim wherein processing the temporal layers of the reference video file and the temporal layers of the synthesized video comprises:\n\nprocessing the temporal layers of the reference video to determine temporal gradient layers associated the temporal layers of the reference video file; and/or\n\nprocessing the temporal layers of the synthesized file to determine temporal gradient layers associated with the temporal layers of the synthesized video\n\n13. The computer-implemented method of claim wherein processing the reference video file and the synthe- sized video file further comprises:\n\n6. The computer-implemented method of claim wherein processing the temporal layers of the reference video file and the temporal layers of the synthesized video file further comprises:\n\nweighting the phase distortion and the amplitude distor- tion respectively associated with flicker distortion in each of the weighted temporal gradient layers of the synthesized video file.\n\nfiltering the temporal gradient layers of the reference video file to remove gradient features with values below a threshold; and\n\n14. The computer-implemented method of claim 13, comprising:\n\nfiltering the temporal gradient layers of the synthesized video file to remove gradient features with values below a threshold.\n\ndetermining an extent of spatial-temporal activity distor- tion of the synthesized video file.\n\n15. The computer-implemented method of claim 14, wherein determining an extent of spatial-temporal activity distortion of the synthesized video file comprises:\n\n7. The computer-implemented method of claim 5, wherein processing the reference video file and the synthe- sized video file further comprises:\n\ndetermining respective extents of spatial-temporal activ- ity distortion for each temporal frame of the synthe- sized video file; and\n\nprocessing a reference depth video file associated with reference video file for facilitating comparison of reference video file and the synthesized video file.\n\ndetermining an overall extent of spatial-temporal activity distortion of the synthesized video file based on respective extents of spatial-temporal activity distor- tion.\n\n8. The computer-implemented method of claim wherein processing the reference depth video file comprises:\n\nprocessing the reference depth video file to detect of the reference depth video file to generate a depth edge video file; and\n\n16. The computer-implemented method of claim wherein determining an extent of spatial-temporal activity distortion of the synthesized video file further comprises:\n\nsegmenting the depth edge video file into a plurality temporal depth layers.\n\nweighting the respective extents of spatial-temporal activ- ity distortion for each temporal frame of the synthe- sized video file to determine the overall extent spatial-temporal activity distortion.\n\n9. The computer-implemented method of claim wherein processing the reference depth video file further comprises:\n\nprocessing the temporal depth layers to expand detected edge width in the temporal depth layers.\n\n17. The computer-implemented method of claim 16, comprising:\n\n10. The computer-implemented method of claim wherein processing the reference video file and the synthe- sized video file further comprises:\n\ndetermining a quality of the synthesized video file based on the determined extent of flicker distortion and determined extent of spatial-temporal activity distor- tion.\n\nprocessing the temporal gradient layers of the reference video file based on the temporal depth layers to obtain weighted temporal gradient layers associated with reference video file; and\n\n18. The computer-implemented method of claim wherein the synthesized video file is a 3D video file taining 3D video data.\n\nprocessing the temporal gradient layers of the synthesized video file based on the temporal depth layers to obtain weighted temporal gradient layers associated with synthesized video file.\n\n19. The computer-implemented method of claim wherein the synthesized video file is a virtual reality video containing virtual reality video data.\n\n20. A system for determining a quality of a synthesized video file, comprising one or more processors arranged to:\n\ncomputer-implemented wherein processing the reference video file and the synthe- sized video file further comprises:\n\nprocess a reference video file and a synthesized video file associated with the reference video file to compare the original video file and the synthesized video file; and a\n\nprocessing the weighted temporal gradient layers ated with the reference video file and the weighted temporal gradient layers associated with the synthe- sized video file using sparse representation processing\n\ndetermine an extent of flicker distortion of the synthesized video file based on the processing.\n\nof\n\nof\n\nto\n\n4,\n\nfile\n\nfile\n\nwith\n\nvideo\n\nfile.\n\n5,\n\nthe the\n\n7,\n\nedges\n\nof\n\n8,\n\nthe\n\n9,\n\nthe\n\nthe\n\n11.\n\nThe\n\nmethod\n\nof\n\nclaim\n\n10,\n\nassoci-\n\n10,\n\n12,\n\nfur-\n\nther\n\nthe\n\n15,\n\nof\n\nfur-\n\nther\n\nthe\n\n1,\n\ncon-\n\n1,\n\nfile\n\nFeb. 11 , 2021\n\nUS 2021/0044791 A1\n\n15\n\n21. The system of claim 20 , further comprising :\n\na display operably connected with the one or more\n\nprocessors for displaying the determined extent of\n\nflicker distortion .\n\n22. The system of claim 20 , wherein the one or more\n\nprocessors are further arranged to :\n\ndetermine an extent of spatial - temporal activity distortion\n\nof the synthesized video file .\n\n23. The system of claim 22 , wherein the one or more\n\nprocessors are further arranged to :\n\ndetermine a quality of the synthesized video file based on\n\nthe determined extent of flicker distortion and the\n\ndetermined extent of spatial - temporal activity distor\n\ntion .\n\n24. The system of claim 20 , wherein the synthesized video\n\nfile is a 3D video file containing 3D video data or a virtual\n\nreality video file containing virtual reality video data .\n\nFeb. 11, 2021\n\nUS 2021/0044791 Al\n\n15\n\n21. The system of claim 20, further comprising:\n\ndisplay operably connected with the one or more processors for displaying the determined extent of flicker distortion.\n\n22. The system of claim 20, wherein the one or more are further arranged to:\n\ndetermine an extent of spatial-temporal activity distortion of the synthesized video file.\n\n23. The system of claim 22, wherein the one or more processors are further arranged to:\n\ndetermine a quality of the synthesized video file based on the determined extent of flicker distortion and the determined extent of spatial-temporal activity distor- tion.\n\nsystem synthesized file is a 3D video file containing 3D video data or a virtual reality video file containing virtual reality video data.\n\nek Ok kk\n\na\n\nprocessors\n\n24. The\n\nof claim 20, wherein the\n\nvideo", "type": "Document"}}